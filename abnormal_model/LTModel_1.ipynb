{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 완전한 선박 이상행동 탐지 시스템\n",
      "훈련 → 평가 → 테스트 예측 파이프라인\n",
      "============================================================\n",
      "\n",
      "🔥 STEP 1: 모델 훈련\n",
      "------------------------------\n",
      "🌟 향상된 선박 이상행동 탐지 모델 훈련\n",
      "============================================================\n",
      "🔍 GPU 사용 가능: True\n",
      "📁 데이터 로딩...\n",
      "✅ 원본 데이터: (2678129, 27)\n",
      "🔧 도메인 지식 기반 특성 엔지니어링...\n",
      "📊 실제 데이터에서 규칙 가중치 계산...\n",
      "  rule1_foreign_slow_non_anchor: 217726건 중 213711건 이상 → 정확도 0.982\n",
      "  rule2_ais_off_non_anchor: 해당 케이스 없음 → 가중치 0.0\n",
      "  rule3_special_zone_frequent: 137666건 중 130756건 이상 → 정확도 0.950\n",
      "  rule4_sharp_turn_non_anchor: 55536건 중 39009건 이상 → 정확도 0.702\n",
      "  rule5_slow_back_forth: 해당 케이스 없음 → 가중치 0.0\n",
      "  rule6_restricted_zone: 584007건 중 584007건 이상 → 정확도 1.000\n",
      "✅ 규칙 가중치 계산 완료\n",
      "💾 규칙 가중치 저장: ./rule_weights.json\n",
      "📊 규칙 통계 저장: ./rule_weights_stats.json\n",
      "✅ 규칙 기반 특성 8개 생성 완료\n",
      "📊 최종 데이터: (2678129, 35)\n",
      "📈 타겟 분포: {1: 1633217, 0: 1044912}\n",
      "📊 이상선박 비율: 60.98%\n",
      "🔧 사용할 특성: 33개\n",
      "📊 훈련 데이터: 2,142,503개\n",
      "📊 테스트 데이터: 535,626개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.12.7\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #107-Ubuntu SMP Wed Feb 7 13:26:48 UTC 2024\n",
      "CPU Count:          2\n",
      "Memory Avail:       15.19 GB / 24.00 GB (63.3%)\n",
      "Disk Space Avail:   449.89 GB / 511.75 GB (87.9%)\n",
      "===================================================\n",
      "Presets specified: ['high_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=2, num_bag_folds=5, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~5x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1800s of the 7200s of remaining time (25%).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳ 모델 훈련 시작... (제한시간: 120분)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 02:58:28,661\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-05-29 02:58:33,053\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/home/elicer/ai/enhanced_ship_anomaly_models_b3/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Beginning AutoGluon training ... Time limit = 1793s\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m AutoGluon will save models to \"/home/elicer/ai/enhanced_ship_anomaly_models_b3/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Train Data Rows:    1904447\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Train Data Columns: 33\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Label Column:       target\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tAvailable Memory:                    14447.25 MB\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTrain Data (Original)  Memory Usage: 479.48 MB (3.3% of available memory)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t\tNote: Converting 10 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tUseless Original Features (Count: 2): ['rule2_ais_off_non_anchor', 'rule5_slow_back_forth']\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tUnused Original Features (Count: 1): ['rule6_restricted_zone']\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t('int', []) : 1 | ['rule6_restricted_zone']\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t('float', []) : 18 | ['trajectory_duration', 'avg_sog', 'std_sog', 'max_sog', 'min_sog', ...]\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t('int', [])   : 12 | ['is_korean_ship', 'num_points', 'sharp_turns', 'num_low_speed_periods', 'off_events', ...]\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t('float', [])     : 18 | ['trajectory_duration', 'avg_sog', 'std_sog', 'max_sog', 'min_sog', ...]\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t('int', [])       :  3 | ['num_points', 'num_low_speed_periods', 'num_zone_entries']\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t('int', ['bool']) :  9 | ['is_korean_ship', 'sharp_turns', 'off_events', 'restricted_zone_flag', 'back_forth_count', ...]\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t16.3s = Fit runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t30 features in original data used to generate 30 features in processed data.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTrain Data (Processed) Memory Usage: 321.47 MB (2.2% of available memory)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Data preprocessing and feature engineering runtime = 17.23s ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t'XGB': [{'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 8, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'objective': 'binary:logistic', 'eval_metric': 'auc', 'scale_pos_weight': 0.62, 'subsample': 0.8, 'colsample_bytree': 0.8}],\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t'GBM': [{'num_boost_round': 500, 'learning_rate': 0.05, 'device': 'gpu', 'objective': 'binary', 'metric': 'auc', 'is_unbalance': True, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}],\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t'CAT': [{'iterations': 500, 'learning_rate': 0.05, 'depth': 8, 'task_type': 'GPU', 'devices': '0', 'auto_class_weights': 'Balanced', 'eval_metric': 'AUC', 'bootstrap_type': 'Bernoulli', 'subsample': 0.8}],\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t'NN_TORCH': [{'num_epochs': 200, 'learning_rate': 0.001, 'batch_size': 512, 'activation': 'relu', 'dropout_prob': 0.3, 'weight_decay': 0.0001}],\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m AutoGluon will fit 3 stack levels (L1 to L3) ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting 4 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 789.17s of the 1776.07s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=15.75%)\n",
      "\u001b[36m(_ray_fit pid=4125174)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4125174)\u001b[0m \tRan out of time, early stopping on iteration 176. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4125174)\u001b[0m \t[176]\tvalid_set's binary_logloss: 0.026618\tvalid_set's auc: 0.999711\n",
      "\u001b[36m(_ray_fit pid=4126149)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4126149)\u001b[0m \tRan out of time, early stopping on iteration 277. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4126149)\u001b[0m \t[277]\tvalid_set's binary_logloss: 0.0208105\tvalid_set's auc: 0.999805\n",
      "\u001b[36m(_ray_fit pid=4127145)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4127145)\u001b[0m \tRan out of time, early stopping on iteration 373. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4127145)\u001b[0m \t[373]\tvalid_set's binary_logloss: 0.0177185\tvalid_set's auc: 0.999856\n",
      "\u001b[36m(_ray_fit pid=4128148)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4128148)\u001b[0m \tRan out of time, early stopping on iteration 377. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4128148)\u001b[0m \t[377]\tvalid_set's binary_logloss: 0.0183819\tvalid_set's auc: 0.999845\n",
      "\u001b[36m(_ray_fit pid=4129156)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4129156)\u001b[0m \tRan out of time, early stopping on iteration 375. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4129156)\u001b[0m \t[375]\tvalid_set's binary_logloss: 0.0182184\tvalid_set's auc: 0.999847\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9998\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t663.04s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t29.88s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 105.11s of the 1092.01s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.82% memory usage per fold, 75.29%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=18.82%)\n",
      "\u001b[36m(_ray_fit pid=4130296)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4130296)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130296)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130420)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4130420)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130420)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130538)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4130538)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130538)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130653)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4130653)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130653)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130772)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4130772)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130772)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9888\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t49.17s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.28s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 47.97s of the 1034.87s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.85% memory usage per fold, 41.70%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=20.85%)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=4131033, ip=10.0.2.100)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                 ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise processed_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                                                                                                                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m ray.exceptions.RayTaskError(UnboundLocalError): \u001b[36mray::_ray_fit()\u001b[39m (pid=4131033, ip=10.0.2.100)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                 ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 36.51s of the 1023.41s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=14.60%)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1000.05s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9998\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t10.7s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.39s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting 4 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 659.08s of the 988.73s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.98% memory usage per fold, 67.90%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=16.98%)\n",
      "\u001b[36m(_ray_fit pid=4131651)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4132307)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4132898)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4133432)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4133967)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4133967)\u001b[0m \tRan out of time, early stopping on iteration 412. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4133967)\u001b[0m \t[397]\tvalid_set's binary_logloss: 0.0142428\tvalid_set's auc: 0.999851\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9999\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t426.55s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t10.75s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 220.98s of the 550.63s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.17% memory usage per fold, 40.33%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=20.17%)\n",
      "\u001b[36m(_ray_fit pid=4134865)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4134865)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4134865)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4134971)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4134971)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4134971)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4135090)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4135090)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4135090)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4135211)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4135211)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4135211)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4135328)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4135328)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4135328)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9997\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t51.34s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.64s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 161.95s of the 491.60s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.48% memory usage per fold, 44.96%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=22.48%)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=4135578, ip=10.0.2.100)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                 ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise processed_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                                                                                                                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m ray.exceptions.RayTaskError(UnboundLocalError): \u001b[36mray::_ray_fit()\u001b[39m (pid=4135578, ip=10.0.2.100)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                 ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 150.96s of the 480.61s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=15.63%)\n",
      "\u001b[36m(_ray_fit pid=4135779)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\u001b[36m(_ray_fit pid=4136003)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\u001b[36m(_ray_fit pid=4136200)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\u001b[36m(_ray_fit pid=4136422)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\u001b[36m(_ray_fit pid=4136666)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9998\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t132.63s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t7.94s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 339.34s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L2': 0.889, 'NeuralNetTorch_BAG_L2': 0.111}\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9999\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t15.98s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.37s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting 4 L3 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: LightGBM_BAG_L3 ... Training model for up to 322.87s of the 322.73s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.77% memory usage per fold, 71.07%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=17.77%)\n",
      "\u001b[36m(_ray_fit pid=4137157)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4137157)\u001b[0m \tRan out of time, early stopping on iteration 193. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4137157)\u001b[0m \t[174]\tvalid_set's binary_logloss: 0.0145792\tvalid_set's auc: 0.999861\n",
      "\u001b[36m(_ray_fit pid=4137552)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4137552)\u001b[0m \tRan out of time, early stopping on iteration 192. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4137552)\u001b[0m \t[192]\tvalid_set's binary_logloss: 0.0142814\tvalid_set's auc: 0.99986\n",
      "\u001b[36m(_ray_fit pid=4137989)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4137989)\u001b[0m \tRan out of time, early stopping on iteration 192. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4137989)\u001b[0m \t[191]\tvalid_set's binary_logloss: 0.0140887\tvalid_set's auc: 0.999878\n",
      "\u001b[36m(_ray_fit pid=4138414)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4138414)\u001b[0m \tRan out of time, early stopping on iteration 196. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4138414)\u001b[0m \t[196]\tvalid_set's binary_logloss: 0.0138516\tvalid_set's auc: 0.999879\n",
      "\u001b[36m(_ray_fit pid=4138845)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4138845)\u001b[0m \tRan out of time, early stopping on iteration 192. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4138845)\u001b[0m \t[192]\tvalid_set's binary_logloss: 0.0136773\tvalid_set's auc: 0.99987\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9999\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t273.46s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t7.22s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: CatBoost_BAG_L3 ... Training model for up to 40.63s of the 40.49s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.08% memory usage per fold, 42.16%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=21.08%)\n",
      "\u001b[36m(_ray_fit pid=4139426)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4139426)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return pickle.loads(ray_exception.serialized_exception)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m ModuleNotFoundError: No module named '_catboost'\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return RayError.from_bytes(obj)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return RayError.from_ray_exception(ray_exception)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise RuntimeError(msg) from e\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m RuntimeError: Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tWarning: Exception caused CatBoost_BAG_L3 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tSystem error: Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m traceback: Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return pickle.loads(ray_exception.serialized_exception)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m ModuleNotFoundError: No module named '_catboost'\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return RayError.from_bytes(obj)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return RayError.from_ray_exception(ray_exception)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise RuntimeError(msg) from e\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m RuntimeError: Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise processed_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                                                                                                                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 931, in get_objects\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise value\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m traceback: Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return pickle.loads(ray_exception.serialized_exception)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m ModuleNotFoundError: No module named '_catboost'\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return RayError.from_bytes(obj)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return RayError.from_ray_exception(ray_exception)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise RuntimeError(msg) from e\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m RuntimeError: Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: XGBoost_BAG_L3 ... Training model for up to 27.72s of the 27.58s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 23.11% memory usage per fold, 46.22%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=23.11%)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tWarning: Exception caused XGBoost_BAG_L3 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=4139668, ip=10.0.2.100)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                 ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise processed_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                                                                                                                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m ray.exceptions.RayTaskError(UnboundLocalError): \u001b[36mray::_ray_fit()\u001b[39m (pid=4139668, ip=10.0.2.100)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                 ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: NeuralNetTorch_BAG_L3 ... Training model for up to 16.57s of the 16.43s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=15.98%)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTime limit exceeded... Skipping NeuralNetTorch_BAG_L3.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: WeightedEnsemble_L4 ... Training model for up to 360.00s of the -7.37s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L3': 0.667, 'LightGBM_BAG_L2': 0.333}\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9999\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t30.56s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.36s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m AutoGluon training complete, total runtime = 1832.4s ... Best model: WeightedEnsemble_L4 | Estimated inference throughput: 6708.1 rows/s (380890 batch size)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t103.03s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tWarning: Exception caused CatBoost_BAG_L1_FULL to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tcatboost/cuda/cuda_lib/cuda_base.h:281: CUDA error 100: no CUDA-capable device is detected\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 261, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self.model.fit(X, **fit_final_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/catboost/core.py\", line 5245, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/catboost/core.py\", line 2410, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._train(\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/catboost/core.py\", line 1790, in _train\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"_catboost.pyx\", line 5023, in _catboost._CatBoost._train\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"_catboost.pyx\", line 5072, in _catboost._CatBoost._train\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m _catboost.CatBoostError: catboost/cuda/cuda_lib/cuda_base.h:281: CUDA error 100: no CUDA-capable device is detected\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m WARNING: Refit training failure detected for 'CatBoost_BAG_L1'... Falling back to using first fold to avoid downstream exception.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tThis is likely due to an out-of-memory error or other memory related issue. \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tPlease create a GitHub issue if this was triggered from a non-memory related problem.\n",
      "Warning: Exception encountered during DyStack sub-fit:\n",
      "\tCannot avoid training failure during refit for 'CatBoost_BAG_L1' by falling back to copying the first fold because it does not exist! (save_bag_folds=False)\n",
      "\tPlease specify `save_bag_folds=True` in the `.fit` call to avoid this exception.\n",
      "\t2\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1956s\t = DyStack   runtime |\t5244s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=2.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=2)`\n",
      "Beginning AutoGluon training ... Time limit = 5244s\n",
      "AutoGluon will save models to \"/home/elicer/ai/enhanced_ship_anomaly_models_b3\"\n",
      "Train Data Rows:    2142503\n",
      "Train Data Columns: 33\n",
      "Label Column:       target\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    15146.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 539.42 MB (3.6% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 10 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 2): ['rule2_ais_off_non_anchor', 'rule5_slow_back_forth']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['rule6_restricted_zone']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['rule6_restricted_zone']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 18 | ['trajectory_duration', 'avg_sog', 'std_sog', 'max_sog', 'min_sog', ...]\n",
      "\t\t('int', [])   : 12 | ['is_korean_ship', 'num_points', 'sharp_turns', 'num_low_speed_periods', 'off_events', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 18 | ['trajectory_duration', 'avg_sog', 'std_sog', 'max_sog', 'min_sog', ...]\n",
      "\t\t('int', [])       :  3 | ['num_points', 'num_low_speed_periods', 'num_zone_entries']\n",
      "\t\t('int', ['bool']) :  9 | ['is_korean_ship', 'sharp_turns', 'off_events', 'restricted_zone_flag', 'back_forth_count', ...]\n",
      "\t9.6s = Fit runtime\n",
      "\t30 features in original data used to generate 30 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 361.66 MB (2.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 10.49s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'XGB': [{'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 8, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'objective': 'binary:logistic', 'eval_metric': 'auc', 'scale_pos_weight': 0.62, 'subsample': 0.8, 'colsample_bytree': 0.8}],\n",
      "\t'GBM': [{'num_boost_round': 500, 'learning_rate': 0.05, 'device': 'gpu', 'objective': 'binary', 'metric': 'auc', 'is_unbalance': True, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}],\n",
      "\t'CAT': [{'iterations': 500, 'learning_rate': 0.05, 'depth': 8, 'task_type': 'GPU', 'devices': '0', 'auto_class_weights': 'Balanced', 'eval_metric': 'AUC', 'bootstrap_type': 'Bernoulli', 'subsample': 0.8}],\n",
      "\t'NN_TORCH': [{'num_epochs': 200, 'learning_rate': 0.001, 'batch_size': 512, 'activation': 'relu', 'dropout_prob': 0.3, 'weight_decay': 0.0001}],\n",
      "}\n",
      "AutoGluon will fit 3 stack levels (L1 to L3) ...\n",
      "Fitting 4 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2325.50s of the 5233.69s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.94% memory usage per fold, 67.76%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=16.94%)\n",
      "\t0.9999\t = Validation score   (roc_auc)\n",
      "\t1439.43s\t = Training   runtime\n",
      "\t70.75s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 861.37s of the 3769.56s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 31.85% memory usage per fold, 63.70%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=31.85%)\n",
      "\t0.9972\t = Validation score   (roc_auc)\n",
      "\t120.67s\t = Training   runtime\n",
      "\t1.13s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 729.05s of the 3637.23s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 35.90% memory usage per fold, 71.81%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=35.90%)\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=4155351, ip=10.0.2.100)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "    metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(UnboundLocalError): \u001b[36mray::_ray_fit()\u001b[39m (pid=4155351, ip=10.0.2.100)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "    metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 707.64s of the 3615.83s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 25.14% memory usage per fold, 50.28%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=25.14%)\n",
      "\t0.9986\t = Validation score   (roc_auc)\n",
      "\t593.23s\t = Training   runtime\n",
      "\t19.71s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 3003.39s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.9999\t = Validation score   (roc_auc)\n",
      "\t19.66s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting 4 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1988.15s of the 2982.75s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 30.33% memory usage per fold, 60.66%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=30.33%)\n",
      "\t0.9999\t = Validation score   (roc_auc)\n",
      "\t1379.45s\t = Training   runtime\n",
      "\t34.95s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 594.61s of the 1589.21s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 35.22% memory usage per fold, 70.43%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=35.22%)\n",
      "\t0.9998\t = Validation score   (roc_auc)\n",
      "\t111.4s\t = Training   runtime\n",
      "\t1.21s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 470.41s of the 1465.01s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 40.09% memory usage per fold, 40.09%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=40.09%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=4172176, ip=10.0.2.100)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "    metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 688, in after_all_folds_scheduled\n",
      "    self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in _run_pseudo_sequential\n",
      "    self._process_fold_results(finished[0], unfinished, fold_ctx)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(UnboundLocalError): \u001b[36mray::_ray_fit()\u001b[39m (pid=4172176, ip=10.0.2.100)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "    metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 448.98s of the 1443.58s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 27.95% memory usage per fold, 55.91%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=27.95%)\n",
      "\t0.9998\t = Validation score   (roc_auc)\n",
      "\t380.1s\t = Training   runtime\n",
      "\t16.19s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 1044.61s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L2': 0.917, 'NeuralNetTorch_BAG_L2': 0.083}\n",
      "\t0.9999\t = Validation score   (roc_auc)\n",
      "\t19.86s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting 4 L3 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L3 ... Training model for up to 1024.09s of the 1023.91s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 31.57% memory usage per fold, 63.13%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=31.57%)\n",
      "\t0.9999\t = Validation score   (roc_auc)\n",
      "\t826.74s\t = Training   runtime\n",
      "\t15.22s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L3 ... Training model for up to 183.74s of the 183.56s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 36.67% memory usage per fold, 73.35%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=36.67%)\n",
      "\t0.9996\t = Validation score   (roc_auc)\n",
      "\t99.04s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L3 ... Training model for up to 72.25s of the 72.07s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 43.10% memory usage per fold, 43.10%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=43.10%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\tWarning: Exception caused XGBoost_BAG_L3 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=4183423, ip=10.0.2.100)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "    metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 688, in after_all_folds_scheduled\n",
      "    self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in _run_pseudo_sequential\n",
      "    self._process_fold_results(finished[0], unfinished, fold_ctx)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(UnboundLocalError): \u001b[36mray::_ray_fit()\u001b[39m (pid=4183423, ip=10.0.2.100)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "    metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "Fitting model: NeuralNetTorch_BAG_L3 ... Training model for up to 49.01s of the 48.83s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 27.98% memory usage per fold, 55.95%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=27.98%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L3.\n",
      "Fitting model: WeightedEnsemble_L4 ... Training model for up to 360.00s of the 2.95s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L2': 0.444, 'LightGBM_BAG_L3': 0.444, 'LightGBM_BAG_L1': 0.111}\n",
      "\t0.9999\t = Validation score   (roc_auc)\n",
      "\t52.39s\t = Training   runtime\n",
      "\t0.77s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5295.07s ... Best model: WeightedEnsemble_L4 | Estimated inference throughput: 2689.8 rows/s (428501 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t231.99s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\t9.22s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\t81.92s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t19.66s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t129.17s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\t8.07s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetTorch_BAG_L2_FULL ...\n",
      "\t47.33s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L2': 0.917, 'NeuralNetTorch_BAG_L2': 0.083}\n",
      "\t19.86s\t = Training   runtime\n",
      "Fitting 1 L3 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L3_FULL ...\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t71.22s\t = Training   runtime\n",
      "Fitting 1 L3 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L3_FULL ...\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\t7.43s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L4_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L2': 0.444, 'LightGBM_BAG_L3': 0.444, 'LightGBM_BAG_L1': 0.111}\n",
      "\t52.39s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L4_FULL\" (Previously \"WeightedEnsemble_L4\"). AutoGluon will default to using \"WeightedEnsemble_L4_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 618.39s ... Best model: \"WeightedEnsemble_L4_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/elicer/ai/enhanced_ship_anomaly_models_b3\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 훈련 완료! (소요시간: 131.2분)\n",
      "\n",
      "📈 포괄적 성능 평가\n",
      "========================================\n",
      "🎯 핵심 성능 지표:\n",
      "  정확도: 0.9949\n",
      "  AUC 점수: 0.9999\n",
      "  훈련 시간: 131.2분\n",
      "  예측 시간: 75.82초\n",
      "  예측 속도: 7064 샘플/초\n",
      "\n",
      "📊 상세 분류 리포트:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          정상       0.99      1.00      0.99    208982\n",
      "          이상       1.00      0.99      1.00    326644\n",
      "\n",
      "    accuracy                           0.99    535626\n",
      "   macro avg       0.99      0.99      0.99    535626\n",
      "weighted avg       0.99      0.99      0.99    535626\n",
      "\n",
      "\n",
      "🔍 혼동 행렬:\n",
      "실제\\예측    정상    이상\n",
      "정상        208003    979\n",
      "이상        1757   324887\n",
      "\n",
      "🏆 모델 리더보드 (상위 5개):\n",
      "  WeightedEnsemble_L4: 0.9999\n",
      "  WeightedEnsemble_L3: 0.9999\n",
      "  LightGBM_BAG_L3: 0.9999\n",
      "  LightGBM_BAG_L2: 0.9999\n",
      "  LightGBM_BAG_L1: 0.9999\n",
      "\n",
      "⚖️ 규칙 기반 vs ML 비교:\n",
      "  규칙 기반 정확도: 0.7242\n",
      "  ML 모델 정확도: 0.9949\n",
      "  개선율: +37.4%\n",
      "\n",
      "⚠️ 특성 중요도 분석 불가\n",
      "\n",
      "============================================================\n",
      "🎉 훈련 파이프라인 완료!\n",
      "🎯 최종 AUC: 0.9999\n",
      "🎯 최종 정확도: 0.9949\n",
      "⏱️ 총 훈련 시간: 131.2분\n",
      "============================================================\n",
      "\n",
      "✅ 훈련 성공!\n",
      "💾 모델 저장 경로: ./enhanced_ship_anomaly_models_b2\n",
      "💾 규칙 가중치 저장: ./rule_weights.json\n",
      "\n",
      "🔮 STEP 2: 테스트 데이터 예측\n",
      "------------------------------\n",
      "🔮 Test 데이터 예측 시작\n",
      "==================================================\n",
      "📂 모델 로딩: ./enhanced_ship_anomaly_models_b2\n",
      "✅ 모델 로딩 완료\n",
      "📁 테스트 데이터 로딩: ./20240701.csv\n",
      "✅ 테스트 데이터 로딩 완료: (6779, 26)\n",
      "⚙️ 특성 엔지니어링 적용...\n",
      "🔧 도메인 지식 기반 특성 엔지니어링...\n",
      "📂 규칙 가중치 로드: ./rule_weights.json\n",
      "🔧 로드된 가중치:\n",
      "  rule1_foreign_slow_non_anchor: 0.982\n",
      "  rule2_ais_off_non_anchor: 0.000\n",
      "  rule3_special_zone_frequent: 0.950\n",
      "  rule4_sharp_turn_non_anchor: 0.702\n",
      "  rule5_slow_back_forth: 0.000\n",
      "  rule6_restricted_zone: 1.000\n",
      "📊 규칙 통계도 로드됨\n",
      "✅ 규칙 기반 특성 8개 생성 완료\n",
      "🔧 예측용 특성: 33개\n",
      "📊 예측할 데이터: 6,779개\n",
      "🔮 예측 수행...\n",
      "✅ 예측 완료! (소요시간: 1.07초)\n",
      "🚀 예측 속도: 6331 샘플/초\n",
      "\n",
      "📊 예측 결과 분포:\n",
      "  True: 3,965개 (58.5%)\n",
      "  False: 2,814개 (41.5%)\n",
      "\n",
      "⚖️ 규칙 vs 하이브리드:\n",
      "  규칙 기반 이상: 2,267개\n",
      "  하이브리드 이상: 3,965개\n",
      "  ML 추가 탐지: 1,698개\n",
      "\n",
      "💾 결과 저장 완료: ./test_predictions_basic_2.csv\n",
      "\n",
      "📋 예측 결과 샘플 (상위 10개):\n",
      "     MMSI result  confidence  rule_based\n",
      "100901213   True    0.999985           1\n",
      "103218823   True    0.999991           1\n",
      "107202858   True    0.999985           1\n",
      "111440519  False    0.000019           0\n",
      "116678950   True    0.999873           1\n",
      "202006213   True    0.999985           1\n",
      "200429918   True    0.999991           1\n",
      "111440500   True    0.999991           1\n",
      "111440105  False    0.018117           0\n",
      "106811519   True    0.999979           1\n",
      "\n",
      "🚨 고신뢰도 이상 선박 (3879개):\n",
      "     MMSI result  confidence  rule_based\n",
      "100901213   True    0.999985           1\n",
      "103218823   True    0.999991           1\n",
      "107202858   True    0.999985           1\n",
      "116678950   True    0.999873           1\n",
      "202006213   True    0.999985           1\n",
      "\n",
      "🎉 예측 완료!\n",
      "📁 결과 파일: ./test_predictions_basic_2.csv\n",
      "==================================================\n",
      "\n",
      "🎉 전체 파이프라인 성공!\n",
      "📊 총 6,779개 선박 예측 완료\n",
      "💾 최종 결과: ./test_predictions_basic_2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import xgboost as xgb\n",
    "\n",
    "class EnhancedShipAnomalyDetector:\n",
    "    \"\"\"향상된 선박 이상행동 탐지 시스템 - 완전 버전\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rule_feature_names = []\n",
    "        self.enhanced_feature_names = []\n",
    "        self.scaler = StandardScaler()\n",
    "        self.rule_weights = {}\n",
    "        self.rule_stats = {}\n",
    "        \n",
    "    def save_rule_weights(self, weights_path='./rule_weights.json'):\n",
    "        \"\"\"규칙 가중치를 파일로 저장\"\"\"\n",
    "        if hasattr(self, 'rule_weights'):\n",
    "            with open(weights_path, 'w') as f:\n",
    "                json.dump(self.rule_weights, f, indent=2)\n",
    "            print(f\"💾 규칙 가중치 저장: {weights_path}\")\n",
    "            \n",
    "            # 통계 정보도 저장\n",
    "            stats_path = weights_path.replace('.json', '_stats.json')\n",
    "            if hasattr(self, 'rule_stats'):\n",
    "                with open(stats_path, 'w') as f:\n",
    "                    json.dump(self.rule_stats, f, indent=2)\n",
    "                print(f\"📊 규칙 통계 저장: {stats_path}\")\n",
    "        else:\n",
    "            print(\"⚠️ 저장할 가중치가 없습니다\")\n",
    "\n",
    "    def load_rule_weights(self, weights_path='./rule_weights.json'):\n",
    "        \"\"\"저장된 규칙 가중치를 로드\"\"\"\n",
    "        if os.path.exists(weights_path):\n",
    "            with open(weights_path, 'r') as f:\n",
    "                self.rule_weights = json.load(f)\n",
    "            print(f\"📂 규칙 가중치 로드: {weights_path}\")\n",
    "            print(\"🔧 로드된 가중치:\")\n",
    "            for rule, weight in self.rule_weights.items():\n",
    "                print(f\"  {rule}: {weight:.3f}\")\n",
    "            \n",
    "            # 통계 정보도 로드 (있다면)\n",
    "            stats_path = weights_path.replace('.json', '_stats.json')\n",
    "            if os.path.exists(stats_path):\n",
    "                with open(stats_path, 'r') as f:\n",
    "                    self.rule_stats = json.load(f)\n",
    "                print(f\"📊 규칙 통계도 로드됨\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(f\"⚠️ 가중치 파일을 찾을 수 없습니다: {weights_path}\")\n",
    "            print(\"기본 가중치를 사용합니다\")\n",
    "            return False\n",
    "\n",
    "    def calculate_rule_weights(self, df):\n",
    "        \"\"\"훈련 데이터에서 각 규칙의 실제 정확도 계산\"\"\"\n",
    "        print(\"📊 실제 데이터에서 규칙 가중치 계산...\")\n",
    "        \n",
    "        rule_weights = {}\n",
    "        rule_stats = {}\n",
    "        \n",
    "        rule_columns = [\n",
    "            'rule1_foreign_slow_non_anchor', 'rule2_ais_off_non_anchor',\n",
    "            'rule3_special_zone_frequent', 'rule4_sharp_turn_non_anchor', \n",
    "            'rule5_slow_back_forth', 'rule6_restricted_zone'\n",
    "        ]\n",
    "        \n",
    "        for rule in rule_columns:\n",
    "            if rule in df.columns:\n",
    "                # 해당 규칙에 적용되는 케이스들\n",
    "                rule_cases = df[df[rule] == 1]\n",
    "                \n",
    "                if len(rule_cases) > 0:\n",
    "                    # 실제 이상 비율 계산\n",
    "                    accuracy = rule_cases['result'].mean()\n",
    "                    rule_weights[rule] = accuracy\n",
    "                    rule_stats[rule] = {\n",
    "                        'total_cases': len(rule_cases),\n",
    "                        'anomaly_cases': int(rule_cases['result'].sum()),\n",
    "                        'accuracy': accuracy\n",
    "                    }\n",
    "                    print(f\"  {rule}: {len(rule_cases)}건 중 {int(rule_cases['result'].sum())}건 이상 → 정확도 {accuracy:.3f}\")\n",
    "                else:\n",
    "                    rule_weights[rule] = 0.0\n",
    "                    rule_stats[rule] = {'total_cases': 0, 'anomaly_cases': 0, 'accuracy': 0.0}\n",
    "                    print(f\"  {rule}: 해당 케이스 없음 → 가중치 0.0\")\n",
    "            else:\n",
    "                rule_weights[rule] = 0.0\n",
    "                rule_stats[rule] = {'total_cases': 0, 'anomaly_cases': 0, 'accuracy': 0.0}\n",
    "        \n",
    "        self.rule_weights = rule_weights\n",
    "        self.rule_stats = rule_stats\n",
    "        \n",
    "        print(f\"✅ 규칙 가중치 계산 완료\")\n",
    "        return rule_weights\n",
    "    \n",
    "    def create_rule_based_features(self, df, is_training=False, weights_path='./rule_weights.json'):\n",
    "        \"\"\"6가지 규칙 기반 특성 생성\"\"\"\n",
    "        print(\"🔧 도메인 지식 기반 특성 엔지니어링...\")\n",
    "        \n",
    "        df_enhanced = df.copy()\n",
    "        \n",
    "        # 규칙 1: 외국국적 + 저속항해 + 비정박지\n",
    "        df_enhanced['rule1_foreign_slow_non_anchor'] = (\n",
    "            (df['is_korean_ship'] == 0) & \n",
    "            (df['anchor_zone_ratio'] < 0.8) & \n",
    "            ((df['low_speed_ratio'] > 0.7) | (df['avg_sog'] <= 5))\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 규칙 2: AIS OFF 이상 + 비정박지\n",
    "        df_enhanced['rule2_ais_off_non_anchor'] = (\n",
    "            (df['anchor_zone_ratio'] < 0.8) & \n",
    "            (df['off_events'] >= 3)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 규칙 3: 특수해역 다중진입\n",
    "        df_enhanced['rule3_special_zone_frequent'] = (\n",
    "            df['num_zone_entries'] > 50\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 규칙 4: 급변침 + 비정박지\n",
    "        df_enhanced['rule4_sharp_turn_non_anchor'] = (\n",
    "            (df['anchor_zone_ratio'] < 0.8) & \n",
    "            (df['sharp_turns'] >= 1)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 규칙 5: 저속 + 왕복항해\n",
    "        df_enhanced['rule5_slow_back_forth'] = (\n",
    "            (df['avg_sog'] <= 5) & \n",
    "            (df['back_forth_count'] >= 2)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 규칙 6: 금지구역 진입\n",
    "        df_enhanced['rule6_restricted_zone'] = df['restricted_zone_flag']\n",
    "        \n",
    "        # 가중치 처리\n",
    "        if is_training and 'result' in df.columns:\n",
    "            # 훈련 시: 실제 데이터에서 가중치 계산\n",
    "            rule_weights = self.calculate_rule_weights(df_enhanced)\n",
    "            # 계산한 가중치를 파일로 저장\n",
    "            self.save_rule_weights(weights_path)\n",
    "        else:\n",
    "            # 테스트 시: 저장된 가중치 로드\n",
    "            loaded = self.load_rule_weights(weights_path)\n",
    "            if loaded:\n",
    "                rule_weights = self.rule_weights\n",
    "            else:\n",
    "                # 로드 실패 시 기본값 사용\n",
    "                rule_weights = {\n",
    "                    'rule6_restricted_zone': 1.0,\n",
    "                    'rule1_foreign_slow_non_anchor': 0.97,\n",
    "                    'rule3_special_zone_frequent': 0.92,\n",
    "                    'rule4_sharp_turn_non_anchor': 0.72,\n",
    "                    'rule2_ais_off_non_anchor': 0.0,\n",
    "                    'rule5_slow_back_forth': 0.0\n",
    "                }\n",
    "                self.rule_weights = rule_weights\n",
    "        \n",
    "        # 종합 규칙 점수 (실제 데이터 기반 가중합)\n",
    "        df_enhanced['rule_composite_score'] = 0\n",
    "        for rule, weight in rule_weights.items():\n",
    "            if rule in df_enhanced.columns:\n",
    "                df_enhanced['rule_composite_score'] += df_enhanced[rule] * weight\n",
    "        \n",
    "        # 규칙 기반 예측\n",
    "        df_enhanced['rule_based_prediction'] = (\n",
    "            (df_enhanced['rule1_foreign_slow_non_anchor'] == 1) |\n",
    "            (df_enhanced['rule2_ais_off_non_anchor'] == 1) |\n",
    "            (df_enhanced['rule3_special_zone_frequent'] == 1) |\n",
    "            (df_enhanced['rule4_sharp_turn_non_anchor'] == 1) |\n",
    "            (df_enhanced['rule5_slow_back_forth'] == 1) |\n",
    "            (df_enhanced['rule6_restricted_zone'] == 1)\n",
    "        ).astype(int)\n",
    "        \n",
    "        rule_features = [\n",
    "            'rule1_foreign_slow_non_anchor', 'rule2_ais_off_non_anchor',\n",
    "            'rule3_special_zone_frequent', 'rule4_sharp_turn_non_anchor',\n",
    "            'rule5_slow_back_forth', 'rule6_restricted_zone',\n",
    "            'rule_composite_score', 'rule_based_prediction'\n",
    "        ]\n",
    "        \n",
    "        self.rule_feature_names = rule_features\n",
    "        print(f\"✅ 규칙 기반 특성 {len(rule_features)}개 생성 완료\")\n",
    "        \n",
    "        return df_enhanced\n",
    "    \n",
    "    \n",
    "    def get_enhanced_hyperparameters(self, gpu_available=False):\n",
    "        \"\"\"향상된 하이퍼파라미터 (불균형 데이터 고려)\"\"\"\n",
    "        \n",
    "        cpu_count = os.cpu_count() or 2\n",
    "        \n",
    "        if gpu_available:\n",
    "            hyperparameters = {\n",
    "                # XGBoost with class balancing\n",
    "                'XGB': [\n",
    "                    {\n",
    "                        'n_estimators': 500,\n",
    "                        'learning_rate': 0.05,\n",
    "                        'max_depth': 8,\n",
    "                        'tree_method': 'gpu_hist',\n",
    "                        'gpu_id': 0,\n",
    "                        'objective': 'binary:logistic',\n",
    "                        'eval_metric': 'auc',\n",
    "                        'scale_pos_weight': 0.62,  # 불균형 조정\n",
    "                        'subsample': 0.8,\n",
    "                        'colsample_bytree': 0.8\n",
    "                    }\n",
    "                ],\n",
    "                \n",
    "                # LightGBM with focal loss\n",
    "                'GBM': [\n",
    "                    {\n",
    "                        'num_boost_round': 500,\n",
    "                        'learning_rate': 0.05,\n",
    "                        'device': 'gpu',\n",
    "                        'objective': 'binary',\n",
    "                        'metric': 'auc',\n",
    "                        'is_unbalance': True,\n",
    "                        'feature_fraction': 0.8,\n",
    "                        'bagging_fraction': 0.8,\n",
    "                        'bagging_freq': 5\n",
    "                    }\n",
    "                ],\n",
    "                \n",
    "                # CatBoost with auto class balancing\n",
    "                'CAT': [\n",
    "                    {\n",
    "                        'iterations': 500,\n",
    "                        'learning_rate': 0.05,\n",
    "                        'depth': 8,\n",
    "                        'task_type': 'GPU',\n",
    "                        'devices': '0',\n",
    "                        'auto_class_weights': 'Balanced',\n",
    "                        'eval_metric': 'AUC',\n",
    "                        'bootstrap_type': 'Bernoulli',\n",
    "                        'subsample': 0.8\n",
    "                    }\n",
    "                ],\n",
    "                \n",
    "                # Neural Network\n",
    "                'NN_TORCH': [\n",
    "                    {\n",
    "                        'num_epochs': 200,\n",
    "                        'learning_rate': 0.001,\n",
    "                        'batch_size': 512,\n",
    "                        'activation': 'relu',\n",
    "                        'dropout_prob': 0.3,\n",
    "                        'weight_decay': 1e-4\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        else:\n",
    "            # CPU 최적화 버전\n",
    "            hyperparameters = {\n",
    "                'GBM': {\n",
    "                    'num_boost_round': 300,\n",
    "                    'learning_rate': 0.1,\n",
    "                    'is_unbalance': True,\n",
    "                    'metric': 'auc'\n",
    "                },\n",
    "                'CAT': {\n",
    "                    'iterations': 300,\n",
    "                    'learning_rate': 0.1,\n",
    "                    'auto_class_weights': 'Balanced'\n",
    "                },\n",
    "                'XGB': {\n",
    "                    'n_estimators': 300,\n",
    "                    'learning_rate': 0.1,\n",
    "                    'scale_pos_weight': 0.62,\n",
    "                    'eval_metric': 'auc'\n",
    "                },\n",
    "                'RF': {\n",
    "                    'n_estimators': 200,\n",
    "                    'max_depth': 15,\n",
    "                    'class_weight': 'balanced',\n",
    "                    'n_jobs': cpu_count\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return hyperparameters\n",
    "    \n",
    "    def train_enhanced_model(self, file_path, time_limit=1800, quality='high_quality'):\n",
    "        \"\"\"향상된 모델 훈련\"\"\"\n",
    "        print(\"🌟 향상된 선박 이상행동 탐지 모델 훈련\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # GPU 상태 확인\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        print(f\"🔍 GPU 사용 가능: {gpu_available}\")\n",
    "        \n",
    "        # 데이터 로드\n",
    "        print(\"📁 데이터 로딩...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"✅ 원본 데이터: {df.shape}\")\n",
    "        \n",
    "        # 타겟 전처리\n",
    "        if 'result' in df.columns:\n",
    "            if df['result'].dtype == 'object':\n",
    "                df['result'] = df['result'].map({'True': 1, 'False': 0})\n",
    "            elif df['result'].dtype == 'bool':\n",
    "                df['result'] = df['result'].astype(int)\n",
    "        \n",
    "        # 특성 엔지니어링 (훈련 모드)\n",
    "        df_final = self.create_rule_based_features(df, is_training=True)\n",
    "        #df_final = self.create_advanced_features(df_enhanced)\n",
    "        \n",
    "        print(f\"📊 최종 데이터: {df_final.shape}\")\n",
    "        \n",
    "        # 타겟 분포 확인\n",
    "        target_dist = df_final['result'].value_counts()\n",
    "        print(f\"📈 타겟 분포: {dict(target_dist)}\")\n",
    "        print(f\"📊 이상선박 비율: {target_dist[1]/len(df_final):.2%}\")\n",
    "        \n",
    "        # 특성과 타겟 분리\n",
    "        feature_columns = [col for col in df_final.columns if col not in ['result', 'MMSI']]\n",
    "        X = df_final[feature_columns]\n",
    "        y = df_final['result']\n",
    "        \n",
    "        print(f\"🔧 사용할 특성: {len(feature_columns)}개\")\n",
    "        \n",
    "        # 데이터 분할 (층화 추출)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"📊 훈련 데이터: {X_train.shape[0]:,}개\")\n",
    "        print(f\"📊 테스트 데이터: {X_test.shape[0]:,}개\")\n",
    "        \n",
    "        # 훈련 데이터 준비\n",
    "        train_data = X_train.copy()\n",
    "        train_data['target'] = y_train\n",
    "        \n",
    "        # 모델 경로 설정\n",
    "        model_path = './enhanced_ship_anomaly_models_b3'\n",
    "        if os.path.exists(model_path):\n",
    "            import shutil\n",
    "            shutil.rmtree(model_path)\n",
    "        \n",
    "        # AutoGluon 설정\n",
    "        predictor = TabularPredictor(\n",
    "            label='target',\n",
    "            problem_type='binary',\n",
    "            eval_metric='roc_auc',  # AUC로 변경 (불균형 데이터에 적합)\n",
    "            path=model_path,\n",
    "            verbosity=2\n",
    "        )\n",
    "        \n",
    "        # 하이퍼파라미터 설정\n",
    "        hyperparameters = self.get_enhanced_hyperparameters(gpu_available)\n",
    "        \n",
    "        # AG args 설정\n",
    "        ag_args_fit = {\n",
    "            'num_gpus': 1 if gpu_available else 0,\n",
    "            'num_cpus': os.cpu_count() or 2,\n",
    "            'auto_stack': True\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n⏳ 모델 훈련 시작... (제한시간: {time_limit//60}분)\")\n",
    "        start_time = pd.Timestamp.now()\n",
    "        \n",
    "        try:\n",
    "            predictor.fit(\n",
    "                train_data,\n",
    "                time_limit=time_limit,\n",
    "                presets=quality,\n",
    "                hyperparameters=hyperparameters,\n",
    "                ag_args_fit=ag_args_fit,\n",
    "                holdout_frac=0.15,  # 검증용\n",
    "                num_bag_folds=5,\n",
    "                num_bag_sets=1,\n",
    "                num_stack_levels=2,  # 스태킹 활용\n",
    "                verbosity=2,\n",
    "                dynamic_stacking=True,  # 동적 스태킹\n",
    "            )\n",
    "            \n",
    "            end_time = pd.Timestamp.now()\n",
    "            training_time = (end_time - start_time).total_seconds()\n",
    "            print(f\"✅ 훈련 완료! (소요시간: {training_time/60:.1f}분)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 훈련 중 오류: {e}\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        # GPU 메모리 정리\n",
    "        if gpu_available:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return predictor, X_test, y_test, training_time\n",
    "    \n",
    "    def comprehensive_evaluation(self, predictor, X_test, y_test, training_time):\n",
    "        \"\"\"포괄적 모델 평가\"\"\"\n",
    "        print(f\"\\n📈 포괄적 성능 평가\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # 예측 수행\n",
    "        start_time = pd.Timestamp.now()\n",
    "        y_pred = predictor.predict(X_test)\n",
    "        y_pred_proba = predictor.predict_proba(X_test)\n",
    "        end_time = pd.Timestamp.now()\n",
    "        \n",
    "        prediction_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        # 기본 성능 지표\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba.iloc[:, 1])\n",
    "        \n",
    "        print(f\"🎯 핵심 성능 지표:\")\n",
    "        print(f\"  정확도: {accuracy:.4f}\")\n",
    "        print(f\"  AUC 점수: {auc_score:.4f}\")\n",
    "        print(f\"  훈련 시간: {training_time/60:.1f}분\")\n",
    "        print(f\"  예측 시간: {prediction_time:.2f}초\")\n",
    "        print(f\"  예측 속도: {len(X_test)/prediction_time:.0f} 샘플/초\")\n",
    "        \n",
    "        # 상세 분류 리포트\n",
    "        print(f\"\\n📊 상세 분류 리포트:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=['정상', '이상']))\n",
    "        \n",
    "        # 혼동 행렬\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"\\n🔍 혼동 행렬:\")\n",
    "        print(f\"실제\\\\예측    정상    이상\")\n",
    "        print(f\"정상        {cm[0,0]:4d}   {cm[0,1]:4d}\")\n",
    "        print(f\"이상        {cm[1,0]:4d}   {cm[1,1]:4d}\")\n",
    "        \n",
    "        # 모델 리더보드\n",
    "        try:\n",
    "            leaderboard = predictor.leaderboard(silent=True)\n",
    "            print(f\"\\n🏆 모델 리더보드 (상위 5개):\")\n",
    "            top_models = leaderboard.head(5)\n",
    "            for idx, row in top_models.iterrows():\n",
    "                model_name = row['model']\n",
    "                score = row['score_val']\n",
    "                print(f\"  {model_name}: {score:.4f}\")\n",
    "        except:\n",
    "            leaderboard = pd.DataFrame()\n",
    "        \n",
    "        # 규칙 기반 vs ML 성능 비교\n",
    "        if 'rule_based_prediction' in X_test.columns:\n",
    "            rule_accuracy = accuracy_score(y_test, X_test['rule_based_prediction'])\n",
    "            print(f\"\\n⚖️ 규칙 기반 vs ML 비교:\")\n",
    "            print(f\"  규칙 기반 정확도: {rule_accuracy:.4f}\")\n",
    "            print(f\"  ML 모델 정확도: {accuracy:.4f}\")\n",
    "            print(f\"  개선율: {((accuracy - rule_accuracy) / rule_accuracy * 100):+.1f}%\")\n",
    "        \n",
    "        # 특성 중요도 (가능한 경우)\n",
    "        try:\n",
    "            feature_importance = predictor.feature_importance(X_test)\n",
    "            print(f\"\\n🔍 상위 10개 중요 특성:\")\n",
    "            top_features = feature_importance.head(10)\n",
    "            for feature, importance in top_features.items():\n",
    "                print(f\"  {feature}: {importance:.4f}\")\n",
    "        except:\n",
    "            print(f\"\\n⚠️ 특성 중요도 분석 불가\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc_score,\n",
    "            'training_time': training_time,\n",
    "            'prediction_time': prediction_time,\n",
    "            'leaderboard': leaderboard\n",
    "        }\n",
    "    \n",
    "    def hybrid_prediction(self, predictor, X_new):\n",
    "        \"\"\"하이브리드 예측 (규칙 + ML)\"\"\"\n",
    "        # ML 예측\n",
    "        ml_pred = predictor.predict(X_new)\n",
    "        ml_proba = predictor.predict_proba(X_new)\n",
    "        \n",
    "        # 규칙 기반 예측\n",
    "        if 'rule_based_prediction' in X_new.columns:\n",
    "            rule_pred = X_new['rule_based_prediction'].values\n",
    "            \n",
    "            # 하이브리드 예측: 규칙이 이상이면 이상, 아니면 ML 예측 사용\n",
    "            hybrid_pred = np.maximum(rule_pred, ml_pred)\n",
    "            \n",
    "            return hybrid_pred, ml_proba\n",
    "        else:\n",
    "            return ml_pred, ml_proba\n",
    "\n",
    "def train_ship_anomaly_model(file_path, time_limit=7200, quality='high_quality'):\n",
    "    \"\"\"모델 훈련 파이프라인\"\"\"\n",
    "    \n",
    "    detector = EnhancedShipAnomalyDetector()\n",
    "    \n",
    "    try:\n",
    "        # 모델 훈련\n",
    "        predictor, X_test, y_test, training_time = detector.train_enhanced_model(\n",
    "            file_path=file_path,\n",
    "            time_limit=time_limit,\n",
    "            quality=quality\n",
    "        )\n",
    "        \n",
    "        if predictor is None:\n",
    "            print(\"❌ 훈련 실패\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # 성능 평가\n",
    "        results = detector.comprehensive_evaluation(\n",
    "            predictor, X_test, y_test, training_time\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"🎉 훈련 파이프라인 완료!\")\n",
    "        print(f\"🎯 최종 AUC: {results['auc']:.4f}\")\n",
    "        print(f\"🎯 최종 정확도: {results['accuracy']:.4f}\")\n",
    "        print(f\"⏱️ 총 훈련 시간: {training_time/60:.1f}분\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return detector, predictor, results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❗ 훈련 파이프라인 오류: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def predict_test_data(test_file_path, model_path='./enhanced_ship_anomaly_models_b3', output_path='./test_predictions_b3.csv'):\n",
    "    \"\"\"학습된 모델로 test 데이터 예측\"\"\"\n",
    "    \n",
    "    print(\"🔮 Test 데이터 예측 시작\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 디텍터 인스턴스 생성\n",
    "    detector = EnhancedShipAnomalyDetector()\n",
    "    \n",
    "    try:\n",
    "        # 학습된 모델 로드\n",
    "        print(f\"📂 모델 로딩: {model_path}\")\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"모델 경로가 존재하지 않습니다: {model_path}\")\n",
    "        \n",
    "        predictor = TabularPredictor.load(model_path)\n",
    "        print(\"✅ 모델 로딩 완료\")\n",
    "        \n",
    "        # 테스트 데이터 로드\n",
    "        print(f\"📁 테스트 데이터 로딩: {test_file_path}\")\n",
    "        test_df = pd.read_csv(test_file_path)\n",
    "        print(f\"✅ 테스트 데이터 로딩 완료: {test_df.shape}\")\n",
    "        \n",
    "        # MMSI 보존\n",
    "        mmsi_column = test_df['MMSI'].copy() if 'MMSI' in test_df.columns else None\n",
    "        \n",
    "        # 동일한 특성 엔지니어링 적용 (테스트 모드)\n",
    "        print(\"⚙️ 특성 엔지니어링 적용...\")\n",
    "        test_final = detector.create_rule_based_features(test_df, is_training=False)\n",
    "        #test_final = detector.create_advanced_features(test_enhanced)\n",
    "        \n",
    "        # 예측용 특성 준비\n",
    "        feature_columns = [col for col in test_final.columns if col not in ['result', 'MMSI']]\n",
    "        X_test = test_final[feature_columns]\n",
    "        \n",
    "        print(f\"🔧 예측용 특성: {len(feature_columns)}개\")\n",
    "        print(f\"📊 예측할 데이터: {X_test.shape[0]:,}개\")\n",
    "        \n",
    "        # 하이브리드 예측 수행\n",
    "        print(\"🔮 예측 수행...\")\n",
    "        start_time = pd.Timestamp.now()\n",
    "        \n",
    "        hybrid_pred, ml_proba = detector.hybrid_prediction(predictor, X_test)\n",
    "        \n",
    "        end_time = pd.Timestamp.now()\n",
    "        prediction_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"✅ 예측 완료! (소요시간: {prediction_time:.2f}초)\")\n",
    "        print(f\"🚀 예측 속도: {len(X_test)/prediction_time:.0f} 샘플/초\")\n",
    "        \n",
    "        # 결과를 TRUE/FALSE로 변환\n",
    "        result_labels = ['False' if pred == 0 else 'True' for pred in hybrid_pred]\n",
    "        \n",
    "        # 결과 DataFrame 생성\n",
    "        result_df = pd.DataFrame()\n",
    "        \n",
    "        if mmsi_column is not None:\n",
    "            result_df['MMSI'] = mmsi_column\n",
    "        \n",
    "        result_df['result'] = result_labels\n",
    "        \n",
    "        # 예측 결과 통계\n",
    "        pred_counts = pd.Series(result_labels).value_counts()\n",
    "        print(f\"\\n📊 예측 결과 분포:\")\n",
    "        for label, count in pred_counts.items():\n",
    "            percentage = count / len(result_labels) * 100\n",
    "            print(f\"  {label}: {count:,}개 ({percentage:.1f}%)\")\n",
    "        \n",
    "        # 규칙 기반 vs 하이브리드 비교\n",
    "        if 'rule_based_prediction' in X_test.columns:\n",
    "            rule_anomalies = X_test['rule_based_prediction'].sum()\n",
    "            hybrid_anomalies = sum(hybrid_pred)\n",
    "            print(f\"\\n⚖️ 규칙 vs 하이브리드:\")\n",
    "            print(f\"  규칙 기반 이상: {rule_anomalies:,}개\")\n",
    "            print(f\"  하이브리드 이상: {hybrid_anomalies:,}개\")\n",
    "            print(f\"  ML 추가 탐지: {hybrid_anomalies - rule_anomalies:,}개\")\n",
    "        \n",
    "        # 결과 저장\n",
    "        result_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\n💾 결과 저장 완료: {output_path}\")\n",
    "        \n",
    "        # 샘플 결과 출력\n",
    "        print(f\"\\n📋 예측 결과 샘플 (상위 10개):\")\n",
    "        print(result_df.head(10).to_string(index=False))\n",
    "        \n",
    "        # 고신뢰도 이상 선박\n",
    "        high_confidence_anomalies = result_df[\n",
    "            (result_df['result'] == 'True') & \n",
    "            (result_df['confidence'] > 0.8)\n",
    "        ]\n",
    "        \n",
    "        if len(high_confidence_anomalies) > 0:\n",
    "            print(f\"\\n🚨 고신뢰도 이상 선박 ({len(high_confidence_anomalies)}개):\")\n",
    "            print(high_confidence_anomalies.head().to_string(index=False))\n",
    "        \n",
    "        print(f\"\\n🎉 예측 완료!\")\n",
    "        print(f\"📁 결과 파일: {output_path}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 예측 중 오류 발생: {e}\")\n",
    "        return None\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 완전한 선박 이상행동 탐지 시스템\")\n",
    "    print(\"훈련 → 평가 → 테스트 예측 파이프라인\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. 모델 훈련\n",
    "    print(\"\\n🔥 STEP 1: 모델 훈련\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    train_file_path = './train/merged_output.csv'  # 훈련 데이터 경로\n",
    "    \n",
    "    detector, predictor, results = train_ship_anomaly_model(\n",
    "        file_path=train_file_path,\n",
    "        time_limit=7200,  # 30분\n",
    "        quality='high_quality'\n",
    "    )\n",
    "    \n",
    "    if detector and predictor:\n",
    "        print(f\"\\n✅ 훈련 성공!\")\n",
    "        print(f\"💾 모델 저장 경로: ./enhanced_ship_anomaly_models_b2\")\n",
    "        print(f\"💾 규칙 가중치 저장: ./rule_weights.json\")\n",
    "        \n",
    "        # 2. 테스트 예측\n",
    "        print(\"\\n🔮 STEP 2: 테스트 데이터 예측\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        test_file_path = './20240701.csv'  # 테스트 데이터 경로\n",
    "        output_path = './test_predictions_basic_2.csv'  # 결과 저장 경로\n",
    "        \n",
    "        predictions = predict_test_data(\n",
    "            test_file_path=test_file_path,\n",
    "            model_path='./enhanced_ship_anomaly_models_b2',\n",
    "            output_path=output_path\n",
    "        )\n",
    "        \n",
    "        if predictions is not None:\n",
    "            print(f\"\\n🎉 전체 파이프라인 성공!\")\n",
    "            print(f\"📊 총 {len(predictions):,}개 선박 예측 완료\")\n",
    "            print(f\"💾 최종 결과: {output_path}\")\n",
    "        else:\n",
    "            print(\"❌ 테스트 예측 실패\")\n",
    "            \n",
    "    else:\n",
    "        print(\"❌ 훈련 실패\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "class ShipAnomalyPredictor:\n",
    "    \"\"\"선박 이상행동 탐지 시스템 - 예측 버전\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rule_weights = {}\n",
    "        self.rule_stats = {}\n",
    "        \n",
    "    def load_rule_weights(self, weights_path='./rule_weights.json'):\n",
    "        \"\"\"저장된 규칙 가중치를 로드\"\"\"\n",
    "        if os.path.exists(weights_path):\n",
    "            with open(weights_path, 'r') as f:\n",
    "                self.rule_weights = json.load(f)\n",
    "            print(f\"📂 규칙 가중치 로드: {weights_path}\")\n",
    "            print(\"🔧 로드된 가중치:\")\n",
    "            for rule, weight in self.rule_weights.items():\n",
    "                print(f\"  {rule}: {weight:.3f}\")\n",
    "            \n",
    "            # 통계 정보도 로드 (있다면)\n",
    "            stats_path = weights_path.replace('.json', '_stats.json')\n",
    "            if os.path.exists(stats_path):\n",
    "                with open(stats_path, 'r') as f:\n",
    "                    self.rule_stats = json.load(f)\n",
    "                print(f\"📊 규칙 통계도 로드됨\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(f\"⚠️ 가중치 파일을 찾을 수 없습니다: {weights_path}\")\n",
    "            print(\"기본 가중치를 사용합니다\")\n",
    "            return False\n",
    "\n",
    "    def create_rule_based_features(self, df, weights_path='./rule_weights.json'):\n",
    "        \"\"\"6가지 규칙 기반 특성 생성 (테스트용)\"\"\"\n",
    "        print(\"🔧 도메인 지식 기반 특성 엔지니어링...\")\n",
    "        \n",
    "        df_enhanced = df.copy()\n",
    "        \n",
    "        # 규칙 1: 외국국적 + 저속항해 + 비정박지\n",
    "        df_enhanced['rule1_foreign_slow_non_anchor'] = (\n",
    "            (df['is_korean_ship'] == 0) & \n",
    "            (df['anchor_zone_ratio'] < 0.8) & \n",
    "            ((df['low_speed_ratio'] > 0.7) | (df['avg_sog'] <= 5))\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 규칙 2: AIS OFF 이상 + 비정박지\n",
    "        df_enhanced['rule2_ais_off_non_anchor'] = (\n",
    "            (df['anchor_zone_ratio'] < 0.8) & \n",
    "            (df['off_events'] >= 3)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 규칙 3: 특수해역 다중진입\n",
    "        df_enhanced['rule3_special_zone_frequent'] = (\n",
    "            df['num_zone_entries'] > 50\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 규칙 4: 급변침 + 비정박지\n",
    "        df_enhanced['rule4_sharp_turn_non_anchor'] = (\n",
    "            (df['anchor_zone_ratio'] < 0.8) & \n",
    "            (df['sharp_turns'] >= 1)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 규칙 5: 저속 + 왕복항해\n",
    "        df_enhanced['rule5_slow_back_forth'] = (\n",
    "            (df['avg_sog'] <= 5) & \n",
    "            (df['back_forth_count'] >= 2)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # 규칙 6: 금지구역 진입\n",
    "        df_enhanced['rule6_restricted_zone'] = df['restricted_zone_flag']\n",
    "        \n",
    "        # 저장된 가중치 로드\n",
    "        loaded = self.load_rule_weights(weights_path)\n",
    "        if loaded:\n",
    "            rule_weights = self.rule_weights\n",
    "        else:\n",
    "            # 로드 실패 시 기본값 사용\n",
    "            rule_weights = {\n",
    "                'rule6_restricted_zone': 1.0,\n",
    "                'rule1_foreign_slow_non_anchor': 0.977,  # 실제 데이터 기반 업데이트된 값\n",
    "                'rule3_special_zone_frequent': 0.964,   # 실제 데이터 기반 업데이트된 값\n",
    "                'rule4_sharp_turn_non_anchor': 0.752,   # 실제 데이터 기반 업데이트된 값\n",
    "                'rule2_ais_off_non_anchor': 0.0,\n",
    "                'rule5_slow_back_forth': 0.0\n",
    "            }\n",
    "            self.rule_weights = rule_weights\n",
    "        \n",
    "        # 종합 규칙 점수 (실제 데이터 기반 가중합)\n",
    "        df_enhanced['rule_composite_score'] = 0\n",
    "        for rule, weight in rule_weights.items():\n",
    "            if rule in df_enhanced.columns:\n",
    "                df_enhanced['rule_composite_score'] += df_enhanced[rule] * weight\n",
    "        \n",
    "        # 규칙 기반 예측\n",
    "        df_enhanced['rule_based_prediction'] = (\n",
    "            (df_enhanced['rule1_foreign_slow_non_anchor'] == 1) |\n",
    "            (df_enhanced['rule2_ais_off_non_anchor'] == 1) |\n",
    "            (df_enhanced['rule3_special_zone_frequent'] == 1) |\n",
    "            (df_enhanced['rule4_sharp_turn_non_anchor'] == 1) |\n",
    "            (df_enhanced['rule5_slow_back_forth'] == 1) |\n",
    "            (df_enhanced['rule6_restricted_zone'] == 1)\n",
    "        ).astype(int)\n",
    "        \n",
    "        print(f\"✅ 규칙 기반 특성 8개 생성 완료\")\n",
    "        \n",
    "        return df_enhanced\n",
    "    \n",
    "    def hybrid_prediction(self, predictor, X_new):\n",
    "        \"\"\"하이브리드 예측 (규칙 + ML)\"\"\"\n",
    "        # ML 예측\n",
    "        ml_pred = predictor.predict(X_new)\n",
    "        ml_proba = predictor.predict_proba(X_new)\n",
    "        \n",
    "        # 규칙 기반 예측\n",
    "        if 'rule_based_prediction' in X_new.columns:\n",
    "            rule_pred = X_new['rule_based_prediction'].values\n",
    "            \n",
    "            # 하이브리드 예측: 규칙이 이상이면 이상, 아니면 ML 예측 사용\n",
    "            hybrid_pred = np.maximum(rule_pred, ml_pred)\n",
    "            \n",
    "            return hybrid_pred, ml_proba\n",
    "        else:\n",
    "            return ml_pred, ml_proba\n",
    "\n",
    "def predict_test_data(test_file_path, model_path='./enhanced_ship_anomaly_models_b2', output_path='./test.csv'):\n",
    "    \"\"\"학습된 모델로 test 데이터 예측\"\"\"\n",
    "    \n",
    "    print(\"🔮 Test 데이터 예측 시작\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 예측기 인스턴스 생성\n",
    "    predictor_system = ShipAnomalyPredictor()\n",
    "    \n",
    "    try:\n",
    "        # 학습된 모델 로드\n",
    "        print(f\"📂 모델 로딩: {model_path}\")\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"모델 경로가 존재하지 않습니다: {model_path}\")\n",
    "        \n",
    "        predictor = TabularPredictor.load(model_path)\n",
    "        print(\"✅ 모델 로딩 완료\")\n",
    "        \n",
    "        # 테스트 데이터 로드\n",
    "        print(f\"📁 테스트 데이터 로딩: {test_file_path}\")\n",
    "        test_df = pd.read_csv(test_file_path)\n",
    "        print(f\"✅ 테스트 데이터 로딩 완료: {test_df.shape}\")\n",
    "        \n",
    "        # MMSI 보존\n",
    "        mmsi_column = test_df['MMSI'].copy() if 'MMSI' in test_df.columns else None\n",
    "        \n",
    "        # 동일한 특성 엔지니어링 적용 (테스트 모드)\n",
    "        print(\"⚙️ 특성 엔지니어링 적용...\")\n",
    "        test_final = predictor_system.create_rule_based_features(test_df)\n",
    "        \n",
    "        # 예측용 특성 준비\n",
    "        feature_columns = [col for col in test_final.columns if col not in ['result', 'MMSI']]\n",
    "        X_test = test_final[feature_columns]\n",
    "        \n",
    "        print(f\"🔧 예측용 특성: {len(feature_columns)}개\")\n",
    "        print(f\"📊 예측할 데이터: {X_test.shape[0]:,}개\")\n",
    "        \n",
    "        # 하이브리드 예측 수행\n",
    "        print(\"🔮 예측 수행...\")\n",
    "        start_time = pd.Timestamp.now()\n",
    "        \n",
    "        hybrid_pred, ml_proba = predictor_system.hybrid_prediction(predictor, X_test)\n",
    "        \n",
    "        end_time = pd.Timestamp.now()\n",
    "        prediction_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"✅ 예측 완료! (소요시간: {prediction_time:.2f}초)\")\n",
    "        print(f\"🚀 예측 속도: {len(X_test)/prediction_time:.0f} 샘플/초\")\n",
    "        \n",
    "        # 결과를 TRUE/FALSE로 변환\n",
    "        result_labels = ['False' if pred == 0 else 'True' for pred in hybrid_pred]\n",
    "        \n",
    "        # 결과 DataFrame 생성\n",
    "        result_df = pd.DataFrame()\n",
    "        \n",
    "        if mmsi_column is not None:\n",
    "            result_df['MMSI'] = mmsi_column\n",
    "        \n",
    "        result_df['result'] = result_labels\n",
    "        \n",
    "        # 예측 결과 통계\n",
    "        pred_counts = pd.Series(result_labels).value_counts()\n",
    "        print(f\"\\n📊 예측 결과 분포:\")\n",
    "        for label, count in pred_counts.items():\n",
    "            percentage = count / len(result_labels) * 100\n",
    "            print(f\"  {label}: {count:,}개 ({percentage:.1f}%)\")\n",
    "        \n",
    "        # 규칙 기반 vs 하이브리드 비교\n",
    "        if 'rule_based_prediction' in X_test.columns:\n",
    "            rule_anomalies = X_test['rule_based_prediction'].sum()\n",
    "            hybrid_anomalies = sum(hybrid_pred)\n",
    "            print(f\"\\n⚖️ 규칙 vs 하이브리드:\")\n",
    "            print(f\"  규칙 기반 이상: {rule_anomalies:,}개\")\n",
    "            print(f\"  하이브리드 이상: {hybrid_anomalies:,}개\")\n",
    "            print(f\"  ML 추가 탐지: {hybrid_anomalies - rule_anomalies:,}개\")\n",
    "        \n",
    "        # 결과 저장\n",
    "        result_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\n💾 결과 저장 완료: {output_path}\")\n",
    "        \n",
    "        # 샘플 결과 출력\n",
    "        print(f\"\\n📋 예측 결과 샘플 (상위 10개):\")\n",
    "        print(result_df.head(10).to_string(index=False))\n",
    "        \n",
    "        # 규칙별 상세 분석\n",
    "        print(f\"\\n🔍 규칙별 탐지 현황:\")\n",
    "        for rule_name in ['rule1_foreign_slow_non_anchor', 'rule3_special_zone_frequent', \n",
    "                         'rule4_sharp_turn_non_anchor', 'rule6_restricted_zone']:\n",
    "            if rule_name in X_test.columns:\n",
    "                count = X_test[rule_name].sum()\n",
    "                print(f\"  {rule_name}: {count}개\")\n",
    "        \n",
    "        print(f\"\\n🎉 예측 완료!\")\n",
    "        print(f\"📁 결과 파일: {output_path}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 예측 중 오류 발생: {e}\")\n",
    "        return None\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 선박 이상행동 탐지 시스템 - 테스트 예측\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 필수 파일 존재 확인\n",
    "    model_path = './enhanced_ship_anomaly_models_b2'\n",
    "    weights_path = './rule_weights.json'\n",
    "    \n",
    "    print(\"📋 필수 파일 확인:\")\n",
    "    print(f\"  모델 폴더: {'✅' if os.path.exists(model_path) else '❌'} {model_path}\")\n",
    "    print(f\"  가중치 파일: {'✅' if os.path.exists(weights_path) else '❌'} {weights_path}\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"\\n❌ 오류: 훈련된 모델이 없습니다!\")\n",
    "        print(\"먼저 train.py를 실행하여 모델을 훈련하세요.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 테스트 파일 경로 설정\n",
    "    test_file_path = './20240701.csv'  # 실제 테스트 데이터 경로로 변경\n",
    "    output_path = './test_predictions_final.csv'  # 결과 저장 경로\n",
    "    \n",
    "    print(f\"\\n📁 테스트 데이터: {test_file_path}\")\n",
    "    print(f\"💾 결과 저장 경로: {output_path}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # 테스트 데이터 예측 실행\n",
    "    predictions = predict_test_data(\n",
    "        test_file_path=test_file_path,\n",
    "        model_path=model_path,\n",
    "        output_path=output_path\n",
    "    )\n",
    "    \n",
    "    if predictions is not None:\n",
    "        print(f\"\\n🎉 전체 예측 완료!\")\n",
    "        print(f\"📊 총 {len(predictions):,}개 선박 예측 완료\")\n",
    "        print(f\"💾 최종 결과: {output_path}\")\n",
    "        \n",
    "        # 요약 통계\n",
    "        anomaly_count = (predictions['result'] == 'True').sum()\n",
    "        total_count = len(predictions)\n",
    "        anomaly_rate = anomaly_count / total_count * 100\n",
    "        \n",
    "        print(f\"\\n📈 최종 요약:\")\n",
    "        print(f\"  전체 선박: {total_count:,}개\")\n",
    "        print(f\"  이상 선박: {anomaly_count:,}개 ({anomaly_rate:.1f}%)\")\n",
    "        print(f\"  정상 선박: {total_count - anomaly_count:,}개 ({100-anomaly_rate:.1f}%)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ 예측 실패\")\n",
    "        print(\"⚠️ 테스트 데이터 경로와 설정을 확인해주세요\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
