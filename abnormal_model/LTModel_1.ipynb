{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ ì™„ì „í•œ ì„ ë°• ì´ìƒí–‰ë™ íƒì§€ ì‹œìŠ¤í…œ\n",
      "í›ˆë ¨ â†’ í‰ê°€ â†’ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸\n",
      "============================================================\n",
      "\n",
      "ðŸ”¥ STEP 1: ëª¨ë¸ í›ˆë ¨\n",
      "------------------------------\n",
      "ðŸŒŸ í–¥ìƒëœ ì„ ë°• ì´ìƒí–‰ë™ íƒì§€ ëª¨ë¸ í›ˆë ¨\n",
      "============================================================\n",
      "ðŸ” GPU ì‚¬ìš© ê°€ëŠ¥: True\n",
      "ðŸ“ ë°ì´í„° ë¡œë”©...\n",
      "âœ… ì›ë³¸ ë°ì´í„°: (2678129, 27)\n",
      "ðŸ”§ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§...\n",
      "ðŸ“Š ì‹¤ì œ ë°ì´í„°ì—ì„œ ê·œì¹™ ê°€ì¤‘ì¹˜ ê³„ì‚°...\n",
      "  rule1_foreign_slow_non_anchor: 217726ê±´ ì¤‘ 213711ê±´ ì´ìƒ â†’ ì •í™•ë„ 0.982\n",
      "  rule2_ais_off_non_anchor: í•´ë‹¹ ì¼€ì´ìŠ¤ ì—†ìŒ â†’ ê°€ì¤‘ì¹˜ 0.0\n",
      "  rule3_special_zone_frequent: 137666ê±´ ì¤‘ 130756ê±´ ì´ìƒ â†’ ì •í™•ë„ 0.950\n",
      "  rule4_sharp_turn_non_anchor: 55536ê±´ ì¤‘ 39009ê±´ ì´ìƒ â†’ ì •í™•ë„ 0.702\n",
      "  rule5_slow_back_forth: í•´ë‹¹ ì¼€ì´ìŠ¤ ì—†ìŒ â†’ ê°€ì¤‘ì¹˜ 0.0\n",
      "  rule6_restricted_zone: 584007ê±´ ì¤‘ 584007ê±´ ì´ìƒ â†’ ì •í™•ë„ 1.000\n",
      "âœ… ê·œì¹™ ê°€ì¤‘ì¹˜ ê³„ì‚° ì™„ë£Œ\n",
      "ðŸ’¾ ê·œì¹™ ê°€ì¤‘ì¹˜ ì €ìž¥: ./rule_weights.json\n",
      "ðŸ“Š ê·œì¹™ í†µê³„ ì €ìž¥: ./rule_weights_stats.json\n",
      "âœ… ê·œì¹™ ê¸°ë°˜ íŠ¹ì„± 8ê°œ ìƒì„± ì™„ë£Œ\n",
      "ðŸ“Š ìµœì¢… ë°ì´í„°: (2678129, 35)\n",
      "ðŸ“ˆ íƒ€ê²Ÿ ë¶„í¬: {1: 1633217, 0: 1044912}\n",
      "ðŸ“Š ì´ìƒì„ ë°• ë¹„ìœ¨: 60.98%\n",
      "ðŸ”§ ì‚¬ìš©í•  íŠ¹ì„±: 33ê°œ\n",
      "ðŸ“Š í›ˆë ¨ ë°ì´í„°: 2,142,503ê°œ\n",
      "ðŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: 535,626ê°œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.3.1\n",
      "Python Version:     3.12.7\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #107-Ubuntu SMP Wed Feb 7 13:26:48 UTC 2024\n",
      "CPU Count:          2\n",
      "Memory Avail:       15.19 GB / 24.00 GB (63.3%)\n",
      "Disk Space Avail:   449.89 GB / 511.75 GB (87.9%)\n",
      "===================================================\n",
      "Presets specified: ['high_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=2, num_bag_folds=5, num_bag_sets=1\n",
      "Note: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~5x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n",
      "\tYou can avoid this risk by setting `save_bag_folds=True`.\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1800s of the 7200s of remaining time (25%).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â³ ëª¨ë¸ í›ˆë ¨ ì‹œìž‘... (ì œí•œì‹œê°„: 120ë¶„)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 02:58:28,661\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.8.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-05-29 02:58:33,053\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/home/elicer/ai/enhanced_ship_anomaly_models_b3/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Beginning AutoGluon training ... Time limit = 1793s\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m AutoGluon will save models to \"/home/elicer/ai/enhanced_ship_anomaly_models_b3/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Train Data Rows:    1904447\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Train Data Columns: 33\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Label Column:       target\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tAvailable Memory:                    14447.25 MB\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTrain Data (Original)  Memory Usage: 479.48 MB (3.3% of available memory)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t\tNote: Converting 10 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tUseless Original Features (Count: 2): ['rule2_ais_off_non_anchor', 'rule5_slow_back_forth']\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tUnused Original Features (Count: 1): ['rule6_restricted_zone']\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t('int', []) : 1 | ['rule6_restricted_zone']\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t('float', []) : 18 | ['trajectory_duration', 'avg_sog', 'std_sog', 'max_sog', 'min_sog', ...]\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t('int', [])   : 12 | ['is_korean_ship', 'num_points', 'sharp_turns', 'num_low_speed_periods', 'off_events', ...]\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t('float', [])     : 18 | ['trajectory_duration', 'avg_sog', 'std_sog', 'max_sog', 'min_sog', ...]\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t('int', [])       :  3 | ['num_points', 'num_low_speed_periods', 'num_zone_entries']\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t('int', ['bool']) :  9 | ['is_korean_ship', 'sharp_turns', 'off_events', 'restricted_zone_flag', 'back_forth_count', ...]\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t16.3s = Fit runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t30 features in original data used to generate 30 features in processed data.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTrain Data (Processed) Memory Usage: 321.47 MB (2.2% of available memory)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Data preprocessing and feature engineering runtime = 17.23s ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t'XGB': [{'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 8, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'objective': 'binary:logistic', 'eval_metric': 'auc', 'scale_pos_weight': 0.62, 'subsample': 0.8, 'colsample_bytree': 0.8}],\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t'GBM': [{'num_boost_round': 500, 'learning_rate': 0.05, 'device': 'gpu', 'objective': 'binary', 'metric': 'auc', 'is_unbalance': True, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}],\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t'CAT': [{'iterations': 500, 'learning_rate': 0.05, 'depth': 8, 'task_type': 'GPU', 'devices': '0', 'auto_class_weights': 'Balanced', 'eval_metric': 'AUC', 'bootstrap_type': 'Bernoulli', 'subsample': 0.8}],\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t'NN_TORCH': [{'num_epochs': 200, 'learning_rate': 0.001, 'batch_size': 512, 'activation': 'relu', 'dropout_prob': 0.3, 'weight_decay': 0.0001}],\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m AutoGluon will fit 3 stack levels (L1 to L3) ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting 4 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 789.17s of the 1776.07s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=15.75%)\n",
      "\u001b[36m(_ray_fit pid=4125174)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4125174)\u001b[0m \tRan out of time, early stopping on iteration 176. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4125174)\u001b[0m \t[176]\tvalid_set's binary_logloss: 0.026618\tvalid_set's auc: 0.999711\n",
      "\u001b[36m(_ray_fit pid=4126149)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4126149)\u001b[0m \tRan out of time, early stopping on iteration 277. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4126149)\u001b[0m \t[277]\tvalid_set's binary_logloss: 0.0208105\tvalid_set's auc: 0.999805\n",
      "\u001b[36m(_ray_fit pid=4127145)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4127145)\u001b[0m \tRan out of time, early stopping on iteration 373. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4127145)\u001b[0m \t[373]\tvalid_set's binary_logloss: 0.0177185\tvalid_set's auc: 0.999856\n",
      "\u001b[36m(_ray_fit pid=4128148)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4128148)\u001b[0m \tRan out of time, early stopping on iteration 377. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4128148)\u001b[0m \t[377]\tvalid_set's binary_logloss: 0.0183819\tvalid_set's auc: 0.999845\n",
      "\u001b[36m(_ray_fit pid=4129156)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4129156)\u001b[0m \tRan out of time, early stopping on iteration 375. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4129156)\u001b[0m \t[375]\tvalid_set's binary_logloss: 0.0182184\tvalid_set's auc: 0.999847\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9998\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t663.04s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t29.88s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 105.11s of the 1092.01s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 18.82% memory usage per fold, 75.29%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=18.82%)\n",
      "\u001b[36m(_ray_fit pid=4130296)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4130296)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130296)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130420)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4130420)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130420)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130538)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4130538)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130538)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130653)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4130653)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130653)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130772)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4130772)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4130772)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9888\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t49.17s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.28s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 47.97s of the 1034.87s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.85% memory usage per fold, 41.70%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=20.85%)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=4131033, ip=10.0.2.100)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                 ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise processed_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                                                                                                                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m ray.exceptions.RayTaskError(UnboundLocalError): \u001b[36mray::_ray_fit()\u001b[39m (pid=4131033, ip=10.0.2.100)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                 ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 36.51s of the 1023.41s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=14.60%)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1000.05s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9998\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t10.7s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.39s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting 4 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 659.08s of the 988.73s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.98% memory usage per fold, 67.90%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=16.98%)\n",
      "\u001b[36m(_ray_fit pid=4131651)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4132307)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4132898)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4133432)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4133967)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4133967)\u001b[0m \tRan out of time, early stopping on iteration 412. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4133967)\u001b[0m \t[397]\tvalid_set's binary_logloss: 0.0142428\tvalid_set's auc: 0.999851\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9999\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t426.55s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t10.75s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 220.98s of the 550.63s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 20.17% memory usage per fold, 40.33%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=20.17%)\n",
      "\u001b[36m(_ray_fit pid=4134865)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4134865)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4134865)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4134971)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4134971)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4134971)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4135090)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4135090)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4135090)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4135211)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4135211)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4135211)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4135328)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4135328)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_ray_fit pid=4135328)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9997\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t51.34s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.64s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: XGBoost_BAG_L2 ... Training model for up to 161.95s of the 491.60s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 22.48% memory usage per fold, 44.96%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=22.48%)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=4135578, ip=10.0.2.100)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                 ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise processed_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                                                                                                                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m ray.exceptions.RayTaskError(UnboundLocalError): \u001b[36mray::_ray_fit()\u001b[39m (pid=4135578, ip=10.0.2.100)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                 ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 150.96s of the 480.61s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=15.63%)\n",
      "\u001b[36m(_ray_fit pid=4135779)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\u001b[36m(_ray_fit pid=4136003)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\u001b[36m(_ray_fit pid=4136200)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\u001b[36m(_ray_fit pid=4136422)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\u001b[36m(_ray_fit pid=4136666)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9998\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t132.63s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t7.94s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 339.34s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L2': 0.889, 'NeuralNetTorch_BAG_L2': 0.111}\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9999\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t15.98s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.37s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting 4 L3 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: LightGBM_BAG_L3 ... Training model for up to 322.87s of the 322.73s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 17.77% memory usage per fold, 71.07%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=17.77%)\n",
      "\u001b[36m(_ray_fit pid=4137157)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4137157)\u001b[0m \tRan out of time, early stopping on iteration 193. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4137157)\u001b[0m \t[174]\tvalid_set's binary_logloss: 0.0145792\tvalid_set's auc: 0.999861\n",
      "\u001b[36m(_ray_fit pid=4137552)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4137552)\u001b[0m \tRan out of time, early stopping on iteration 192. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4137552)\u001b[0m \t[192]\tvalid_set's binary_logloss: 0.0142814\tvalid_set's auc: 0.99986\n",
      "\u001b[36m(_ray_fit pid=4137989)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4137989)\u001b[0m \tRan out of time, early stopping on iteration 192. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4137989)\u001b[0m \t[191]\tvalid_set's binary_logloss: 0.0140887\tvalid_set's auc: 0.999878\n",
      "\u001b[36m(_ray_fit pid=4138414)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4138414)\u001b[0m \tRan out of time, early stopping on iteration 196. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4138414)\u001b[0m \t[196]\tvalid_set's binary_logloss: 0.0138516\tvalid_set's auc: 0.999879\n",
      "\u001b[36m(_ray_fit pid=4138845)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_ray_fit pid=4138845)\u001b[0m \tRan out of time, early stopping on iteration 192. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=4138845)\u001b[0m \t[192]\tvalid_set's binary_logloss: 0.0136773\tvalid_set's auc: 0.99987\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9999\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t273.46s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t7.22s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: CatBoost_BAG_L3 ... Training model for up to 40.63s of the 40.49s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 21.08% memory usage per fold, 42.16%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=21.08%)\n",
      "\u001b[36m(_ray_fit pid=4139426)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_ray_fit pid=4139426)\u001b[0m Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return pickle.loads(ray_exception.serialized_exception)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m ModuleNotFoundError: No module named '_catboost'\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return RayError.from_bytes(obj)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return RayError.from_ray_exception(ray_exception)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise RuntimeError(msg) from e\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m RuntimeError: Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tWarning: Exception caused CatBoost_BAG_L3 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tSystem error: Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m traceback: Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return pickle.loads(ray_exception.serialized_exception)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m ModuleNotFoundError: No module named '_catboost'\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return RayError.from_bytes(obj)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return RayError.from_ray_exception(ray_exception)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise RuntimeError(msg) from e\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m RuntimeError: Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise processed_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                                                                                                                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 931, in get_objects\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise value\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m ray.exceptions.RaySystemError: System error: Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m traceback: Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 51, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return pickle.loads(ray_exception.serialized_exception)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m ModuleNotFoundError: No module named '_catboost'\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/serialization.py\", line 460, in deserialize_objects\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     obj = self._deserialize_object(data, metadata, object_ref)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/serialization.py\", line 342, in _deserialize_object\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return RayError.from_bytes(obj)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 45, in from_bytes\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return RayError.from_ray_exception(ray_exception)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/exceptions.py\", line 54, in from_ray_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise RuntimeError(msg) from e\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m RuntimeError: Failed to unpickle serialized exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: XGBoost_BAG_L3 ... Training model for up to 27.72s of the 27.58s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 23.11% memory usage per fold, 46.22%/80.00% total).\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=23.11%)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tWarning: Exception caused XGBoost_BAG_L3 to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=4139668, ip=10.0.2.100)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                 ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._fit_folds(\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_fitting_strategy.after_all_folds_scheduled()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise processed_exception\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                                                                                                                      ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return fn(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m ray.exceptions.RayTaskError(UnboundLocalError): \u001b[36mray::_ray_fit()\u001b[39m (pid=4139668, ip=10.0.2.100)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m                 ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: NeuralNetTorch_BAG_L3 ... Training model for up to 16.57s of the 16.43s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=15.98%)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTime limit exceeded... Skipping NeuralNetTorch_BAG_L3.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: WeightedEnsemble_L4 ... Training model for up to 360.00s of the -7.37s of remaining time.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L3': 0.667, 'LightGBM_BAG_L2': 0.333}\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.9999\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t30.56s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t0.36s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m AutoGluon training complete, total runtime = 1832.4s ... Best model: WeightedEnsemble_L4 | Estimated inference throughput: 6708.1 rows/s (380890 batch size)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t103.03s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tWarning: Exception caused CatBoost_BAG_L1_FULL to fail during training... Skipping this model.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \t\tcatboost/cuda/cuda_lib/cuda_base.h:281: CUDA error 100: no CUDA-capable device is detected\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Detailed Traceback:\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = self._train_single(**model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 365, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._fit_single(\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 629, in _fit_single\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     model_base.fit(X=X_fit, y=y_fit, time_limit=time_limit, **kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     out = self._fit(**kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m           ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 261, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self.model.fit(X, **fit_final_kwargs)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/catboost/core.py\", line 5245, in fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/catboost/core.py\", line 2410, in _fit\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._train(\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"/home/elicer/anaconda3/lib/python3.12/site-packages/catboost/core.py\", line 1790, in _train\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m     self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"_catboost.pyx\", line 5023, in _catboost._CatBoost._train\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m   File \"_catboost.pyx\", line 5072, in _catboost._CatBoost._train\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m _catboost.CatBoostError: catboost/cuda/cuda_lib/cuda_base.h:281: CUDA error 100: no CUDA-capable device is detected\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m WARNING: Refit training failure detected for 'CatBoost_BAG_L1'... Falling back to using first fold to avoid downstream exception.\n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tThis is likely due to an out-of-memory error or other memory related issue. \n",
      "\u001b[36m(_dystack pid=4124654)\u001b[0m \tPlease create a GitHub issue if this was triggered from a non-memory related problem.\n",
      "Warning: Exception encountered during DyStack sub-fit:\n",
      "\tCannot avoid training failure during refit for 'CatBoost_BAG_L1' by falling back to copying the first fold because it does not exist! (save_bag_folds=False)\n",
      "\tPlease specify `save_bag_folds=True` in the `.fit` call to avoid this exception.\n",
      "\t2\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1956s\t = DyStack   runtime |\t5244s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=2.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=2)`\n",
      "Beginning AutoGluon training ... Time limit = 5244s\n",
      "AutoGluon will save models to \"/home/elicer/ai/enhanced_ship_anomaly_models_b3\"\n",
      "Train Data Rows:    2142503\n",
      "Train Data Columns: 33\n",
      "Label Column:       target\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    15146.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 539.42 MB (3.6% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 10 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 2): ['rule2_ais_off_non_anchor', 'rule5_slow_back_forth']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 1): ['rule6_restricted_zone']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 1 | ['rule6_restricted_zone']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 18 | ['trajectory_duration', 'avg_sog', 'std_sog', 'max_sog', 'min_sog', ...]\n",
      "\t\t('int', [])   : 12 | ['is_korean_ship', 'num_points', 'sharp_turns', 'num_low_speed_periods', 'off_events', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 18 | ['trajectory_duration', 'avg_sog', 'std_sog', 'max_sog', 'min_sog', ...]\n",
      "\t\t('int', [])       :  3 | ['num_points', 'num_low_speed_periods', 'num_zone_entries']\n",
      "\t\t('int', ['bool']) :  9 | ['is_korean_ship', 'sharp_turns', 'off_events', 'restricted_zone_flag', 'back_forth_count', ...]\n",
      "\t9.6s = Fit runtime\n",
      "\t30 features in original data used to generate 30 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 361.66 MB (2.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 10.49s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'XGB': [{'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 8, 'tree_method': 'gpu_hist', 'gpu_id': 0, 'objective': 'binary:logistic', 'eval_metric': 'auc', 'scale_pos_weight': 0.62, 'subsample': 0.8, 'colsample_bytree': 0.8}],\n",
      "\t'GBM': [{'num_boost_round': 500, 'learning_rate': 0.05, 'device': 'gpu', 'objective': 'binary', 'metric': 'auc', 'is_unbalance': True, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 5}],\n",
      "\t'CAT': [{'iterations': 500, 'learning_rate': 0.05, 'depth': 8, 'task_type': 'GPU', 'devices': '0', 'auto_class_weights': 'Balanced', 'eval_metric': 'AUC', 'bootstrap_type': 'Bernoulli', 'subsample': 0.8}],\n",
      "\t'NN_TORCH': [{'num_epochs': 200, 'learning_rate': 0.001, 'batch_size': 512, 'activation': 'relu', 'dropout_prob': 0.3, 'weight_decay': 0.0001}],\n",
      "}\n",
      "AutoGluon will fit 3 stack levels (L1 to L3) ...\n",
      "Fitting 4 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2325.50s of the 5233.69s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.94% memory usage per fold, 67.76%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=16.94%)\n",
      "\t0.9999\t = Validation score   (roc_auc)\n",
      "\t1439.43s\t = Training   runtime\n",
      "\t70.75s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 861.37s of the 3769.56s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 31.85% memory usage per fold, 63.70%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=31.85%)\n",
      "\t0.9972\t = Validation score   (roc_auc)\n",
      "\t120.67s\t = Training   runtime\n",
      "\t1.13s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 729.05s of the 3637.23s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 35.90% memory usage per fold, 71.81%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=35.90%)\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=4155351, ip=10.0.2.100)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "    metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 690, in after_all_folds_scheduled\n",
      "    self._run_parallel(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 631, in _run_parallel\n",
      "    self._process_fold_results(finished, unfinished, fold_ctx)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(UnboundLocalError): \u001b[36mray::_ray_fit()\u001b[39m (pid=4155351, ip=10.0.2.100)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "    metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 707.64s of the 3615.83s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 25.14% memory usage per fold, 50.28%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=25.14%)\n",
      "\t0.9986\t = Validation score   (roc_auc)\n",
      "\t593.23s\t = Training   runtime\n",
      "\t19.71s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 3003.39s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t0.9999\t = Validation score   (roc_auc)\n",
      "\t19.66s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Fitting 4 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 1988.15s of the 2982.75s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 30.33% memory usage per fold, 60.66%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=30.33%)\n",
      "\t0.9999\t = Validation score   (roc_auc)\n",
      "\t1379.45s\t = Training   runtime\n",
      "\t34.95s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 594.61s of the 1589.21s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 35.22% memory usage per fold, 70.43%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=35.22%)\n",
      "\t0.9998\t = Validation score   (roc_auc)\n",
      "\t111.4s\t = Training   runtime\n",
      "\t1.21s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 470.41s of the 1465.01s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 40.09% memory usage per fold, 40.09%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=40.09%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=4172176, ip=10.0.2.100)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "    metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 688, in after_all_folds_scheduled\n",
      "    self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in _run_pseudo_sequential\n",
      "    self._process_fold_results(finished[0], unfinished, fold_ctx)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(UnboundLocalError): \u001b[36mray::_ray_fit()\u001b[39m (pid=4172176, ip=10.0.2.100)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "    metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 448.98s of the 1443.58s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 27.95% memory usage per fold, 55.91%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=27.95%)\n",
      "\t0.9998\t = Validation score   (roc_auc)\n",
      "\t380.1s\t = Training   runtime\n",
      "\t16.19s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 1044.61s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L2': 0.917, 'NeuralNetTorch_BAG_L2': 0.083}\n",
      "\t0.9999\t = Validation score   (roc_auc)\n",
      "\t19.86s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting 4 L3 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L3 ... Training model for up to 1024.09s of the 1023.91s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 31.57% memory usage per fold, 63.13%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=31.57%)\n",
      "\t0.9999\t = Validation score   (roc_auc)\n",
      "\t826.74s\t = Training   runtime\n",
      "\t15.22s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L3 ... Training model for up to 183.74s of the 183.56s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 36.67% memory usage per fold, 73.35%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=36.67%)\n",
      "\t0.9996\t = Validation score   (roc_auc)\n",
      "\t99.04s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L3 ... Training model for up to 72.25s of the 72.07s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 43.10% memory usage per fold, 43.10%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=43.10%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "\tWarning: Exception caused XGBoost_BAG_L3 to fail during training... Skipping this model.\n",
      "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=4183423, ip=10.0.2.100)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "    metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "Detailed Traceback:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2169, in _train_and_save\n",
      "    model = self._train_single(**model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 2055, in _train_single\n",
      "    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, X_test=X_test, y_test=y_test, total_resources=total_resources, **model_fit_kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 270, in _fit\n",
      "    return super()._fit(X=X, y=y, time_limit=time_limit, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 390, in _fit\n",
      "    self._fit_folds(\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 848, in _fit_folds\n",
      "    fold_fitting_strategy.after_all_folds_scheduled()\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 688, in after_all_folds_scheduled\n",
      "    self._run_pseudo_sequential(X, y, X_pseudo, y_pseudo, model_base_ref, time_limit_fold, head_node_id)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 666, in _run_pseudo_sequential\n",
      "    self._process_fold_results(finished[0], unfinished, fold_ctx)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 587, in _process_fold_results\n",
      "    raise processed_exception\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 550, in _process_fold_results\n",
      "    fold_model, pred_proba, time_start_fit, time_end_fit, predict_time, predict_1_time, predict_n_size, fit_num_cpus, fit_num_gpus = self.ray.get(finished)\n",
      "                                                                                                                                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(UnboundLocalError): \u001b[36mray::_ray_fit()\u001b[39m (pid=4183423, ip=10.0.2.100)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 413, in _ray_fit\n",
      "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 1051, in fit\n",
      "    out = self._fit(**kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/elicer/anaconda3/lib/python3.12/site-packages/autogluon/tabular/models/xgboost/xgboost_model.py\", line 171, in _fit\n",
      "    metric_name=eval_metric_name,  # forces stopping_metric rather than arbitrary last metric\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'eval_metric_name' where it is not associated with a value\n",
      "Fitting model: NeuralNetTorch_BAG_L3 ... Training model for up to 49.01s of the 48.83s of remaining time.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 2 folds in parallel instead (Estimated 27.98% memory usage per fold, 55.95%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1 workers, per: cpus=2, gpus=1, memory=27.98%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L3.\n",
      "Fitting model: WeightedEnsemble_L4 ... Training model for up to 360.00s of the 2.95s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L2': 0.444, 'LightGBM_BAG_L3': 0.444, 'LightGBM_BAG_L1': 0.111}\n",
      "\t0.9999\t = Validation score   (roc_auc)\n",
      "\t52.39s\t = Training   runtime\n",
      "\t0.77s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5295.07s ... Best model: WeightedEnsemble_L4 | Estimated inference throughput: 2689.8 rows/s (428501 batch size)\n",
      "Automatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\n",
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L1_FULL ...\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t231.99s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L1_FULL ...\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\t9.22s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetTorch_BAG_L1_FULL ...\n",
      "\t81.92s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n",
      "\t19.66s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L2_FULL ...\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t129.17s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L2_FULL ...\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\t8.07s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: NeuralNetTorch_BAG_L2_FULL ...\n",
      "\t47.33s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L2': 0.917, 'NeuralNetTorch_BAG_L2': 0.083}\n",
      "\t19.86s\t = Training   runtime\n",
      "Fitting 1 L3 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBM_BAG_L3_FULL ...\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t71.22s\t = Training   runtime\n",
      "Fitting 1 L3 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost_BAG_L3_FULL ...\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "\t7.43s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L4_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L2': 0.444, 'LightGBM_BAG_L3': 0.444, 'LightGBM_BAG_L1': 0.111}\n",
      "\t52.39s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L4_FULL\" (Previously \"WeightedEnsemble_L4\"). AutoGluon will default to using \"WeightedEnsemble_L4_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 618.39s ... Best model: \"WeightedEnsemble_L4_FULL\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/elicer/ai/enhanced_ship_anomaly_models_b3\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… í›ˆë ¨ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: 131.2ë¶„)\n",
      "\n",
      "ðŸ“ˆ í¬ê´„ì  ì„±ëŠ¥ í‰ê°€\n",
      "========================================\n",
      "ðŸŽ¯ í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ:\n",
      "  ì •í™•ë„: 0.9949\n",
      "  AUC ì ìˆ˜: 0.9999\n",
      "  í›ˆë ¨ ì‹œê°„: 131.2ë¶„\n",
      "  ì˜ˆì¸¡ ì‹œê°„: 75.82ì´ˆ\n",
      "  ì˜ˆì¸¡ ì†ë„: 7064 ìƒ˜í”Œ/ì´ˆ\n",
      "\n",
      "ðŸ“Š ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ì •ìƒ       0.99      1.00      0.99    208982\n",
      "          ì´ìƒ       1.00      0.99      1.00    326644\n",
      "\n",
      "    accuracy                           0.99    535626\n",
      "   macro avg       0.99      0.99      0.99    535626\n",
      "weighted avg       0.99      0.99      0.99    535626\n",
      "\n",
      "\n",
      "ðŸ” í˜¼ë™ í–‰ë ¬:\n",
      "ì‹¤ì œ\\ì˜ˆì¸¡    ì •ìƒ    ì´ìƒ\n",
      "ì •ìƒ        208003    979\n",
      "ì´ìƒ        1757   324887\n",
      "\n",
      "ðŸ† ëª¨ë¸ ë¦¬ë”ë³´ë“œ (ìƒìœ„ 5ê°œ):\n",
      "  WeightedEnsemble_L4: 0.9999\n",
      "  WeightedEnsemble_L3: 0.9999\n",
      "  LightGBM_BAG_L3: 0.9999\n",
      "  LightGBM_BAG_L2: 0.9999\n",
      "  LightGBM_BAG_L1: 0.9999\n",
      "\n",
      "âš–ï¸ ê·œì¹™ ê¸°ë°˜ vs ML ë¹„êµ:\n",
      "  ê·œì¹™ ê¸°ë°˜ ì •í™•ë„: 0.7242\n",
      "  ML ëª¨ë¸ ì •í™•ë„: 0.9949\n",
      "  ê°œì„ ìœ¨: +37.4%\n",
      "\n",
      "âš ï¸ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ë¶ˆê°€\n",
      "\n",
      "============================================================\n",
      "ðŸŽ‰ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\n",
      "ðŸŽ¯ ìµœì¢… AUC: 0.9999\n",
      "ðŸŽ¯ ìµœì¢… ì •í™•ë„: 0.9949\n",
      "â±ï¸ ì´ í›ˆë ¨ ì‹œê°„: 131.2ë¶„\n",
      "============================================================\n",
      "\n",
      "âœ… í›ˆë ¨ ì„±ê³µ!\n",
      "ðŸ’¾ ëª¨ë¸ ì €ìž¥ ê²½ë¡œ: ./enhanced_ship_anomaly_models_b2\n",
      "ðŸ’¾ ê·œì¹™ ê°€ì¤‘ì¹˜ ì €ìž¥: ./rule_weights.json\n",
      "\n",
      "ðŸ”® STEP 2: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
      "------------------------------\n",
      "ðŸ”® Test ë°ì´í„° ì˜ˆì¸¡ ì‹œìž‘\n",
      "==================================================\n",
      "ðŸ“‚ ëª¨ë¸ ë¡œë”©: ./enhanced_ship_anomaly_models_b2\n",
      "âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ\n",
      "ðŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©: ./20240701.csv\n",
      "âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì™„ë£Œ: (6779, 26)\n",
      "âš™ï¸ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ìš©...\n",
      "ðŸ”§ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§...\n",
      "ðŸ“‚ ê·œì¹™ ê°€ì¤‘ì¹˜ ë¡œë“œ: ./rule_weights.json\n",
      "ðŸ”§ ë¡œë“œëœ ê°€ì¤‘ì¹˜:\n",
      "  rule1_foreign_slow_non_anchor: 0.982\n",
      "  rule2_ais_off_non_anchor: 0.000\n",
      "  rule3_special_zone_frequent: 0.950\n",
      "  rule4_sharp_turn_non_anchor: 0.702\n",
      "  rule5_slow_back_forth: 0.000\n",
      "  rule6_restricted_zone: 1.000\n",
      "ðŸ“Š ê·œì¹™ í†µê³„ë„ ë¡œë“œë¨\n",
      "âœ… ê·œì¹™ ê¸°ë°˜ íŠ¹ì„± 8ê°œ ìƒì„± ì™„ë£Œ\n",
      "ðŸ”§ ì˜ˆì¸¡ìš© íŠ¹ì„±: 33ê°œ\n",
      "ðŸ“Š ì˜ˆì¸¡í•  ë°ì´í„°: 6,779ê°œ\n",
      "ðŸ”® ì˜ˆì¸¡ ìˆ˜í–‰...\n",
      "âœ… ì˜ˆì¸¡ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: 1.07ì´ˆ)\n",
      "ðŸš€ ì˜ˆì¸¡ ì†ë„: 6331 ìƒ˜í”Œ/ì´ˆ\n",
      "\n",
      "ðŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:\n",
      "  True: 3,965ê°œ (58.5%)\n",
      "  False: 2,814ê°œ (41.5%)\n",
      "\n",
      "âš–ï¸ ê·œì¹™ vs í•˜ì´ë¸Œë¦¬ë“œ:\n",
      "  ê·œì¹™ ê¸°ë°˜ ì´ìƒ: 2,267ê°œ\n",
      "  í•˜ì´ë¸Œë¦¬ë“œ ì´ìƒ: 3,965ê°œ\n",
      "  ML ì¶”ê°€ íƒì§€: 1,698ê°œ\n",
      "\n",
      "ðŸ’¾ ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: ./test_predictions_basic_2.csv\n",
      "\n",
      "ðŸ“‹ ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ (ìƒìœ„ 10ê°œ):\n",
      "     MMSI result  confidence  rule_based\n",
      "100901213   True    0.999985           1\n",
      "103218823   True    0.999991           1\n",
      "107202858   True    0.999985           1\n",
      "111440519  False    0.000019           0\n",
      "116678950   True    0.999873           1\n",
      "202006213   True    0.999985           1\n",
      "200429918   True    0.999991           1\n",
      "111440500   True    0.999991           1\n",
      "111440105  False    0.018117           0\n",
      "106811519   True    0.999979           1\n",
      "\n",
      "ðŸš¨ ê³ ì‹ ë¢°ë„ ì´ìƒ ì„ ë°• (3879ê°œ):\n",
      "     MMSI result  confidence  rule_based\n",
      "100901213   True    0.999985           1\n",
      "103218823   True    0.999991           1\n",
      "107202858   True    0.999985           1\n",
      "116678950   True    0.999873           1\n",
      "202006213   True    0.999985           1\n",
      "\n",
      "ðŸŽ‰ ì˜ˆì¸¡ ì™„ë£Œ!\n",
      "ðŸ“ ê²°ê³¼ íŒŒì¼: ./test_predictions_basic_2.csv\n",
      "==================================================\n",
      "\n",
      "ðŸŽ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ê³µ!\n",
      "ðŸ“Š ì´ 6,779ê°œ ì„ ë°• ì˜ˆì¸¡ ì™„ë£Œ\n",
      "ðŸ’¾ ìµœì¢… ê²°ê³¼: ./test_predictions_basic_2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import xgboost as xgb\n",
    "\n",
    "class EnhancedShipAnomalyDetector:\n",
    "    \"\"\"í–¥ìƒëœ ì„ ë°• ì´ìƒí–‰ë™ íƒì§€ ì‹œìŠ¤í…œ - ì™„ì „ ë²„ì „\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rule_feature_names = []\n",
    "        self.enhanced_feature_names = []\n",
    "        self.scaler = StandardScaler()\n",
    "        self.rule_weights = {}\n",
    "        self.rule_stats = {}\n",
    "        \n",
    "    def save_rule_weights(self, weights_path='./rule_weights.json'):\n",
    "        \"\"\"ê·œì¹™ ê°€ì¤‘ì¹˜ë¥¼ íŒŒì¼ë¡œ ì €ìž¥\"\"\"\n",
    "        if hasattr(self, 'rule_weights'):\n",
    "            with open(weights_path, 'w') as f:\n",
    "                json.dump(self.rule_weights, f, indent=2)\n",
    "            print(f\"ðŸ’¾ ê·œì¹™ ê°€ì¤‘ì¹˜ ì €ìž¥: {weights_path}\")\n",
    "            \n",
    "            # í†µê³„ ì •ë³´ë„ ì €ìž¥\n",
    "            stats_path = weights_path.replace('.json', '_stats.json')\n",
    "            if hasattr(self, 'rule_stats'):\n",
    "                with open(stats_path, 'w') as f:\n",
    "                    json.dump(self.rule_stats, f, indent=2)\n",
    "                print(f\"ðŸ“Š ê·œì¹™ í†µê³„ ì €ìž¥: {stats_path}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ ì €ìž¥í•  ê°€ì¤‘ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤\")\n",
    "\n",
    "    def load_rule_weights(self, weights_path='./rule_weights.json'):\n",
    "        \"\"\"ì €ìž¥ëœ ê·œì¹™ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œ\"\"\"\n",
    "        if os.path.exists(weights_path):\n",
    "            with open(weights_path, 'r') as f:\n",
    "                self.rule_weights = json.load(f)\n",
    "            print(f\"ðŸ“‚ ê·œì¹™ ê°€ì¤‘ì¹˜ ë¡œë“œ: {weights_path}\")\n",
    "            print(\"ðŸ”§ ë¡œë“œëœ ê°€ì¤‘ì¹˜:\")\n",
    "            for rule, weight in self.rule_weights.items():\n",
    "                print(f\"  {rule}: {weight:.3f}\")\n",
    "            \n",
    "            # í†µê³„ ì •ë³´ë„ ë¡œë“œ (ìžˆë‹¤ë©´)\n",
    "            stats_path = weights_path.replace('.json', '_stats.json')\n",
    "            if os.path.exists(stats_path):\n",
    "                with open(stats_path, 'r') as f:\n",
    "                    self.rule_stats = json.load(f)\n",
    "                print(f\"ðŸ“Š ê·œì¹™ í†µê³„ë„ ë¡œë“œë¨\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âš ï¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {weights_path}\")\n",
    "            print(\"ê¸°ë³¸ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤\")\n",
    "            return False\n",
    "\n",
    "    def calculate_rule_weights(self, df):\n",
    "        \"\"\"í›ˆë ¨ ë°ì´í„°ì—ì„œ ê° ê·œì¹™ì˜ ì‹¤ì œ ì •í™•ë„ ê³„ì‚°\"\"\"\n",
    "        print(\"ðŸ“Š ì‹¤ì œ ë°ì´í„°ì—ì„œ ê·œì¹™ ê°€ì¤‘ì¹˜ ê³„ì‚°...\")\n",
    "        \n",
    "        rule_weights = {}\n",
    "        rule_stats = {}\n",
    "        \n",
    "        rule_columns = [\n",
    "            'rule1_foreign_slow_non_anchor', 'rule2_ais_off_non_anchor',\n",
    "            'rule3_special_zone_frequent', 'rule4_sharp_turn_non_anchor', \n",
    "            'rule5_slow_back_forth', 'rule6_restricted_zone'\n",
    "        ]\n",
    "        \n",
    "        for rule in rule_columns:\n",
    "            if rule in df.columns:\n",
    "                # í•´ë‹¹ ê·œì¹™ì— ì ìš©ë˜ëŠ” ì¼€ì´ìŠ¤ë“¤\n",
    "                rule_cases = df[df[rule] == 1]\n",
    "                \n",
    "                if len(rule_cases) > 0:\n",
    "                    # ì‹¤ì œ ì´ìƒ ë¹„ìœ¨ ê³„ì‚°\n",
    "                    accuracy = rule_cases['result'].mean()\n",
    "                    rule_weights[rule] = accuracy\n",
    "                    rule_stats[rule] = {\n",
    "                        'total_cases': len(rule_cases),\n",
    "                        'anomaly_cases': int(rule_cases['result'].sum()),\n",
    "                        'accuracy': accuracy\n",
    "                    }\n",
    "                    print(f\"  {rule}: {len(rule_cases)}ê±´ ì¤‘ {int(rule_cases['result'].sum())}ê±´ ì´ìƒ â†’ ì •í™•ë„ {accuracy:.3f}\")\n",
    "                else:\n",
    "                    rule_weights[rule] = 0.0\n",
    "                    rule_stats[rule] = {'total_cases': 0, 'anomaly_cases': 0, 'accuracy': 0.0}\n",
    "                    print(f\"  {rule}: í•´ë‹¹ ì¼€ì´ìŠ¤ ì—†ìŒ â†’ ê°€ì¤‘ì¹˜ 0.0\")\n",
    "            else:\n",
    "                rule_weights[rule] = 0.0\n",
    "                rule_stats[rule] = {'total_cases': 0, 'anomaly_cases': 0, 'accuracy': 0.0}\n",
    "        \n",
    "        self.rule_weights = rule_weights\n",
    "        self.rule_stats = rule_stats\n",
    "        \n",
    "        print(f\"âœ… ê·œì¹™ ê°€ì¤‘ì¹˜ ê³„ì‚° ì™„ë£Œ\")\n",
    "        return rule_weights\n",
    "    \n",
    "    def create_rule_based_features(self, df, is_training=False, weights_path='./rule_weights.json'):\n",
    "        \"\"\"6ê°€ì§€ ê·œì¹™ ê¸°ë°˜ íŠ¹ì„± ìƒì„±\"\"\"\n",
    "        print(\"ðŸ”§ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§...\")\n",
    "        \n",
    "        df_enhanced = df.copy()\n",
    "        \n",
    "        # ê·œì¹™ 1: ì™¸êµ­êµ­ì  + ì €ì†í•­í•´ + ë¹„ì •ë°•ì§€\n",
    "        df_enhanced['rule1_foreign_slow_non_anchor'] = (\n",
    "            (df['is_korean_ship'] == 0) & \n",
    "            (df['anchor_zone_ratio'] < 0.8) & \n",
    "            ((df['low_speed_ratio'] > 0.7) | (df['avg_sog'] <= 5))\n",
    "        ).astype(int)\n",
    "        \n",
    "        # ê·œì¹™ 2: AIS OFF ì´ìƒ + ë¹„ì •ë°•ì§€\n",
    "        df_enhanced['rule2_ais_off_non_anchor'] = (\n",
    "            (df['anchor_zone_ratio'] < 0.8) & \n",
    "            (df['off_events'] >= 3)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # ê·œì¹™ 3: íŠ¹ìˆ˜í•´ì—­ ë‹¤ì¤‘ì§„ìž…\n",
    "        df_enhanced['rule3_special_zone_frequent'] = (\n",
    "            df['num_zone_entries'] > 50\n",
    "        ).astype(int)\n",
    "        \n",
    "        # ê·œì¹™ 4: ê¸‰ë³€ì¹¨ + ë¹„ì •ë°•ì§€\n",
    "        df_enhanced['rule4_sharp_turn_non_anchor'] = (\n",
    "            (df['anchor_zone_ratio'] < 0.8) & \n",
    "            (df['sharp_turns'] >= 1)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # ê·œì¹™ 5: ì €ì† + ì™•ë³µí•­í•´\n",
    "        df_enhanced['rule5_slow_back_forth'] = (\n",
    "            (df['avg_sog'] <= 5) & \n",
    "            (df['back_forth_count'] >= 2)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # ê·œì¹™ 6: ê¸ˆì§€êµ¬ì—­ ì§„ìž…\n",
    "        df_enhanced['rule6_restricted_zone'] = df['restricted_zone_flag']\n",
    "        \n",
    "        # ê°€ì¤‘ì¹˜ ì²˜ë¦¬\n",
    "        if is_training and 'result' in df.columns:\n",
    "            # í›ˆë ¨ ì‹œ: ì‹¤ì œ ë°ì´í„°ì—ì„œ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "            rule_weights = self.calculate_rule_weights(df_enhanced)\n",
    "            # ê³„ì‚°í•œ ê°€ì¤‘ì¹˜ë¥¼ íŒŒì¼ë¡œ ì €ìž¥\n",
    "            self.save_rule_weights(weights_path)\n",
    "        else:\n",
    "            # í…ŒìŠ¤íŠ¸ ì‹œ: ì €ìž¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "            loaded = self.load_rule_weights(weights_path)\n",
    "            if loaded:\n",
    "                rule_weights = self.rule_weights\n",
    "            else:\n",
    "                # ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "                rule_weights = {\n",
    "                    'rule6_restricted_zone': 1.0,\n",
    "                    'rule1_foreign_slow_non_anchor': 0.97,\n",
    "                    'rule3_special_zone_frequent': 0.92,\n",
    "                    'rule4_sharp_turn_non_anchor': 0.72,\n",
    "                    'rule2_ais_off_non_anchor': 0.0,\n",
    "                    'rule5_slow_back_forth': 0.0\n",
    "                }\n",
    "                self.rule_weights = rule_weights\n",
    "        \n",
    "        # ì¢…í•© ê·œì¹™ ì ìˆ˜ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ê°€ì¤‘í•©)\n",
    "        df_enhanced['rule_composite_score'] = 0\n",
    "        for rule, weight in rule_weights.items():\n",
    "            if rule in df_enhanced.columns:\n",
    "                df_enhanced['rule_composite_score'] += df_enhanced[rule] * weight\n",
    "        \n",
    "        # ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡\n",
    "        df_enhanced['rule_based_prediction'] = (\n",
    "            (df_enhanced['rule1_foreign_slow_non_anchor'] == 1) |\n",
    "            (df_enhanced['rule2_ais_off_non_anchor'] == 1) |\n",
    "            (df_enhanced['rule3_special_zone_frequent'] == 1) |\n",
    "            (df_enhanced['rule4_sharp_turn_non_anchor'] == 1) |\n",
    "            (df_enhanced['rule5_slow_back_forth'] == 1) |\n",
    "            (df_enhanced['rule6_restricted_zone'] == 1)\n",
    "        ).astype(int)\n",
    "        \n",
    "        rule_features = [\n",
    "            'rule1_foreign_slow_non_anchor', 'rule2_ais_off_non_anchor',\n",
    "            'rule3_special_zone_frequent', 'rule4_sharp_turn_non_anchor',\n",
    "            'rule5_slow_back_forth', 'rule6_restricted_zone',\n",
    "            'rule_composite_score', 'rule_based_prediction'\n",
    "        ]\n",
    "        \n",
    "        self.rule_feature_names = rule_features\n",
    "        print(f\"âœ… ê·œì¹™ ê¸°ë°˜ íŠ¹ì„± {len(rule_features)}ê°œ ìƒì„± ì™„ë£Œ\")\n",
    "        \n",
    "        return df_enhanced\n",
    "    \n",
    "    \n",
    "    def get_enhanced_hyperparameters(self, gpu_available=False):\n",
    "        \"\"\"í–¥ìƒëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° (ë¶ˆê· í˜• ë°ì´í„° ê³ ë ¤)\"\"\"\n",
    "        \n",
    "        cpu_count = os.cpu_count() or 2\n",
    "        \n",
    "        if gpu_available:\n",
    "            hyperparameters = {\n",
    "                # XGBoost with class balancing\n",
    "                'XGB': [\n",
    "                    {\n",
    "                        'n_estimators': 500,\n",
    "                        'learning_rate': 0.05,\n",
    "                        'max_depth': 8,\n",
    "                        'tree_method': 'gpu_hist',\n",
    "                        'gpu_id': 0,\n",
    "                        'objective': 'binary:logistic',\n",
    "                        'eval_metric': 'auc',\n",
    "                        'scale_pos_weight': 0.62,  # ë¶ˆê· í˜• ì¡°ì •\n",
    "                        'subsample': 0.8,\n",
    "                        'colsample_bytree': 0.8\n",
    "                    }\n",
    "                ],\n",
    "                \n",
    "                # LightGBM with focal loss\n",
    "                'GBM': [\n",
    "                    {\n",
    "                        'num_boost_round': 500,\n",
    "                        'learning_rate': 0.05,\n",
    "                        'device': 'gpu',\n",
    "                        'objective': 'binary',\n",
    "                        'metric': 'auc',\n",
    "                        'is_unbalance': True,\n",
    "                        'feature_fraction': 0.8,\n",
    "                        'bagging_fraction': 0.8,\n",
    "                        'bagging_freq': 5\n",
    "                    }\n",
    "                ],\n",
    "                \n",
    "                # CatBoost with auto class balancing\n",
    "                'CAT': [\n",
    "                    {\n",
    "                        'iterations': 500,\n",
    "                        'learning_rate': 0.05,\n",
    "                        'depth': 8,\n",
    "                        'task_type': 'GPU',\n",
    "                        'devices': '0',\n",
    "                        'auto_class_weights': 'Balanced',\n",
    "                        'eval_metric': 'AUC',\n",
    "                        'bootstrap_type': 'Bernoulli',\n",
    "                        'subsample': 0.8\n",
    "                    }\n",
    "                ],\n",
    "                \n",
    "                # Neural Network\n",
    "                'NN_TORCH': [\n",
    "                    {\n",
    "                        'num_epochs': 200,\n",
    "                        'learning_rate': 0.001,\n",
    "                        'batch_size': 512,\n",
    "                        'activation': 'relu',\n",
    "                        'dropout_prob': 0.3,\n",
    "                        'weight_decay': 1e-4\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        else:\n",
    "            # CPU ìµœì í™” ë²„ì „\n",
    "            hyperparameters = {\n",
    "                'GBM': {\n",
    "                    'num_boost_round': 300,\n",
    "                    'learning_rate': 0.1,\n",
    "                    'is_unbalance': True,\n",
    "                    'metric': 'auc'\n",
    "                },\n",
    "                'CAT': {\n",
    "                    'iterations': 300,\n",
    "                    'learning_rate': 0.1,\n",
    "                    'auto_class_weights': 'Balanced'\n",
    "                },\n",
    "                'XGB': {\n",
    "                    'n_estimators': 300,\n",
    "                    'learning_rate': 0.1,\n",
    "                    'scale_pos_weight': 0.62,\n",
    "                    'eval_metric': 'auc'\n",
    "                },\n",
    "                'RF': {\n",
    "                    'n_estimators': 200,\n",
    "                    'max_depth': 15,\n",
    "                    'class_weight': 'balanced',\n",
    "                    'n_jobs': cpu_count\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return hyperparameters\n",
    "    \n",
    "    def train_enhanced_model(self, file_path, time_limit=1800, quality='high_quality'):\n",
    "        \"\"\"í–¥ìƒëœ ëª¨ë¸ í›ˆë ¨\"\"\"\n",
    "        print(\"ðŸŒŸ í–¥ìƒëœ ì„ ë°• ì´ìƒí–‰ë™ íƒì§€ ëª¨ë¸ í›ˆë ¨\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # GPU ìƒíƒœ í™•ì¸\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        print(f\"ðŸ” GPU ì‚¬ìš© ê°€ëŠ¥: {gpu_available}\")\n",
    "        \n",
    "        # ë°ì´í„° ë¡œë“œ\n",
    "        print(\"ðŸ“ ë°ì´í„° ë¡œë”©...\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"âœ… ì›ë³¸ ë°ì´í„°: {df.shape}\")\n",
    "        \n",
    "        # íƒ€ê²Ÿ ì „ì²˜ë¦¬\n",
    "        if 'result' in df.columns:\n",
    "            if df['result'].dtype == 'object':\n",
    "                df['result'] = df['result'].map({'True': 1, 'False': 0})\n",
    "            elif df['result'].dtype == 'bool':\n",
    "                df['result'] = df['result'].astype(int)\n",
    "        \n",
    "        # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (í›ˆë ¨ ëª¨ë“œ)\n",
    "        df_final = self.create_rule_based_features(df, is_training=True)\n",
    "        #df_final = self.create_advanced_features(df_enhanced)\n",
    "        \n",
    "        print(f\"ðŸ“Š ìµœì¢… ë°ì´í„°: {df_final.shape}\")\n",
    "        \n",
    "        # íƒ€ê²Ÿ ë¶„í¬ í™•ì¸\n",
    "        target_dist = df_final['result'].value_counts()\n",
    "        print(f\"ðŸ“ˆ íƒ€ê²Ÿ ë¶„í¬: {dict(target_dist)}\")\n",
    "        print(f\"ðŸ“Š ì´ìƒì„ ë°• ë¹„ìœ¨: {target_dist[1]/len(df_final):.2%}\")\n",
    "        \n",
    "        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬\n",
    "        feature_columns = [col for col in df_final.columns if col not in ['result', 'MMSI']]\n",
    "        X = df_final[feature_columns]\n",
    "        y = df_final['result']\n",
    "        \n",
    "        print(f\"ðŸ”§ ì‚¬ìš©í•  íŠ¹ì„±: {len(feature_columns)}ê°œ\")\n",
    "        \n",
    "        # ë°ì´í„° ë¶„í•  (ì¸µí™” ì¶”ì¶œ)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"ðŸ“Š í›ˆë ¨ ë°ì´í„°: {X_train.shape[0]:,}ê°œ\")\n",
    "        print(f\"ðŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape[0]:,}ê°œ\")\n",
    "        \n",
    "        # í›ˆë ¨ ë°ì´í„° ì¤€ë¹„\n",
    "        train_data = X_train.copy()\n",
    "        train_data['target'] = y_train\n",
    "        \n",
    "        # ëª¨ë¸ ê²½ë¡œ ì„¤ì •\n",
    "        model_path = './enhanced_ship_anomaly_models_b3'\n",
    "        if os.path.exists(model_path):\n",
    "            import shutil\n",
    "            shutil.rmtree(model_path)\n",
    "        \n",
    "        # AutoGluon ì„¤ì •\n",
    "        predictor = TabularPredictor(\n",
    "            label='target',\n",
    "            problem_type='binary',\n",
    "            eval_metric='roc_auc',  # AUCë¡œ ë³€ê²½ (ë¶ˆê· í˜• ë°ì´í„°ì— ì í•©)\n",
    "            path=model_path,\n",
    "            verbosity=2\n",
    "        )\n",
    "        \n",
    "        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n",
    "        hyperparameters = self.get_enhanced_hyperparameters(gpu_available)\n",
    "        \n",
    "        # AG args ì„¤ì •\n",
    "        ag_args_fit = {\n",
    "            'num_gpus': 1 if gpu_available else 0,\n",
    "            'num_cpus': os.cpu_count() or 2,\n",
    "            'auto_stack': True\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nâ³ ëª¨ë¸ í›ˆë ¨ ì‹œìž‘... (ì œí•œì‹œê°„: {time_limit//60}ë¶„)\")\n",
    "        start_time = pd.Timestamp.now()\n",
    "        \n",
    "        try:\n",
    "            predictor.fit(\n",
    "                train_data,\n",
    "                time_limit=time_limit,\n",
    "                presets=quality,\n",
    "                hyperparameters=hyperparameters,\n",
    "                ag_args_fit=ag_args_fit,\n",
    "                holdout_frac=0.15,  # ê²€ì¦ìš©\n",
    "                num_bag_folds=5,\n",
    "                num_bag_sets=1,\n",
    "                num_stack_levels=2,  # ìŠ¤íƒœí‚¹ í™œìš©\n",
    "                verbosity=2,\n",
    "                dynamic_stacking=True,  # ë™ì  ìŠ¤íƒœí‚¹\n",
    "            )\n",
    "            \n",
    "            end_time = pd.Timestamp.now()\n",
    "            training_time = (end_time - start_time).total_seconds()\n",
    "            print(f\"âœ… í›ˆë ¨ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {training_time/60:.1f}ë¶„)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "        if gpu_available:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return predictor, X_test, y_test, training_time\n",
    "    \n",
    "    def comprehensive_evaluation(self, predictor, X_test, y_test, training_time):\n",
    "        \"\"\"í¬ê´„ì  ëª¨ë¸ í‰ê°€\"\"\"\n",
    "        print(f\"\\nðŸ“ˆ í¬ê´„ì  ì„±ëŠ¥ í‰ê°€\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        start_time = pd.Timestamp.now()\n",
    "        y_pred = predictor.predict(X_test)\n",
    "        y_pred_proba = predictor.predict_proba(X_test)\n",
    "        end_time = pd.Timestamp.now()\n",
    "        \n",
    "        prediction_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        # ê¸°ë³¸ ì„±ëŠ¥ ì§€í‘œ\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba.iloc[:, 1])\n",
    "        \n",
    "        print(f\"ðŸŽ¯ í•µì‹¬ ì„±ëŠ¥ ì§€í‘œ:\")\n",
    "        print(f\"  ì •í™•ë„: {accuracy:.4f}\")\n",
    "        print(f\"  AUC ì ìˆ˜: {auc_score:.4f}\")\n",
    "        print(f\"  í›ˆë ¨ ì‹œê°„: {training_time/60:.1f}ë¶„\")\n",
    "        print(f\"  ì˜ˆì¸¡ ì‹œê°„: {prediction_time:.2f}ì´ˆ\")\n",
    "        print(f\"  ì˜ˆì¸¡ ì†ë„: {len(X_test)/prediction_time:.0f} ìƒ˜í”Œ/ì´ˆ\")\n",
    "        \n",
    "        # ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸\n",
    "        print(f\"\\nðŸ“Š ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=['ì •ìƒ', 'ì´ìƒ']))\n",
    "        \n",
    "        # í˜¼ë™ í–‰ë ¬\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(f\"\\nðŸ” í˜¼ë™ í–‰ë ¬:\")\n",
    "        print(f\"ì‹¤ì œ\\\\ì˜ˆì¸¡    ì •ìƒ    ì´ìƒ\")\n",
    "        print(f\"ì •ìƒ        {cm[0,0]:4d}   {cm[0,1]:4d}\")\n",
    "        print(f\"ì´ìƒ        {cm[1,0]:4d}   {cm[1,1]:4d}\")\n",
    "        \n",
    "        # ëª¨ë¸ ë¦¬ë”ë³´ë“œ\n",
    "        try:\n",
    "            leaderboard = predictor.leaderboard(silent=True)\n",
    "            print(f\"\\nðŸ† ëª¨ë¸ ë¦¬ë”ë³´ë“œ (ìƒìœ„ 5ê°œ):\")\n",
    "            top_models = leaderboard.head(5)\n",
    "            for idx, row in top_models.iterrows():\n",
    "                model_name = row['model']\n",
    "                score = row['score_val']\n",
    "                print(f\"  {model_name}: {score:.4f}\")\n",
    "        except:\n",
    "            leaderboard = pd.DataFrame()\n",
    "        \n",
    "        # ê·œì¹™ ê¸°ë°˜ vs ML ì„±ëŠ¥ ë¹„êµ\n",
    "        if 'rule_based_prediction' in X_test.columns:\n",
    "            rule_accuracy = accuracy_score(y_test, X_test['rule_based_prediction'])\n",
    "            print(f\"\\nâš–ï¸ ê·œì¹™ ê¸°ë°˜ vs ML ë¹„êµ:\")\n",
    "            print(f\"  ê·œì¹™ ê¸°ë°˜ ì •í™•ë„: {rule_accuracy:.4f}\")\n",
    "            print(f\"  ML ëª¨ë¸ ì •í™•ë„: {accuracy:.4f}\")\n",
    "            print(f\"  ê°œì„ ìœ¨: {((accuracy - rule_accuracy) / rule_accuracy * 100):+.1f}%\")\n",
    "        \n",
    "        # íŠ¹ì„± ì¤‘ìš”ë„ (ê°€ëŠ¥í•œ ê²½ìš°)\n",
    "        try:\n",
    "            feature_importance = predictor.feature_importance(X_test)\n",
    "            print(f\"\\nðŸ” ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:\")\n",
    "            top_features = feature_importance.head(10)\n",
    "            for feature, importance in top_features.items():\n",
    "                print(f\"  {feature}: {importance:.4f}\")\n",
    "        except:\n",
    "            print(f\"\\nâš ï¸ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ë¶ˆê°€\")\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'auc': auc_score,\n",
    "            'training_time': training_time,\n",
    "            'prediction_time': prediction_time,\n",
    "            'leaderboard': leaderboard\n",
    "        }\n",
    "    \n",
    "    def hybrid_prediction(self, predictor, X_new):\n",
    "        \"\"\"í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡ (ê·œì¹™ + ML)\"\"\"\n",
    "        # ML ì˜ˆì¸¡\n",
    "        ml_pred = predictor.predict(X_new)\n",
    "        ml_proba = predictor.predict_proba(X_new)\n",
    "        \n",
    "        # ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡\n",
    "        if 'rule_based_prediction' in X_new.columns:\n",
    "            rule_pred = X_new['rule_based_prediction'].values\n",
    "            \n",
    "            # í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡: ê·œì¹™ì´ ì´ìƒì´ë©´ ì´ìƒ, ì•„ë‹ˆë©´ ML ì˜ˆì¸¡ ì‚¬ìš©\n",
    "            hybrid_pred = np.maximum(rule_pred, ml_pred)\n",
    "            \n",
    "            return hybrid_pred, ml_proba\n",
    "        else:\n",
    "            return ml_pred, ml_proba\n",
    "\n",
    "def train_ship_anomaly_model(file_path, time_limit=7200, quality='high_quality'):\n",
    "    \"\"\"ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    \n",
    "    detector = EnhancedShipAnomalyDetector()\n",
    "    \n",
    "    try:\n",
    "        # ëª¨ë¸ í›ˆë ¨\n",
    "        predictor, X_test, y_test, training_time = detector.train_enhanced_model(\n",
    "            file_path=file_path,\n",
    "            time_limit=time_limit,\n",
    "            quality=quality\n",
    "        )\n",
    "        \n",
    "        if predictor is None:\n",
    "            print(\"âŒ í›ˆë ¨ ì‹¤íŒ¨\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # ì„±ëŠ¥ í‰ê°€\n",
    "        results = detector.comprehensive_evaluation(\n",
    "            predictor, X_test, y_test, training_time\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ðŸŽ‰ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "        print(f\"ðŸŽ¯ ìµœì¢… AUC: {results['auc']:.4f}\")\n",
    "        print(f\"ðŸŽ¯ ìµœì¢… ì •í™•ë„: {results['accuracy']:.4f}\")\n",
    "        print(f\"â±ï¸ ì´ í›ˆë ¨ ì‹œê°„: {training_time/60:.1f}ë¶„\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return detector, predictor, results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"â— í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def predict_test_data(test_file_path, model_path='./enhanced_ship_anomaly_models_b3', output_path='./test_predictions_b3.csv'):\n",
    "    \"\"\"í•™ìŠµëœ ëª¨ë¸ë¡œ test ë°ì´í„° ì˜ˆì¸¡\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”® Test ë°ì´í„° ì˜ˆì¸¡ ì‹œìž‘\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ë””í…í„° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "    detector = EnhancedShipAnomalyDetector()\n",
    "    \n",
    "    try:\n",
    "        # í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ\n",
    "        print(f\"ðŸ“‚ ëª¨ë¸ ë¡œë”©: {model_path}\")\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"ëª¨ë¸ ê²½ë¡œê°€ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}\")\n",
    "        \n",
    "        predictor = TabularPredictor.load(model_path)\n",
    "        print(\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ\")\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "        print(f\"ðŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©: {test_file_path}\")\n",
    "        test_df = pd.read_csv(test_file_path)\n",
    "        print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {test_df.shape}\")\n",
    "        \n",
    "        # MMSI ë³´ì¡´\n",
    "        mmsi_column = test_df['MMSI'].copy() if 'MMSI' in test_df.columns else None\n",
    "        \n",
    "        # ë™ì¼í•œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ìš© (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)\n",
    "        print(\"âš™ï¸ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ìš©...\")\n",
    "        test_final = detector.create_rule_based_features(test_df, is_training=False)\n",
    "        #test_final = detector.create_advanced_features(test_enhanced)\n",
    "        \n",
    "        # ì˜ˆì¸¡ìš© íŠ¹ì„± ì¤€ë¹„\n",
    "        feature_columns = [col for col in test_final.columns if col not in ['result', 'MMSI']]\n",
    "        X_test = test_final[feature_columns]\n",
    "        \n",
    "        print(f\"ðŸ”§ ì˜ˆì¸¡ìš© íŠ¹ì„±: {len(feature_columns)}ê°œ\")\n",
    "        print(f\"ðŸ“Š ì˜ˆì¸¡í•  ë°ì´í„°: {X_test.shape[0]:,}ê°œ\")\n",
    "        \n",
    "        # í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        print(\"ðŸ”® ì˜ˆì¸¡ ìˆ˜í–‰...\")\n",
    "        start_time = pd.Timestamp.now()\n",
    "        \n",
    "        hybrid_pred, ml_proba = detector.hybrid_prediction(predictor, X_test)\n",
    "        \n",
    "        end_time = pd.Timestamp.now()\n",
    "        prediction_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"âœ… ì˜ˆì¸¡ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {prediction_time:.2f}ì´ˆ)\")\n",
    "        print(f\"ðŸš€ ì˜ˆì¸¡ ì†ë„: {len(X_test)/prediction_time:.0f} ìƒ˜í”Œ/ì´ˆ\")\n",
    "        \n",
    "        # ê²°ê³¼ë¥¼ TRUE/FALSEë¡œ ë³€í™˜\n",
    "        result_labels = ['False' if pred == 0 else 'True' for pred in hybrid_pred]\n",
    "        \n",
    "        # ê²°ê³¼ DataFrame ìƒì„±\n",
    "        result_df = pd.DataFrame()\n",
    "        \n",
    "        if mmsi_column is not None:\n",
    "            result_df['MMSI'] = mmsi_column\n",
    "        \n",
    "        result_df['result'] = result_labels\n",
    "        \n",
    "        # ì˜ˆì¸¡ ê²°ê³¼ í†µê³„\n",
    "        pred_counts = pd.Series(result_labels).value_counts()\n",
    "        print(f\"\\nðŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:\")\n",
    "        for label, count in pred_counts.items():\n",
    "            percentage = count / len(result_labels) * 100\n",
    "            print(f\"  {label}: {count:,}ê°œ ({percentage:.1f}%)\")\n",
    "        \n",
    "        # ê·œì¹™ ê¸°ë°˜ vs í•˜ì´ë¸Œë¦¬ë“œ ë¹„êµ\n",
    "        if 'rule_based_prediction' in X_test.columns:\n",
    "            rule_anomalies = X_test['rule_based_prediction'].sum()\n",
    "            hybrid_anomalies = sum(hybrid_pred)\n",
    "            print(f\"\\nâš–ï¸ ê·œì¹™ vs í•˜ì´ë¸Œë¦¬ë“œ:\")\n",
    "            print(f\"  ê·œì¹™ ê¸°ë°˜ ì´ìƒ: {rule_anomalies:,}ê°œ\")\n",
    "            print(f\"  í•˜ì´ë¸Œë¦¬ë“œ ì´ìƒ: {hybrid_anomalies:,}ê°œ\")\n",
    "            print(f\"  ML ì¶”ê°€ íƒì§€: {hybrid_anomalies - rule_anomalies:,}ê°œ\")\n",
    "        \n",
    "        # ê²°ê³¼ ì €ìž¥\n",
    "        result_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nðŸ’¾ ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: {output_path}\")\n",
    "        \n",
    "        # ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥\n",
    "        print(f\"\\nðŸ“‹ ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ (ìƒìœ„ 10ê°œ):\")\n",
    "        print(result_df.head(10).to_string(index=False))\n",
    "        \n",
    "        # ê³ ì‹ ë¢°ë„ ì´ìƒ ì„ ë°•\n",
    "        high_confidence_anomalies = result_df[\n",
    "            (result_df['result'] == 'True') & \n",
    "            (result_df['confidence'] > 0.8)\n",
    "        ]\n",
    "        \n",
    "        if len(high_confidence_anomalies) > 0:\n",
    "            print(f\"\\nðŸš¨ ê³ ì‹ ë¢°ë„ ì´ìƒ ì„ ë°• ({len(high_confidence_anomalies)}ê°œ):\")\n",
    "            print(high_confidence_anomalies.head().to_string(index=False))\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
    "        print(f\"ðŸ“ ê²°ê³¼ íŒŒì¼: {output_path}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return None\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸš€ ì™„ì „í•œ ì„ ë°• ì´ìƒí–‰ë™ íƒì§€ ì‹œìŠ¤í…œ\")\n",
    "    print(\"í›ˆë ¨ â†’ í‰ê°€ â†’ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. ëª¨ë¸ í›ˆë ¨\n",
    "    print(\"\\nðŸ”¥ STEP 1: ëª¨ë¸ í›ˆë ¨\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    train_file_path = './train/merged_output.csv'  # í›ˆë ¨ ë°ì´í„° ê²½ë¡œ\n",
    "    \n",
    "    detector, predictor, results = train_ship_anomaly_model(\n",
    "        file_path=train_file_path,\n",
    "        time_limit=7200,  # 30ë¶„\n",
    "        quality='high_quality'\n",
    "    )\n",
    "    \n",
    "    if detector and predictor:\n",
    "        print(f\"\\nâœ… í›ˆë ¨ ì„±ê³µ!\")\n",
    "        print(f\"ðŸ’¾ ëª¨ë¸ ì €ìž¥ ê²½ë¡œ: ./enhanced_ship_anomaly_models_b2\")\n",
    "        print(f\"ðŸ’¾ ê·œì¹™ ê°€ì¤‘ì¹˜ ì €ìž¥: ./rule_weights.json\")\n",
    "        \n",
    "        # 2. í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\n",
    "        print(\"\\nðŸ”® STEP 2: í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        test_file_path = './20240701.csv'  # í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ\n",
    "        output_path = './test_predictions_basic_2.csv'  # ê²°ê³¼ ì €ìž¥ ê²½ë¡œ\n",
    "        \n",
    "        predictions = predict_test_data(\n",
    "            test_file_path=test_file_path,\n",
    "            model_path='./enhanced_ship_anomaly_models_b2',\n",
    "            output_path=output_path\n",
    "        )\n",
    "        \n",
    "        if predictions is not None:\n",
    "            print(f\"\\nðŸŽ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ê³µ!\")\n",
    "            print(f\"ðŸ“Š ì´ {len(predictions):,}ê°œ ì„ ë°• ì˜ˆì¸¡ ì™„ë£Œ\")\n",
    "            print(f\"ðŸ’¾ ìµœì¢… ê²°ê³¼: {output_path}\")\n",
    "        else:\n",
    "            print(\"âŒ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ì‹¤íŒ¨\")\n",
    "            \n",
    "    else:\n",
    "        print(\"âŒ í›ˆë ¨ ì‹¤íŒ¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "class ShipAnomalyPredictor:\n",
    "    \"\"\"ì„ ë°• ì´ìƒí–‰ë™ íƒì§€ ì‹œìŠ¤í…œ - ì˜ˆì¸¡ ë²„ì „\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rule_weights = {}\n",
    "        self.rule_stats = {}\n",
    "        \n",
    "    def load_rule_weights(self, weights_path='./rule_weights.json'):\n",
    "        \"\"\"ì €ìž¥ëœ ê·œì¹™ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œ\"\"\"\n",
    "        if os.path.exists(weights_path):\n",
    "            with open(weights_path, 'r') as f:\n",
    "                self.rule_weights = json.load(f)\n",
    "            print(f\"ðŸ“‚ ê·œì¹™ ê°€ì¤‘ì¹˜ ë¡œë“œ: {weights_path}\")\n",
    "            print(\"ðŸ”§ ë¡œë“œëœ ê°€ì¤‘ì¹˜:\")\n",
    "            for rule, weight in self.rule_weights.items():\n",
    "                print(f\"  {rule}: {weight:.3f}\")\n",
    "            \n",
    "            # í†µê³„ ì •ë³´ë„ ë¡œë“œ (ìžˆë‹¤ë©´)\n",
    "            stats_path = weights_path.replace('.json', '_stats.json')\n",
    "            if os.path.exists(stats_path):\n",
    "                with open(stats_path, 'r') as f:\n",
    "                    self.rule_stats = json.load(f)\n",
    "                print(f\"ðŸ“Š ê·œì¹™ í†µê³„ë„ ë¡œë“œë¨\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âš ï¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {weights_path}\")\n",
    "            print(\"ê¸°ë³¸ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤\")\n",
    "            return False\n",
    "\n",
    "    def create_rule_based_features(self, df, weights_path='./rule_weights.json'):\n",
    "        \"\"\"6ê°€ì§€ ê·œì¹™ ê¸°ë°˜ íŠ¹ì„± ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)\"\"\"\n",
    "        print(\"ðŸ”§ ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§...\")\n",
    "        \n",
    "        df_enhanced = df.copy()\n",
    "        \n",
    "        # ê·œì¹™ 1: ì™¸êµ­êµ­ì  + ì €ì†í•­í•´ + ë¹„ì •ë°•ì§€\n",
    "        df_enhanced['rule1_foreign_slow_non_anchor'] = (\n",
    "            (df['is_korean_ship'] == 0) & \n",
    "            (df['anchor_zone_ratio'] < 0.8) & \n",
    "            ((df['low_speed_ratio'] > 0.7) | (df['avg_sog'] <= 5))\n",
    "        ).astype(int)\n",
    "        \n",
    "        # ê·œì¹™ 2: AIS OFF ì´ìƒ + ë¹„ì •ë°•ì§€\n",
    "        df_enhanced['rule2_ais_off_non_anchor'] = (\n",
    "            (df['anchor_zone_ratio'] < 0.8) & \n",
    "            (df['off_events'] >= 3)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # ê·œì¹™ 3: íŠ¹ìˆ˜í•´ì—­ ë‹¤ì¤‘ì§„ìž…\n",
    "        df_enhanced['rule3_special_zone_frequent'] = (\n",
    "            df['num_zone_entries'] > 50\n",
    "        ).astype(int)\n",
    "        \n",
    "        # ê·œì¹™ 4: ê¸‰ë³€ì¹¨ + ë¹„ì •ë°•ì§€\n",
    "        df_enhanced['rule4_sharp_turn_non_anchor'] = (\n",
    "            (df['anchor_zone_ratio'] < 0.8) & \n",
    "            (df['sharp_turns'] >= 1)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # ê·œì¹™ 5: ì €ì† + ì™•ë³µí•­í•´\n",
    "        df_enhanced['rule5_slow_back_forth'] = (\n",
    "            (df['avg_sog'] <= 5) & \n",
    "            (df['back_forth_count'] >= 2)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # ê·œì¹™ 6: ê¸ˆì§€êµ¬ì—­ ì§„ìž…\n",
    "        df_enhanced['rule6_restricted_zone'] = df['restricted_zone_flag']\n",
    "        \n",
    "        # ì €ìž¥ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ\n",
    "        loaded = self.load_rule_weights(weights_path)\n",
    "        if loaded:\n",
    "            rule_weights = self.rule_weights\n",
    "        else:\n",
    "            # ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©\n",
    "            rule_weights = {\n",
    "                'rule6_restricted_zone': 1.0,\n",
    "                'rule1_foreign_slow_non_anchor': 0.977,  # ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì—…ë°ì´íŠ¸ëœ ê°’\n",
    "                'rule3_special_zone_frequent': 0.964,   # ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì—…ë°ì´íŠ¸ëœ ê°’\n",
    "                'rule4_sharp_turn_non_anchor': 0.752,   # ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì—…ë°ì´íŠ¸ëœ ê°’\n",
    "                'rule2_ais_off_non_anchor': 0.0,\n",
    "                'rule5_slow_back_forth': 0.0\n",
    "            }\n",
    "            self.rule_weights = rule_weights\n",
    "        \n",
    "        # ì¢…í•© ê·œì¹™ ì ìˆ˜ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ê°€ì¤‘í•©)\n",
    "        df_enhanced['rule_composite_score'] = 0\n",
    "        for rule, weight in rule_weights.items():\n",
    "            if rule in df_enhanced.columns:\n",
    "                df_enhanced['rule_composite_score'] += df_enhanced[rule] * weight\n",
    "        \n",
    "        # ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡\n",
    "        df_enhanced['rule_based_prediction'] = (\n",
    "            (df_enhanced['rule1_foreign_slow_non_anchor'] == 1) |\n",
    "            (df_enhanced['rule2_ais_off_non_anchor'] == 1) |\n",
    "            (df_enhanced['rule3_special_zone_frequent'] == 1) |\n",
    "            (df_enhanced['rule4_sharp_turn_non_anchor'] == 1) |\n",
    "            (df_enhanced['rule5_slow_back_forth'] == 1) |\n",
    "            (df_enhanced['rule6_restricted_zone'] == 1)\n",
    "        ).astype(int)\n",
    "        \n",
    "        print(f\"âœ… ê·œì¹™ ê¸°ë°˜ íŠ¹ì„± 8ê°œ ìƒì„± ì™„ë£Œ\")\n",
    "        \n",
    "        return df_enhanced\n",
    "    \n",
    "    def hybrid_prediction(self, predictor, X_new):\n",
    "        \"\"\"í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡ (ê·œì¹™ + ML)\"\"\"\n",
    "        # ML ì˜ˆì¸¡\n",
    "        ml_pred = predictor.predict(X_new)\n",
    "        ml_proba = predictor.predict_proba(X_new)\n",
    "        \n",
    "        # ê·œì¹™ ê¸°ë°˜ ì˜ˆì¸¡\n",
    "        if 'rule_based_prediction' in X_new.columns:\n",
    "            rule_pred = X_new['rule_based_prediction'].values\n",
    "            \n",
    "            # í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡: ê·œì¹™ì´ ì´ìƒì´ë©´ ì´ìƒ, ì•„ë‹ˆë©´ ML ì˜ˆì¸¡ ì‚¬ìš©\n",
    "            hybrid_pred = np.maximum(rule_pred, ml_pred)\n",
    "            \n",
    "            return hybrid_pred, ml_proba\n",
    "        else:\n",
    "            return ml_pred, ml_proba\n",
    "\n",
    "def predict_test_data(test_file_path, model_path='./enhanced_ship_anomaly_models_b2', output_path='./test.csv'):\n",
    "    \"\"\"í•™ìŠµëœ ëª¨ë¸ë¡œ test ë°ì´í„° ì˜ˆì¸¡\"\"\"\n",
    "    \n",
    "    print(\"ðŸ”® Test ë°ì´í„° ì˜ˆì¸¡ ì‹œìž‘\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ì˜ˆì¸¡ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "    predictor_system = ShipAnomalyPredictor()\n",
    "    \n",
    "    try:\n",
    "        # í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ\n",
    "        print(f\"ðŸ“‚ ëª¨ë¸ ë¡œë”©: {model_path}\")\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"ëª¨ë¸ ê²½ë¡œê°€ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}\")\n",
    "        \n",
    "        predictor = TabularPredictor.load(model_path)\n",
    "        print(\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ\")\n",
    "        \n",
    "        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "        print(f\"ðŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©: {test_file_path}\")\n",
    "        test_df = pd.read_csv(test_file_path)\n",
    "        print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {test_df.shape}\")\n",
    "        \n",
    "        # MMSI ë³´ì¡´\n",
    "        mmsi_column = test_df['MMSI'].copy() if 'MMSI' in test_df.columns else None\n",
    "        \n",
    "        # ë™ì¼í•œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ìš© (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)\n",
    "        print(\"âš™ï¸ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ìš©...\")\n",
    "        test_final = predictor_system.create_rule_based_features(test_df)\n",
    "        \n",
    "        # ì˜ˆì¸¡ìš© íŠ¹ì„± ì¤€ë¹„\n",
    "        feature_columns = [col for col in test_final.columns if col not in ['result', 'MMSI']]\n",
    "        X_test = test_final[feature_columns]\n",
    "        \n",
    "        print(f\"ðŸ”§ ì˜ˆì¸¡ìš© íŠ¹ì„±: {len(feature_columns)}ê°œ\")\n",
    "        print(f\"ðŸ“Š ì˜ˆì¸¡í•  ë°ì´í„°: {X_test.shape[0]:,}ê°œ\")\n",
    "        \n",
    "        # í•˜ì´ë¸Œë¦¬ë“œ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "        print(\"ðŸ”® ì˜ˆì¸¡ ìˆ˜í–‰...\")\n",
    "        start_time = pd.Timestamp.now()\n",
    "        \n",
    "        hybrid_pred, ml_proba = predictor_system.hybrid_prediction(predictor, X_test)\n",
    "        \n",
    "        end_time = pd.Timestamp.now()\n",
    "        prediction_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"âœ… ì˜ˆì¸¡ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {prediction_time:.2f}ì´ˆ)\")\n",
    "        print(f\"ðŸš€ ì˜ˆì¸¡ ì†ë„: {len(X_test)/prediction_time:.0f} ìƒ˜í”Œ/ì´ˆ\")\n",
    "        \n",
    "        # ê²°ê³¼ë¥¼ TRUE/FALSEë¡œ ë³€í™˜\n",
    "        result_labels = ['False' if pred == 0 else 'True' for pred in hybrid_pred]\n",
    "        \n",
    "        # ê²°ê³¼ DataFrame ìƒì„±\n",
    "        result_df = pd.DataFrame()\n",
    "        \n",
    "        if mmsi_column is not None:\n",
    "            result_df['MMSI'] = mmsi_column\n",
    "        \n",
    "        result_df['result'] = result_labels\n",
    "        \n",
    "        # ì˜ˆì¸¡ ê²°ê³¼ í†µê³„\n",
    "        pred_counts = pd.Series(result_labels).value_counts()\n",
    "        print(f\"\\nðŸ“Š ì˜ˆì¸¡ ê²°ê³¼ ë¶„í¬:\")\n",
    "        for label, count in pred_counts.items():\n",
    "            percentage = count / len(result_labels) * 100\n",
    "            print(f\"  {label}: {count:,}ê°œ ({percentage:.1f}%)\")\n",
    "        \n",
    "        # ê·œì¹™ ê¸°ë°˜ vs í•˜ì´ë¸Œë¦¬ë“œ ë¹„êµ\n",
    "        if 'rule_based_prediction' in X_test.columns:\n",
    "            rule_anomalies = X_test['rule_based_prediction'].sum()\n",
    "            hybrid_anomalies = sum(hybrid_pred)\n",
    "            print(f\"\\nâš–ï¸ ê·œì¹™ vs í•˜ì´ë¸Œë¦¬ë“œ:\")\n",
    "            print(f\"  ê·œì¹™ ê¸°ë°˜ ì´ìƒ: {rule_anomalies:,}ê°œ\")\n",
    "            print(f\"  í•˜ì´ë¸Œë¦¬ë“œ ì´ìƒ: {hybrid_anomalies:,}ê°œ\")\n",
    "            print(f\"  ML ì¶”ê°€ íƒì§€: {hybrid_anomalies - rule_anomalies:,}ê°œ\")\n",
    "        \n",
    "        # ê²°ê³¼ ì €ìž¥\n",
    "        result_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nðŸ’¾ ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: {output_path}\")\n",
    "        \n",
    "        # ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥\n",
    "        print(f\"\\nðŸ“‹ ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ (ìƒìœ„ 10ê°œ):\")\n",
    "        print(result_df.head(10).to_string(index=False))\n",
    "        \n",
    "        # ê·œì¹™ë³„ ìƒì„¸ ë¶„ì„\n",
    "        print(f\"\\nðŸ” ê·œì¹™ë³„ íƒì§€ í˜„í™©:\")\n",
    "        for rule_name in ['rule1_foreign_slow_non_anchor', 'rule3_special_zone_frequent', \n",
    "                         'rule4_sharp_turn_non_anchor', 'rule6_restricted_zone']:\n",
    "            if rule_name in X_test.columns:\n",
    "                count = X_test[rule_name].sum()\n",
    "                print(f\"  {rule_name}: {count}ê°œ\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
    "        print(f\"ðŸ“ ê²°ê³¼ íŒŒì¼: {output_path}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "        return None\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸš€ ì„ ë°• ì´ìƒí–‰ë™ íƒì§€ ì‹œìŠ¤í…œ - í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # í•„ìˆ˜ íŒŒì¼ ì¡´ìž¬ í™•ì¸\n",
    "    model_path = './enhanced_ship_anomaly_models_b2'\n",
    "    weights_path = './rule_weights.json'\n",
    "    \n",
    "    print(\"ðŸ“‹ í•„ìˆ˜ íŒŒì¼ í™•ì¸:\")\n",
    "    print(f\"  ëª¨ë¸ í´ë”: {'âœ…' if os.path.exists(model_path) else 'âŒ'} {model_path}\")\n",
    "    print(f\"  ê°€ì¤‘ì¹˜ íŒŒì¼: {'âœ…' if os.path.exists(weights_path) else 'âŒ'} {weights_path}\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(\"\\nâŒ ì˜¤ë¥˜: í›ˆë ¨ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "        print(\"ë¨¼ì € train.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ì„¸ìš”.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "    test_file_path = './20240701.csv'  # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œë¡œ ë³€ê²½\n",
    "    output_path = './test_predictions_final.csv'  # ê²°ê³¼ ì €ìž¥ ê²½ë¡œ\n",
    "    \n",
    "    print(f\"\\nðŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_file_path}\")\n",
    "    print(f\"ðŸ’¾ ê²°ê³¼ ì €ìž¥ ê²½ë¡œ: {output_path}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì‹¤í–‰\n",
    "    predictions = predict_test_data(\n",
    "        test_file_path=test_file_path,\n",
    "        model_path=model_path,\n",
    "        output_path=output_path\n",
    "    )\n",
    "    \n",
    "    if predictions is not None:\n",
    "        print(f\"\\nðŸŽ‰ ì „ì²´ ì˜ˆì¸¡ ì™„ë£Œ!\")\n",
    "        print(f\"ðŸ“Š ì´ {len(predictions):,}ê°œ ì„ ë°• ì˜ˆì¸¡ ì™„ë£Œ\")\n",
    "        print(f\"ðŸ’¾ ìµœì¢… ê²°ê³¼: {output_path}\")\n",
    "        \n",
    "        # ìš”ì•½ í†µê³„\n",
    "        anomaly_count = (predictions['result'] == 'True').sum()\n",
    "        total_count = len(predictions)\n",
    "        anomaly_rate = anomaly_count / total_count * 100\n",
    "        \n",
    "        print(f\"\\nðŸ“ˆ ìµœì¢… ìš”ì•½:\")\n",
    "        print(f\"  ì „ì²´ ì„ ë°•: {total_count:,}ê°œ\")\n",
    "        print(f\"  ì´ìƒ ì„ ë°•: {anomaly_count:,}ê°œ ({anomaly_rate:.1f}%)\")\n",
    "        print(f\"  ì •ìƒ ì„ ë°•: {total_count - anomaly_count:,}ê°œ ({100-anomaly_rate:.1f}%)\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨\")\n",
    "        print(\"âš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œì™€ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
