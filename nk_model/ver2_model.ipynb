{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1vREpWbk2RaULMKdTAcrzyyLGk5XSIX2a","authorship_tag":"ABX9TyPFCcdFyGIIvfbRlMI+qfZP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"thOUWgOjLGnT","executionInfo":{"status":"ok","timestamp":1748475171863,"user_tz":-540,"elapsed":2620,"user":{"displayName":"YOONGEE","userId":"13265385226373116446"}}},"outputs":[],"source":["import os\n","import glob\n","import random\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import roc_auc_score\n","\n","# -----------------------------\n","# 설정\n","# -----------------------------\n","TRACK_DIR = '/content/drive/MyDrive/25년 해군 AI 경진대회/nk_dataset/mmsi_tracks'\n","MAPPING_FILE = '/content/drive/MyDrive/25년 해군 AI 경진대회/nk_dataset/Anonymized_MMSI_list-매핑용.csv'\n","VALID_FILE = '/content/drive/MyDrive/25년 해군 AI 경진대회/2번문제/Anonymized_MMSI_list_validation(2번문제)_수정.csv'\n","OUTPUT_FILE = 'submission_nkmodel2.csv'\n","\n","SEQ_LEN = 30           # 시퀀스 길이\n","STEP = 5               # 윈도우 슬라이드 간격\n","RESAMPLE_FREQ = '10s'   # 리샘플링 주기\n","MAX_SEQ_PER_VESSEL = 100\n","LATENT_DIM = 16\n","BATCH_SIZE = 128\n","EPOCHS = 20\n","LR = 1e-3\n","PATIENCE = 3\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"]},{"cell_type":"code","source":["# -----------------------------\n","# 매핑 로드\n","# -----------------------------\n","mapping_df = pd.read_csv(MAPPING_FILE)\n","mapping = dict(zip(mapping_df['Anonymized_MMSI'].astype(str), mapping_df['MMSI'].astype(str)))\n","files = glob.glob(os.path.join(TRACK_DIR, '*.csv'))\n","anon_ids = [os.path.splitext(os.path.basename(f))[0] for f in files]"],"metadata":{"id":"qk6yPwEeLjof","executionInfo":{"status":"ok","timestamp":1748475171871,"user_tz":-540,"elapsed":2,"user":{"displayName":"YOONGEE","userId":"13265385226373116446"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# Dataset 정의\n","# -----------------------------\n","class TrackSeqDataset(Dataset):\n","    def __init__(self, files, seq_len, step, resample_freq, max_per_id):\n","        self.seq_len = seq_len\n","        self.step = step\n","        self.sequences = []\n","        self.seq_ids = []\n","        # 1) 모든 선박 raw 데이터 로드\n","        raw_data = {}\n","        for f in files:\n","            aid = os.path.splitext(os.path.basename(f))[0]\n","            df = pd.read_csv(f, parse_dates=['timestamp'])\n","            df = df.sort_values('timestamp').set_index('timestamp')\n","            df = df[['sog','cog','latitude','longitude']]\n","            df = df.resample(resample_freq).mean().interpolate().dropna()\n","            raw_data[aid] = df.values\n","        # 2) 글로벌 스케일러 학습\n","        concat = np.vstack(list(raw_data.values()))\n","        self.scaler = StandardScaler().fit(concat)\n","        # 3) 시퀀스 생성 및 샘플링\n","        for aid, arr in raw_data.items():\n","            normed = self.scaler.transform(arr)\n","            seqs = []\n","            for i in range(0, len(normed) - seq_len + 1, step):\n","                seqs.append(normed[i:i+seq_len])\n","            if len(seqs) > max_per_id:\n","                seqs = random.sample(seqs, max_per_id)\n","            for s in seqs:\n","                self.sequences.append(s)\n","                self.seq_ids.append(aid)\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.sequences[idx], dtype=torch.float32), self.seq_ids[idx]"],"metadata":{"id":"kWSAz6dALmTj","executionInfo":{"status":"ok","timestamp":1748475171873,"user_tz":-540,"elapsed":1,"user":{"displayName":"YOONGEE","userId":"13265385226373116446"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# GRU Autoencoder\n","# -----------------------------\n","class GRUAutoencoder(nn.Module):\n","    def __init__(self, seq_len, n_feat, latent_dim):\n","        super().__init__()\n","        self.encoder = nn.GRU(n_feat, latent_dim, batch_first=True)\n","        self.decoder = nn.GRU(latent_dim, n_feat, batch_first=True)\n","    def forward(self, x):\n","        out, h = self.encoder(x)\n","        latent = h.repeat(x.size(1), 1, 1).permute(1,0,2)\n","        recon, _ = self.decoder(latent)\n","        return recon"],"metadata":{"id":"u_BpgBQ3LoR6","executionInfo":{"status":"ok","timestamp":1748475171876,"user_tz":-540,"elapsed":1,"user":{"displayName":"YOONGEE","userId":"13265385226373116446"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# 학습 및 조기 종료\n","# -----------------------------\n","def train_model(dataset):\n","    # Train/Val 분리\n","    val_size = int(0.2 * len(dataset))\n","    train_size = len(dataset) - val_size\n","    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n","    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n","                              num_workers=4, pin_memory=True)\n","    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n","                            num_workers=4, pin_memory=True)\n","\n","    model = GRUAutoencoder(SEQ_LEN, 4, LATENT_DIM).to(DEVICE)\n","    optim = torch.optim.Adam(model.parameters(), lr=LR)\n","    criterion = nn.MSELoss()\n","    best_val = float('inf')\n","    patience_cnt = 0\n","\n","    for epoch in range(1, EPOCHS+1):\n","        model.train()\n","        train_loss = 0\n","        for seqs, _ in train_loader:\n","            seqs = seqs.to(DEVICE)\n","            optim.zero_grad()\n","            recon = model(seqs)\n","            loss = criterion(recon, seqs)\n","            loss.backward()\n","            optim.step()\n","            train_loss += loss.item()\n","        train_loss /= len(train_loader)\n","\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for seqs, _ in val_loader:\n","                seqs = seqs.to(DEVICE)\n","                recon = model(seqs)\n","                val_loss += criterion(recon, seqs).item()\n","        val_loss /= len(val_loader)\n","\n","        print(f\"Epoch {epoch}: Train Loss={train_loss:.6f}, Val Loss={val_loss:.6f}\")\n","        # Early Stopping\n","        if val_loss < best_val:\n","            best_val = val_loss\n","            torch.save(model.state_dict(), 'best_gru_ae.pth')\n","            patience_cnt = 0\n","        else:\n","            patience_cnt += 1\n","            if patience_cnt >= PATIENCE:\n","                print(\"Early stopping...\")\n","                break\n","    model.load_state_dict(torch.load('best_gru_ae.pth'))\n","    return model, dataset.scaler"],"metadata":{"id":"u2FKVRxQLp2k","executionInfo":{"status":"ok","timestamp":1748475171883,"user_tz":-540,"elapsed":5,"user":{"displayName":"YOONGEE","userId":"13265385226373116446"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# 재구성 오차 계산\n","# -----------------------------\n","def compute_errors(model, dataset):\n","    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False,\n","                        num_workers=4, pin_memory=True)\n","    model.eval()\n","    errs = {}\n","    with torch.no_grad():\n","        for seqs, aids in loader:\n","            seqs = seqs.to(DEVICE)\n","            recon = model(seqs)\n","            batch_err = ((recon - seqs)**2).mean(dim=(1,2)).cpu().numpy()\n","            for e, aid in zip(batch_err, aids):\n","                errs.setdefault(aid, []).append(e)\n","    return {aid: np.mean(v) for aid,v in errs.items()}"],"metadata":{"id":"NITVD1zCLr2g","executionInfo":{"status":"ok","timestamp":1748475171888,"user_tz":-540,"elapsed":3,"user":{"displayName":"YOONGEE","userId":"13265385226373116446"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# 이상치 탐지 및 예측 저장\n","# -----------------------------\n","def predict_and_save(model, scaler, threshold):\n","    results = []\n","    val_df = pd.read_csv(VALID_FILE)\n","    for aid in val_df['Anonymized_MMSI'].astype(str):\n","        f = os.path.join(TRACK_DIR, f\"{aid}.csv\")\n","        if not os.path.exists(f):\n","            pred = False\n","        else:\n","            df = pd.read_csv(f, parse_dates=['timestamp']).sort_values('timestamp').set_index('timestamp')\n","            arr = df[['sog','cog','latitude','longitude']]\n","            arr = arr.resample(RESAMPLE_FREQ).mean().interpolate().dropna().values\n","            normed = scaler.transform(arr)\n","            seqs = [normed[i:i+SEQ_LEN] for i in range(0, len(normed)-SEQ_LEN+1, STEP)]\n","            if not seqs:\n","                pred = False\n","            else:\n","                seqs_t = torch.tensor(np.stack(seqs), dtype=torch.float32).to(DEVICE)\n","                with torch.no_grad():\n","                    recon = model(seqs_t)\n","                errs = ((recon - seqs_t)**2).mean(dim=(1,2)).cpu().numpy()\n","                pred = errs.mean() > threshold\n","        results.append({'Anonymized_MMSI': aid, 'result': 'TRUE' if pred else 'FALSE'})\n","    pd.DataFrame(results).to_csv(OUTPUT_FILE, index=False)\n","    print(f\"Saved submission to {OUTPUT_FILE}\")"],"metadata":{"id":"uiEMLAbhLt0a","executionInfo":{"status":"ok","timestamp":1748475171898,"user_tz":-540,"elapsed":6,"user":{"displayName":"YOONGEE","userId":"13265385226373116446"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# Main\n","# -----------------------------\n","if __name__ == '__main__':\n","    dataset = TrackSeqDataset(files, SEQ_LEN, STEP, RESAMPLE_FREQ, MAX_SEQ_PER_VESSEL)\n","    model, scaler = train_model(dataset)\n","    errs = compute_errors(model, dataset)\n","    thr = np.percentile(list(errs.values()), 95)\n","    print(f\"Threshold: {thr:.6f}\")\n","    predict_and_save(model, scaler, thr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VeiUWEGpLwDw","outputId":"0d6500c2-bb6e-441e-9f0c-191c53c09eab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-535efb6dba3b>:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  df = pd.read_csv(f, parse_dates=['timestamp'])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"NK7UaFU_LzG7"},"execution_count":null,"outputs":[]}]}