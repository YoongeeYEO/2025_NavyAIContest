{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1iQVraLc8GfyFEkuByd_r8nKCj2Do2bLI","authorship_tag":"ABX9TyNfDVtUr6Dfb+P6afDrIrtF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"-_a0-gYTGuSh","executionInfo":{"status":"ok","timestamp":1748475179608,"user_tz":-540,"elapsed":2992,"user":{"displayName":"YOONGEE","userId":"13265385226373116446"}}},"outputs":[],"source":["import os\n","import glob\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import roc_auc_score, precision_recall_curve\n","\n","# -----------------------------\n","# 설정\n","# -----------------------------\n","TRACK_DIR = '/content/drive/MyDrive/25년 해군 AI 경진대회/nk_dataset/mmsi_tracks'                        # AIS 항적 데이터 폴더\n","MAPPING_FILE = '/content/drive/MyDrive/25년 해군 AI 경진대회/nk_dataset/Anonymized_MMSI_list-매핑용.csv'  # MMSI 매핑 파일\n","\n","SEQ_LEN = 50        # 시퀀스 길이\n","BATCH_SIZE = 64\n","LATENT_DIM = 32     # 잠재차원\n","EPOCHS = 30\n","LR = 1e-3\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","source":["# -----------------------------\n","# 데이터 로드 및 매핑\n","# -----------------------------\n","mapping_df = pd.read_csv(MAPPING_FILE)\n","mapping = dict(zip(mapping_df['Anonymized_MMSI'].astype(str), mapping_df['MMSI'].astype(str)))\n","files = glob.glob(os.path.join(TRACK_DIR, '*.csv'))"],"metadata":{"id":"Wr6NgZg3Haux","executionInfo":{"status":"ok","timestamp":1748475179631,"user_tz":-540,"elapsed":24,"user":{"displayName":"YOONGEE","userId":"13265385226373116446"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# Dataset 정의: LSTM-AE 입력 시퀀스\n","# -----------------------------\n","class TrackSeqDataset(Dataset):\n","    def __init__(self, files, seq_len, scaler=None):\n","        self.seq_len = seq_len\n","        self.scaler = scaler or StandardScaler()\n","        data_all = []\n","        self.ids = []\n","        for f in files:\n","            df = pd.read_csv(f, parse_dates=['timestamp'])\n","            df = df.sort_values('timestamp').set_index('timestamp')\n","            df = df[['sog','cog','latitude','longitude']].resample('5s').mean().interpolate().dropna()\n","            arr = df.values\n","            data_all.append(arr)\n","            self.ids.append(os.path.splitext(os.path.basename(f))[0])\n","        concat = np.vstack(data_all)\n","        self.scaler.fit(concat)\n","        self.sequences = []\n","        self.seq_ids = []\n","        for arr, aid in zip(data_all, self.ids):\n","            normed = self.scaler.transform(arr)\n","            for i in range(len(normed) - seq_len + 1):\n","                self.sequences.append(normed[i:i+seq_len])\n","                self.seq_ids.append(aid)\n","\n","    def __len__(self):\n","        return len(self.sequences)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.sequences[idx], dtype=torch.float32), self.seq_ids[idx]"],"metadata":{"id":"PQ95dtsJHfLI","executionInfo":{"status":"ok","timestamp":1748475179648,"user_tz":-540,"elapsed":2,"user":{"displayName":"YOONGEE","userId":"13265385226373116446"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# LSTM Autoencoder 모델\n","# -----------------------------\n","class LSTMAutoencoder(nn.Module):\n","    def __init__(self, seq_len, n_feat, latent_dim):\n","        super().__init__()\n","        self.encoder = nn.LSTM(n_feat, latent_dim, batch_first=True)\n","        self.decoder = nn.LSTM(latent_dim, n_feat, batch_first=True)\n","    def forward(self, x):\n","        _, (h, _) = self.encoder(x)\n","        latent = h[-1].unsqueeze(1).repeat(1, x.size(1), 1)\n","        out, _ = self.decoder(latent)\n","        return out"],"metadata":{"id":"cUj2PXVvHiAg","executionInfo":{"status":"ok","timestamp":1748475179653,"user_tz":-540,"elapsed":2,"user":{"displayName":"YOONGEE","userId":"13265385226373116446"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# 학습 함수\n","# -----------------------------\n","def train_autoencoder(dataset):\n","    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n","    model = LSTMAutoencoder(SEQ_LEN, 4, LATENT_DIM).to(DEVICE)\n","    optim = torch.optim.Adam(model.parameters(), lr=LR)\n","    loss_fn = nn.MSELoss()\n","    model.train()\n","    for ep in range(1, EPOCHS+1):\n","        total_loss = 0\n","        for seqs, _ in loader:\n","            seqs = seqs.to(DEVICE)\n","            optim.zero_grad()\n","            recon = model(seqs)\n","            loss = loss_fn(recon, seqs)\n","            loss.backward()\n","            optim.step()\n","            total_loss += loss.item()\n","        print(f\"Epoch {ep}/{EPOCHS} | Loss: {total_loss/len(loader):.6f}\")\n","    return model"],"metadata":{"id":"c5RPrM_eHj02","executionInfo":{"status":"ok","timestamp":1748475179663,"user_tz":-540,"elapsed":7,"user":{"displayName":"YOONGEE","userId":"13265385226373116446"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# Reconstruction error 계산\n","# -----------------------------\n","def compute_errors(model, dataset):\n","    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n","    model.eval()\n","    errors = {aid: [] for aid in set(dataset.seq_ids)}\n","    with torch.no_grad():\n","        for seqs, aids in loader:\n","            seqs = seqs.to(DEVICE)\n","            recon = model(seqs)\n","            errs = ((recon - seqs)**2).mean(dim=(1,2)).cpu().numpy()\n","            for e, aid in zip(errs, aids):\n","                errors[aid].append(e)\n","    mean_err = {aid: np.mean(errs) for aid, errs in errors.items()}\n","    return mean_err"],"metadata":{"id":"q4hRXejfHlqP","executionInfo":{"status":"ok","timestamp":1748475179666,"user_tz":-540,"elapsed":1,"user":{"displayName":"YOONGEE","userId":"13265385226373116446"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# 이상치 감지 함수\n","# -----------------------------\n","def detect_anomalies(mean_errs, threshold=None, percentile=95):\n","    errs = np.array(list(mean_errs.values()))\n","    if threshold is None:\n","        threshold = np.percentile(errs, percentile)\n","        print(f\"Threshold set to {percentile}th percentile: {threshold:.6f}\")\n","    anomalies = [aid for aid, e in mean_errs.items() if e > threshold]\n","    return anomalies, threshold"],"metadata":{"id":"KKJ8IYy2HoFC","executionInfo":{"status":"ok","timestamp":1748475179700,"user_tz":-540,"elapsed":32,"user":{"displayName":"YOONGEE","userId":"13265385226373116446"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# Validation set 예측 및 제출 파일 생성\n","# -----------------------------\n","def predict_and_save(validation_file, track_dir, model, scaler, seq_len, threshold, output_file):\n","    val_df = pd.read_csv(validation_file)\n","    results = []\n","    for aid in val_df['Anonymized_MMSI'].astype(str):\n","        fpath = os.path.join(track_dir, f\"{aid}.csv\")\n","        if not os.path.exists(fpath):\n","            results.append({'Anonymized_MMSI': aid, 'result': 'FALSE'})\n","            continue\n","        df = pd.read_csv(fpath, parse_dates=['timestamp']).sort_values('timestamp').set_index('timestamp')\n","        df = df[['sog','cog','latitude','longitude']].resample('5s').mean().interpolate().dropna()\n","        arr = scaler.transform(df.values)\n","        seqs = []\n","        for i in range(len(arr) - seq_len + 1):\n","            seqs.append(arr[i:i+seq_len])\n","        if not seqs:\n","            mean_err = np.nan\n","        else:\n","            seqs_tensor = torch.tensor(np.stack(seqs), dtype=torch.float32).to(DEVICE)\n","            with torch.no_grad():\n","                recon = model(seqs_tensor)\n","            errs = ((recon - seqs_tensor)**2).mean(dim=(1,2)).cpu().numpy()\n","            mean_err = np.mean(errs)\n","        pred = True if mean_err > threshold else False\n","        results.append({'Anonymized_MMSI': aid, 'result': 'TRUE' if pred else 'FALSE'})\n","    out_df = pd.DataFrame(results)\n","    out_df.to_csv(output_file, index=False)\n","    print(f\"Saved predictions to {output_file}\")"],"metadata":{"id":"rbmD1SnHJTf9","executionInfo":{"status":"ok","timestamp":1748475179715,"user_tz":-540,"elapsed":11,"user":{"displayName":"YOONGEE","userId":"13265385226373116446"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# -----------------------------\n","# 메인 실행\n","# -----------------------------\n","if __name__ == '__main__':\n","    # Training 데이터셋 생성 및 학습\n","    dataset = TrackSeqDataset(files, SEQ_LEN)\n","    ae_model = train_autoencoder(dataset)\n","    torch.save(ae_model.state_dict(), 'lstm_ae_nk_detection.pth')\n","\n","    # Training set 오류 계산 및 이상치 탐지 (Threshold 결정)\n","    mean_errors = compute_errors(ae_model, dataset)\n","    anomalies, thr = detect_anomalies(mean_errors)\n","    print(f\"Detected anomalies (training): {anomalies}\")\n","\n","    # Validation set 예측 및 결과 저장\n","    validation_file = '/content/drive/MyDrive/25년 해군 AI 경진대회/2번문제/Anonymized_MMSI_list_validation(2번문제)_수정.csv'\n","    output_file = 'submission_nkmodel_ver1.csv'\n","    predict_and_save(validation_file, TRACK_DIR, ae_model, dataset.scaler, SEQ_LEN, thr, output_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GLZj2SOdHqCV","outputId":"1d7801bb-2718-4e99-f848-82c30856a701"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-3d1a63f2d9ba>:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n","  df = pd.read_csv(f, parse_dates=['timestamp'])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"zxRlaCFuH9Th"},"execution_count":null,"outputs":[]}]}