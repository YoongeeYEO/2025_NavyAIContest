{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3189092a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbf51870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Mixed Precision Training 지원 확인\n",
    "try:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    AMP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    AMP_AVAILABLE = False\n",
    "    print(\"Warning: Mixed Precision Training not available. Using standard training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "da0bae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "VESSEL DETECTION MODEL TRAINING\n",
      "======================================================================\n",
      "\n",
      "Select mode:\n",
      "1. Quick test (basic functionality check)\n",
      "2. Full pipeline test (complete test with small data)\n",
      "3. Full training (complete training with all data)\n",
      "\n",
      "Enter mode (1/2/3): 3\n",
      "\n",
      "======================================================================\n",
      "Starting full training...\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Cache file found: preprocessed_data_cache.pkl\n",
      "Use cached preprocessed data? (y/n) [default: y]: n\n",
      "Checking data availability...\n",
      "Total MMSI in meta_data: 753\n",
      "Available CSV files: 1299\n",
      "Valid MMSI (file + label): 640\n",
      "Warning: 659 files without labels in meta_data\n",
      "Warning: 113 MMSI in meta_data without files\n",
      "\n",
      "Class distribution in valid data:\n",
      "TRUE (NK vessels): 129 (20.2%)\n",
      "FALSE (non-NK vessels): 511 (79.8%)\n",
      "\n",
      "Use parallel processing? (y/n) [default: y]: \n",
      "\n",
      "Using 1 parallel workers for preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing vessels:   0%|          | 1/640 [00:12<2:13:54, 12.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large file detected: 286094249 (141.5 MB), using chunk reading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing vessels:   0%|          | 2/640 [00:32<2:58:29, 16.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many points: 286094249 (2678456 points), sampling to 1000000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-290:\n",
      "Traceback (most recent call last):\n",
      "Processing vessels:   0%|          | 2/640 [00:43<3:49:20, 21.57s/it]  File \"/usr/local/lib/python3.10/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.10/multiprocessing/pool.py\", line 131, in worker\n",
      "    put((job, i, result))\n",
      "  File \"/usr/local/lib/python3.10/multiprocessing/queues.py\", line 377, in put\n",
      "    self._writer.send_bytes(obj)\n",
      "  File \"/usr/local/lib/python3.10/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/local/lib/python3.10/multiprocessing/connection.py\", line 405, in _send_bytes\n",
      "    self._send(buf)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 375\u001b[0m, in \u001b[0;36mload_data_parallel\u001b[0;34m(data_path, meta_path, valid_meta_data, n_jobs)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mn_jobs) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m--> 375\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_single_vessel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessing vessels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# 유효한 결과만 필터링\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 861\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 1367\u001b[0m\n\u001b[1;32m   1363\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1367\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 1094\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1090\u001b[0m use_parallel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUse parallel processing? (y/n) [default: y]: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_parallel\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;66;03m# 병렬 처리\u001b[39;00m\n\u001b[0;32m-> 1094\u001b[0m     all_sequences, all_features, all_labels, all_mmsi \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_meta_data\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# 기존 순차 처리 (최적화된 버전)\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     all_sequences \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[67], line 374\u001b[0m, in \u001b[0;36mload_data_parallel\u001b[0;34m(data_path, meta_path, valid_meta_data, n_jobs)\u001b[0m\n\u001b[1;32m    371\u001b[0m     args_list\u001b[38;5;241m.\u001b[39mappend((mmsi, data_path, meta_path, label))\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# 병렬 처리\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mn_jobs) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m    375\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(tqdm(\n\u001b[1;32m    376\u001b[0m         pool\u001b[38;5;241m.\u001b[39mimap(process_single_vessel, args_list),\n\u001b[1;32m    377\u001b[0m         total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(args_list),\n\u001b[1;32m    378\u001b[0m         desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing vessels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    379\u001b[0m     ))\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# 유효한 결과만 필터링\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/multiprocessing/pool.py:739\u001b[0m, in \u001b[0;36mPool.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 739\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/multiprocessing/pool.py:657\u001b[0m, in \u001b[0;36mPool.terminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    655\u001b[0m util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterminating pool\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m TERMINATE\n\u001b[0;32m--> 657\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_terminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/multiprocessing/util.py:224\u001b[0m, in \u001b[0;36mFinalize.__call__\u001b[0;34m(self, wr, _finalizer_registry, sub_debug, getpid)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    222\u001b[0m     sub_debug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinalizer calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with args \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and kwargs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    223\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs)\n\u001b[0;32m--> 224\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weakref \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    226\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/multiprocessing/pool.py:724\u001b[0m, in \u001b[0;36mPool._terminate_pool\u001b[0;34m(cls, taskqueue, inqueue, outqueue, pool, change_notifier, worker_handler, task_handler, result_handler, cache)\u001b[0m\n\u001b[1;32m    722\u001b[0m util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoining result handler\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threading\u001b[38;5;241m.\u001b[39mcurrent_thread() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result_handler:\n\u001b[0;32m--> 724\u001b[0m     \u001b[43mresult_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pool \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(pool[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterminate\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    727\u001b[0m     util\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjoining pool workers\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Mixed Precision Training 지원 확인\n",
    "try:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    AMP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    AMP_AVAILABLE = False\n",
    "    print(\"Warning: Mixed Precision Training not available. Using standard training.\")\n",
    "\n",
    "\n",
    "class VesselBehaviorAnalyzer:\n",
    "    \"\"\"선박 행동 패턴 분석기 - 속도 최적화 버전\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, meta_path):\n",
    "        self.data_path = data_path\n",
    "        self.meta_data = pd.read_csv(meta_path)\n",
    "        self.nll_latitude = 38.0  # 북방한계선 대략적 위도\n",
    "        \n",
    "    def load_and_preprocess(self, mmsi):\n",
    "        \"\"\"데이터 로드 및 전처리 - 최적화된 버전\"\"\"\n",
    "        file_path = os.path.join(self.data_path, f\"{mmsi}.csv\")\n",
    "\n",
    "        # 파일 크기 확인\n",
    "        file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "\n",
    "        # 대용량 파일 처리\n",
    "        if file_size > 50:  # 50MB 이상\n",
    "            print(f\"Large file detected: {mmsi} ({file_size:.1f} MB), using chunk reading...\")\n",
    "            chunks = []\n",
    "            for chunk in pd.read_csv(file_path, chunksize=100000):\n",
    "                chunks.append(chunk)\n",
    "            df = pd.concat(chunks, ignore_index=True)\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "        # 데이터 포인트가 너무 많으면 샘플링\n",
    "        MAX_POINTS = 1000000\n",
    "        if len(df) > MAX_POINTS:\n",
    "            print(f\"Too many points: {mmsi} ({len(df)} points), sampling to {MAX_POINTS}...\")\n",
    "            step = len(df) // MAX_POINTS\n",
    "            df = df.iloc[::step]\n",
    "\n",
    "        # 시간대 처리\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "        df = df.sort_values('timestamp')\n",
    "\n",
    "        # 중복 타임스탬프 처리 - drop_duplicates가 groupby보다 빠름\n",
    "        df = df.drop_duplicates(subset=['timestamp'], keep='first')\n",
    "\n",
    "        # 30초 간격 리샘플링으로 변경 (5초 -> 30초)\n",
    "        df.set_index('timestamp', inplace=True)\n",
    "\n",
    "        # 데이터가 충분히 조밀하면 리샘플링 생략\n",
    "        time_diff = df.index.to_series().diff().dt.total_seconds()\n",
    "        median_diff = time_diff.median()\n",
    "\n",
    "        if median_diff > 60:  # 중간값이 60초 초과면 리샘플링\n",
    "            df = df.resample('30s').mean()  # 30초 간격으로 변경\n",
    "            df = df.interpolate(method='linear', limit=3)\n",
    "\n",
    "        df = df.dropna()\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def extract_behavioral_features(self, df):\n",
    "        \"\"\"행동 패턴 기반 특징 추출 - 벡터화 최적화\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # NumPy 배열로 변환하여 더 빠른 연산\n",
    "        lat = df['latitude'].values\n",
    "        lon = df['longitude'].values\n",
    "        sog = df['sog'].values\n",
    "        cog = df['cog'].values\n",
    "        \n",
    "        # 1. 속도 패턴 분석 - 벡터화\n",
    "        features['avg_speed'] = np.mean(sog)\n",
    "        features['speed_std'] = np.std(sog) if len(sog) > 1 else 0\n",
    "        features['speed_q25'] = np.percentile(sog, 25)\n",
    "        features['speed_q75'] = np.percentile(sog, 75)\n",
    "        features['zero_speed_ratio'] = np.mean(sog < 0.5)\n",
    "        features['high_speed_ratio'] = np.mean(sog > 15)\n",
    "        \n",
    "        # 속도 변화율\n",
    "        if len(sog) > 1:\n",
    "            speed_change = np.abs(np.diff(sog))\n",
    "            features['avg_speed_change'] = np.mean(speed_change)\n",
    "            features['sudden_speed_changes'] = int(np.sum(speed_change > 5))\n",
    "        else:\n",
    "            features['avg_speed_change'] = 0\n",
    "            features['sudden_speed_changes'] = 0\n",
    "        \n",
    "        # 2. 방향 패턴 분석 - 벡터화\n",
    "        if len(cog) > 1:\n",
    "            cog_diff = np.diff(cog)\n",
    "            # 각도 차이를 -180 ~ 180 범위로 정규화\n",
    "            cog_diff = (cog_diff + 180) % 360 - 180\n",
    "            \n",
    "            features['avg_course_change'] = np.mean(np.abs(cog_diff))\n",
    "            features['zigzag_score'] = np.std(cog_diff)\n",
    "            features['sharp_turns'] = int(np.sum(np.abs(cog_diff) > 45))\n",
    "            features['u_turns'] = int(np.sum(np.abs(cog_diff) > 150))\n",
    "        else:\n",
    "            features['avg_course_change'] = 0\n",
    "            features['zigzag_score'] = 0\n",
    "            features['sharp_turns'] = 0\n",
    "            features['u_turns'] = 0\n",
    "        \n",
    "        # 3. 공간 패턴 분석\n",
    "        features['lat_range'] = lat.max() - lat.min()\n",
    "        features['lon_range'] = lon.max() - lon.min()\n",
    "        features['max_latitude'] = lat.max()\n",
    "        features['min_latitude'] = lat.min()\n",
    "        \n",
    "        # NLL 관련 특징\n",
    "        features['north_of_nll_ratio'] = np.mean(lat > self.nll_latitude)\n",
    "        features['nll_crossings'] = self._count_nll_crossings_fast(lat)\n",
    "        features['max_north_distance'] = lat.max() - self.nll_latitude\n",
    "        \n",
    "        # 4. 이동 패턴 분석 - 벡터화된 거리 계산\n",
    "        features['total_distance'] = self._calculate_trajectory_distance_vectorized(lat, lon)\n",
    "        features['displacement'] = self._calculate_displacement_fast(lat, lon)\n",
    "        features['trajectory_efficiency'] = (\n",
    "            features['displacement'] / features['total_distance'] \n",
    "            if features['total_distance'] > 0 else 0\n",
    "        )\n",
    "        \n",
    "        # 5. 정박/대기 패턴 - 벡터화\n",
    "        stationary_features = self._find_stationary_periods_vectorized(sog)\n",
    "        features.update(stationary_features)\n",
    "        \n",
    "        # 6. 활동 시간 패턴\n",
    "        if hasattr(df.index, 'hour'):\n",
    "            hours = df.index.hour\n",
    "            features['night_activity_ratio'] = np.mean((hours >= 20) | (hours <= 5))\n",
    "            features['dawn_activity_ratio'] = np.mean((hours >= 3) & (hours <= 6))\n",
    "        else:\n",
    "            features['night_activity_ratio'] = 0\n",
    "            features['dawn_activity_ratio'] = 0\n",
    "        \n",
    "        # 7. 궤적 복잡도 - 제거됨 (heading_entropy, location_revisits)\n",
    "        # features['heading_entropy'] = self._calculate_heading_entropy_fast(cog)\n",
    "        # features['location_revisits'] = self._count_location_revisits_fast(lat, lon)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _count_nll_crossings_fast(self, lat):\n",
    "        \"\"\"NLL 경계선 통과 횟수 - 벡터화\"\"\"\n",
    "        north_flags = lat > self.nll_latitude\n",
    "        if len(north_flags) > 1:\n",
    "            crossings = np.sum(np.diff(north_flags.astype(int)) != 0)\n",
    "        else:\n",
    "            crossings = 0\n",
    "        return int(crossings)\n",
    "    \n",
    "    def _calculate_trajectory_distance_vectorized(self, lat, lon):\n",
    "        \"\"\"벡터화된 총 이동 거리 계산\"\"\"\n",
    "        if len(lat) < 2:\n",
    "            return 0\n",
    "        \n",
    "        # 라디안 변환\n",
    "        lat_rad = np.radians(lat)\n",
    "        lon_rad = np.radians(lon)\n",
    "        \n",
    "        # 연속된 점들 간의 차이\n",
    "        dlat = lat_rad[1:] - lat_rad[:-1]\n",
    "        dlon = lon_rad[1:] - lon_rad[:-1]\n",
    "        \n",
    "        # Haversine 공식 벡터화\n",
    "        a = (np.sin(dlat/2)**2 + \n",
    "             np.cos(lat_rad[:-1]) * np.cos(lat_rad[1:]) * \n",
    "             np.sin(dlon/2)**2)\n",
    "        c = 2 * np.arcsin(np.sqrt(np.minimum(1, a)))  # 수치 안정성\n",
    "        \n",
    "        distances = 6371 * c  # 지구 반지름 (km)\n",
    "        return float(np.sum(distances))\n",
    "    \n",
    "    def _calculate_displacement_fast(self, lat, lon):\n",
    "        \"\"\"시작점-끝점 직선 거리 - 벡터화\"\"\"\n",
    "        if len(lat) < 1:\n",
    "            return 0\n",
    "            \n",
    "        lat1, lon1 = np.radians(lat[0]), np.radians(lon[0])\n",
    "        lat2, lon2 = np.radians(lat[-1]), np.radians(lon[-1])\n",
    "        \n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        \n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(np.minimum(1, a)))\n",
    "        \n",
    "        return float(6371 * c)\n",
    "    \n",
    "    def _find_stationary_periods_vectorized(self, sog):\n",
    "        \"\"\"정박 기간 찾기 - 벡터화\"\"\"\n",
    "        stationary = sog < 0.5\n",
    "        \n",
    "        if len(stationary) == 0:\n",
    "            return {\n",
    "                'num_stops': 0,\n",
    "                'avg_stop_duration': 0,\n",
    "                'longest_stop': 0\n",
    "            }\n",
    "        \n",
    "        # 상태 변화 감지\n",
    "        padded = np.concatenate([[False], stationary, [False]])\n",
    "        diff = np.diff(padded.astype(int))\n",
    "        starts = np.where(diff == 1)[0]\n",
    "        ends = np.where(diff == -1)[0]\n",
    "        \n",
    "        if len(starts) == 0 or len(ends) == 0:\n",
    "            return {\n",
    "                'num_stops': 0,\n",
    "                'avg_stop_duration': 0,\n",
    "                'longest_stop': 0\n",
    "            }\n",
    "        \n",
    "        # 시작과 끝 매칭\n",
    "        if ends[0] < starts[0]:\n",
    "            ends = ends[1:]\n",
    "        if len(starts) > len(ends):\n",
    "            starts = starts[:len(ends)]\n",
    "        \n",
    "        durations = (ends - starts) * 30  # 30초 간격으로 변경\n",
    "        \n",
    "        return {\n",
    "            'num_stops': int(len(starts)),\n",
    "            'avg_stop_duration': float(np.mean(durations)) if len(durations) > 0 else 0,\n",
    "            'longest_stop': float(np.max(durations)) if len(durations) > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def _calculate_heading_entropy_fast(self, cog):\n",
    "        \"\"\"방향 엔트로피 계산 - 최적화\"\"\"\n",
    "        if len(cog) == 0 or np.all(np.isnan(cog)):\n",
    "            return 0\n",
    "            \n",
    "        # NaN 제거\n",
    "        cog_clean = cog[~np.isnan(cog)]\n",
    "        if len(cog_clean) == 0:\n",
    "            return 0\n",
    "        \n",
    "        # 8방향으로 양자화\n",
    "        bins = np.arange(0, 361, 45)\n",
    "        hist, _ = np.histogram(cog_clean, bins=bins)\n",
    "        \n",
    "        # 확률 계산\n",
    "        hist = hist + 1e-10  # 0 방지\n",
    "        hist = hist / hist.sum()\n",
    "        \n",
    "        # 엔트로피\n",
    "        entropy = -np.sum(hist * np.log(hist))\n",
    "        return float(entropy)\n",
    "    \n",
    "    def _count_location_revisits_fast(self, lat, lon):\n",
    "        \"\"\"위치 재방문 횟수 - 최적화\"\"\"\n",
    "        if len(lat) == 0:\n",
    "            return 0\n",
    "            \n",
    "        # 그리드 양자화\n",
    "        lat_grid = np.round(lat * 100).astype(int)\n",
    "        lon_grid = np.round(lon * 100).astype(int)\n",
    "        \n",
    "        # 유니크 위치와 카운트\n",
    "        locations = np.column_stack([lat_grid, lon_grid])\n",
    "        unique_locs, counts = np.unique(locations, axis=0, return_counts=True)\n",
    "        \n",
    "        return int(np.sum(counts > 1))\n",
    "\n",
    "\n",
    "class SequenceFeatureExtractor:\n",
    "    \"\"\"시퀀스 데이터에서 추가 특징 추출\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=20):\n",
    "        self.window_size = window_size\n",
    "        \n",
    "    def extract_sequence_features(self, df):\n",
    "        \"\"\"슬라이딩 윈도우 기반 시퀀스 특징 - rolling window 제거\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # 기본 특징\n",
    "        base_features = ['latitude', 'longitude', 'sog', 'cog']\n",
    "        \n",
    "        # 파생 특징 추가\n",
    "        df['lat_diff'] = df['latitude'].diff()\n",
    "        df['lon_diff'] = df['longitude'].diff()\n",
    "        df['speed_acc'] = df['sog'].diff()\n",
    "        df['course_change'] = df['cog'].diff().abs()\n",
    "        \n",
    "        # rolling window 제거 - 불필요한 연산 제거\n",
    "        # df['sog_ma'] = df['sog'].rolling(window=5, min_periods=1).mean()\n",
    "        # df['cog_ma'] = df['cog'].rolling(window=5, min_periods=1).mean()\n",
    "        # df['sog_std'] = df['sog'].rolling(window=10, min_periods=1).std()\n",
    "        # df['cog_std'] = df['cog'].rolling(window=10, min_periods=1).std()\n",
    "        \n",
    "        # 북방 이동 지표\n",
    "        df['northward'] = (df['lat_diff'] > 0).astype(int)\n",
    "        df['near_nll'] = (df['latitude'] > 37.5).astype(int)\n",
    "        \n",
    "        feature_cols = base_features + ['lat_diff', 'lon_diff', 'speed_acc', \n",
    "                                       'course_change', 'northward', 'near_nll']\n",
    "        \n",
    "        return df[feature_cols].fillna(0).values\n",
    "\n",
    "\n",
    "def process_single_vessel(args):\n",
    "    \"\"\"병렬 처리를 위한 단일 선박 처리 함수\"\"\"\n",
    "    mmsi, data_path, meta_path, label = args\n",
    "    \n",
    "    try:\n",
    "        # 분석기 생성\n",
    "        analyzer = VesselBehaviorAnalyzer(data_path, meta_path)\n",
    "        seq_extractor = SequenceFeatureExtractor()\n",
    "        \n",
    "        # 데이터 로드 및 전처리\n",
    "        vessel_data = analyzer.load_and_preprocess(mmsi)\n",
    "        \n",
    "        if len(vessel_data) < 50:\n",
    "            return None\n",
    "        \n",
    "        # 특징 추출\n",
    "        behavioral_features = analyzer.extract_behavioral_features(vessel_data)\n",
    "        sequence_features = seq_extractor.extract_sequence_features(vessel_data)\n",
    "        \n",
    "        return {\n",
    "            'mmsi': mmsi,\n",
    "            'sequence': sequence_features,\n",
    "            'features': behavioral_features,\n",
    "            'label': label\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_data_parallel(data_path, meta_path, valid_meta_data, n_jobs=None):\n",
    "    \"\"\"병렬 처리를 사용한 데이터 로드\"\"\"\n",
    "    if n_jobs is None:\n",
    "        n_jobs = min(cpu_count() - 1, 8)\n",
    "    \n",
    "    print(f\"\\nUsing {n_jobs} parallel workers for preprocessing...\")\n",
    "    \n",
    "    # 병렬 처리를 위한 인자 준비\n",
    "    args_list = []\n",
    "    for idx, row in valid_meta_data.iterrows():\n",
    "        if 'MMSI' in valid_meta_data.columns:\n",
    "            mmsi = str(row['MMSI'])\n",
    "        else:\n",
    "            mmsi = str(row['mmsi'])\n",
    "        \n",
    "        if 'label' in valid_meta_data.columns:\n",
    "            label = int(row['label'])  # boolean을 int로 변환\n",
    "        else:\n",
    "            label = int(row['result'])\n",
    "        \n",
    "        args_list.append((mmsi, data_path, meta_path, label))\n",
    "    \n",
    "    # 병렬 처리\n",
    "    with Pool(processes=n_jobs) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap(process_single_vessel, args_list),\n",
    "            total=len(args_list),\n",
    "            desc=\"Processing vessels\"\n",
    "        ))\n",
    "    \n",
    "    # 유효한 결과만 필터링\n",
    "    valid_results = [r for r in results if r is not None]\n",
    "    \n",
    "    # 결과 분리\n",
    "    all_sequences = [r['sequence'] for r in valid_results]\n",
    "    all_features = [r['features'] for r in valid_results]\n",
    "    all_labels = [r['label'] for r in valid_results]\n",
    "    all_mmsi = [r['mmsi'] for r in valid_results]\n",
    "    \n",
    "    return all_sequences, all_features, all_labels, all_mmsi\n",
    "\n",
    "\n",
    "class VesselDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for vessel trajectories\"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, labels, seq_length=200, augment=True):\n",
    "        self.data_list = data_list\n",
    "        self.labels = labels\n",
    "        self.seq_length = seq_length\n",
    "        self.augment = augment\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_list[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 데이터 증강\n",
    "        if self.augment and np.random.random() > 0.5:\n",
    "            data = self._augment_sequence(data)\n",
    "        \n",
    "        # 시퀀스 길이 조정\n",
    "        if len(data) > self.seq_length:\n",
    "            # 랜덤 크롭\n",
    "            start = np.random.randint(0, len(data) - self.seq_length)\n",
    "            data = data[start:start + self.seq_length]\n",
    "        else:\n",
    "            # 패딩\n",
    "            pad_length = self.seq_length - len(data)\n",
    "            padding = np.zeros((pad_length, data.shape[1]))\n",
    "            data = np.vstack([data, padding])\n",
    "            \n",
    "        return torch.FloatTensor(data), torch.FloatTensor([label])\n",
    "    \n",
    "    def _augment_sequence(self, data):\n",
    "        \"\"\"데이터 증강\"\"\"\n",
    "        augmented = data.copy()\n",
    "        \n",
    "        # 1. 노이즈 추가\n",
    "        if np.random.random() > 0.5:\n",
    "            noise = np.random.normal(0, 0.01, augmented.shape)\n",
    "            augmented[:, :4] += noise[:, :4]  # 위치와 속도에만 노이즈\n",
    "            \n",
    "        # 2. 시간 축 스케일링 (일부 구간 스킵)\n",
    "        if np.random.random() > 0.5:\n",
    "            indices = np.sort(np.random.choice(len(augmented), \n",
    "                                             int(len(augmented) * 0.9), \n",
    "                                             replace=False))\n",
    "            augmented = augmented[indices]\n",
    "            \n",
    "        return augmented\n",
    "\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    \"\"\"단순화된 LSTM 모델 (autocast 호환)\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=10, hidden_dim=64, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 입력 정규화\n",
    "        self.input_norm = nn.BatchNorm1d(input_dim)\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, \n",
    "                           num_layers=num_layers,\n",
    "                           batch_first=True, \n",
    "                           bidirectional=True,\n",
    "                           dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        # Global average pooling 대신 마지막 hidden state 사용\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(32, 1)  # logits 출력 (sigmoid 제거)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.size()\n",
    "        \n",
    "        # 입력 정규화 (시퀀스 차원을 평탄화)\n",
    "        x = x.reshape(-1, features)\n",
    "        x = self.input_norm(x)\n",
    "        x = x.reshape(batch_size, seq_len, features)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, (hidden, _) = self.lstm(x)\n",
    "        \n",
    "        # 마지막 타임스텝의 hidden state 사용\n",
    "        # hidden shape: (num_layers * 2, batch, hidden_dim)\n",
    "        # 양방향 LSTM의 마지막 레이어 출력을 결합\n",
    "        forward_hidden = hidden[-2, :, :]  # 마지막 레이어의 forward\n",
    "        backward_hidden = hidden[-1, :, :]  # 마지막 레이어의 backward\n",
    "        combined_hidden = torch.cat([forward_hidden, backward_hidden], dim=1)\n",
    "        \n",
    "        # Classification (logits 반환)\n",
    "        logits = self.classifier(combined_hidden)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class OptimizedFocalLoss(nn.Module):\n",
    "    \"\"\"클래스 분포를 반영한 최적화된 Focal Loss (logits 입력용)\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.19, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # TRUE 클래스 비율 (19%)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs는 이제 logits (sigmoid 적용 전)\n",
    "        # binary_cross_entropy_with_logits 사용 (autocast 안전)\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, targets, reduction='none'\n",
    "        )\n",
    "        \n",
    "        # 예측 확률 계산\n",
    "        pt = torch.sigmoid(inputs) * targets + (1 - torch.sigmoid(inputs)) * (1 - targets)\n",
    "        \n",
    "        # 클래스별 가중치 적용\n",
    "        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        focal_loss = alpha_t * (1 - pt) ** self.gamma * bce_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "def find_optimal_threshold(y_true, y_pred_proba):\n",
    "    \"\"\"F1 Score를 최대화하는 최적 임계값 찾기\"\"\"\n",
    "    thresholds = np.arange(0.3, 0.9, 0.01)\n",
    "    best_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba > threshold).astype(int)\n",
    "        \n",
    "        # F1 계산\n",
    "        tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "        fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "        fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_f1\n",
    "\n",
    "\n",
    "def train_model_with_strategy(model, train_loader, val_loader, epochs=50, device='cuda'):\n",
    "    \"\"\"클래스 분포를 고려한 전략적 모델 학습 (GPU 최적화 적용)\"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 최적화된 손실 함수 (테스트셋 TRUE 비율 19% 반영)\n",
    "    criterion = OptimizedFocalLoss(alpha=0.19, gamma=2.0)\n",
    "    \n",
    "    # 단순화된 옵티마이저\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, patience=5, factor=0.5, mode='max'\n",
    "    )\n",
    "    \n",
    "    # Mixed Precision Training을 위한 GradScaler\n",
    "    scaler = GradScaler() if AMP_AVAILABLE else None\n",
    "    \n",
    "    best_val_f1 = 0\n",
    "    best_threshold = 0.5\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        \n",
    "        for batch_x, batch_y in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if AMP_AVAILABLE:\n",
    "                # Mixed Precision Training\n",
    "                with autocast():\n",
    "                    logits = model(batch_x)  # 이제 logits 반환\n",
    "                    loss = criterion(logits, batch_y)\n",
    "                \n",
    "                # Scaled backward pass\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                \n",
    "                # Optimizer step with scaler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard training\n",
    "                logits = model(batch_x)\n",
    "                loss = criterion(logits, batch_y)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            # 예측을 위해 sigmoid 적용\n",
    "            train_preds.extend(torch.sigmoid(logits).detach().cpu().numpy())\n",
    "            train_labels.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                \n",
    "                if AMP_AVAILABLE:\n",
    "                    # Mixed Precision for validation\n",
    "                    with autocast():\n",
    "                        logits = model(batch_x)\n",
    "                else:\n",
    "                    logits = model(batch_x)\n",
    "                \n",
    "                # 예측을 위해 sigmoid 적용\n",
    "                val_preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                val_labels.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        # 최적 임계값 찾기\n",
    "        val_preds = np.array(val_preds)\n",
    "        val_labels = np.array(val_labels)\n",
    "        \n",
    "        current_threshold, current_f1 = find_optimal_threshold(val_labels, val_preds)\n",
    "        \n",
    "        # 기본 임계값(0.5)로도 평가\n",
    "        val_preds_binary = (val_preds > 0.5).astype(int)\n",
    "        default_f1 = f1_score(val_labels, val_preds_binary)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Loss={train_loss/len(train_loader):.4f}, '\n",
    "              f'F1@0.5={default_f1:.4f}, F1@opt={current_f1:.4f} (threshold={current_threshold:.3f})')\n",
    "        \n",
    "        # 최고 성능 모델 저장\n",
    "        if current_f1 > best_val_f1:\n",
    "            best_val_f1 = current_f1\n",
    "            best_threshold = current_threshold\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'threshold': best_threshold,\n",
    "                'f1_score': best_val_f1\n",
    "            }, 'best_vessel_model_strategic.pth')\n",
    "        \n",
    "        scheduler.step(current_f1)\n",
    "    \n",
    "    print(f\"\\nBest validation F1: {best_val_f1:.4f} at threshold {best_threshold:.3f}\")\n",
    "    \n",
    "    return model, best_threshold\n",
    "\n",
    "\n",
    "def quick_test(data_path='data/', meta_path='meta_data.csv', test_size=10):\n",
    "    \"\"\"소량의 데이터로 빠른 테스트 실행\"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(f\"QUICK TEST MODE - Testing with {test_size} samples\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 데이터 분석기 초기화\n",
    "    analyzer = VesselBehaviorAnalyzer(data_path, meta_path)\n",
    "    seq_extractor = SequenceFeatureExtractor()\n",
    "    \n",
    "    # 메타데이터 로드 및 샘플링\n",
    "    if os.path.exists(meta_path):\n",
    "        # 샘플 데이터만 선택\n",
    "        sample_meta = analyzer.meta_data.sample(n=min(test_size, len(analyzer.meta_data)), random_state=42)\n",
    "        print(f\"Sampled {len(sample_meta)} vessels for testing\")\n",
    "    else:\n",
    "        print(f\"Error: meta_data file not found at {meta_path}\")\n",
    "        return False\n",
    "    \n",
    "    # 데이터 로드 테스트\n",
    "    success_count = 0\n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"\\nTesting data loading...\")\n",
    "    for idx, row in sample_meta.iterrows():\n",
    "        # MMSI 컬럼명 처리\n",
    "        if 'MMSI' in sample_meta.columns:\n",
    "            mmsi = str(row['MMSI'])\n",
    "        else:\n",
    "            mmsi = str(row['mmsi'])\n",
    "        \n",
    "        # 레이블 컬럼명 처리\n",
    "        if 'label' in sample_meta.columns:\n",
    "            label = int(row['label'])  # boolean을 int로 변환\n",
    "        else:\n",
    "            label = int(row['result'])\n",
    "        \n",
    "        try:\n",
    "            file_path = os.path.join(data_path, f\"{mmsi}.csv\")\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"  - File not found: {file_path}\")\n",
    "                continue\n",
    "            \n",
    "            # 데이터 로드\n",
    "            vessel_data = analyzer.load_and_preprocess(mmsi)\n",
    "            \n",
    "            if len(vessel_data) < 50:\n",
    "                print(f\"  - MMSI {mmsi}: Too few data points ({len(vessel_data)})\")\n",
    "                continue\n",
    "            \n",
    "            # 특징 추출\n",
    "            behavioral_features = analyzer.extract_behavioral_features(vessel_data)\n",
    "            sequence_features = seq_extractor.extract_sequence_features(vessel_data)\n",
    "            \n",
    "            all_sequences.append(sequence_features)\n",
    "            all_labels.append(label)\n",
    "            success_count += 1\n",
    "            print(f\"  ✓ MMSI {mmsi}: Successfully processed ({len(vessel_data)} points)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ MMSI {mmsi}: Error - {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nSuccessfully loaded: {success_count}/{len(sample_meta)} vessels\")\n",
    "    \n",
    "    if success_count < 2:\n",
    "        print(\"ERROR: Not enough data for testing. Need at least 2 samples.\")\n",
    "        return False\n",
    "    \n",
    "    # 간단한 모델 테스트\n",
    "    print(\"\\nTesting model training...\")\n",
    "    try:\n",
    "        # 데이터셋 생성\n",
    "        all_labels = np.array(all_labels)\n",
    "        dataset = VesselDataset(all_sequences, all_labels, augment=True)\n",
    "        dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "        \n",
    "        # 모델 초기화 - input_dim 변경 (14 -> 10)\n",
    "        model = SimpleLSTM(input_dim=10).to(device)\n",
    "        criterion = OptimizedFocalLoss(alpha=0.19, gamma=2.0)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # 1 에폭만 테스트\n",
    "        model.train()\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            if AMP_AVAILABLE:\n",
    "                try:\n",
    "                    from torch.cuda.amp import autocast\n",
    "                    with autocast():\n",
    "                        logits = model(batch_x)\n",
    "                        loss = criterion(logits, batch_y)\n",
    "                except:\n",
    "                    logits = model(batch_x)\n",
    "                    loss = criterion(logits, batch_y)\n",
    "            else:\n",
    "                logits = model(batch_x)\n",
    "                loss = criterion(logits, batch_y)\n",
    "            \n",
    "            print(f\"  ✓ Forward pass successful\")\n",
    "            print(f\"    - Input shape: {batch_x.shape}\")\n",
    "            print(f\"    - Output shape: {logits.shape}\")\n",
    "            print(f\"    - Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"  ✓ Backward pass successful\")\n",
    "            \n",
    "            break  # 한 배치만 테스트\n",
    "        \n",
    "        print(\"\\n✓ All tests passed! Ready for full training.\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Model test failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "def full_pipeline_test(data_path='data/', meta_path='meta_data.csv', test_size=20, epochs=3):\n",
    "    \"\"\"전체 파이프라인을 소량의 데이터로 테스트 (학습 포함)\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(f\"FULL PIPELINE TEST MODE\")\n",
    "    print(f\"Testing with {test_size} samples and {epochs} epochs\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 1. 데이터 로드 및 전처리 테스트\n",
    "    print(\"\\n[Step 1/5] Data Loading and Preprocessing\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    analyzer = VesselBehaviorAnalyzer(data_path, meta_path)\n",
    "    seq_extractor = SequenceFeatureExtractor()\n",
    "    \n",
    "    # 샘플 데이터 선택 (클래스 균형 유지)\n",
    "    if 'label' in analyzer.meta_data.columns:\n",
    "        label_col = 'label'\n",
    "        true_samples = analyzer.meta_data[analyzer.meta_data[label_col] == True]\n",
    "        false_samples = analyzer.meta_data[analyzer.meta_data[label_col] == False]\n",
    "    else:\n",
    "        label_col = 'result'\n",
    "        true_samples = analyzer.meta_data[analyzer.meta_data[label_col] == True]\n",
    "        false_samples = analyzer.meta_data[analyzer.meta_data[label_col] == False]\n",
    "    \n",
    "    # 샘플링\n",
    "    true_samples = true_samples.sample(\n",
    "        n=min(test_size//2, len(true_samples)), \n",
    "        random_state=42\n",
    "    )\n",
    "    false_samples = false_samples.sample(\n",
    "        n=min(test_size//2, len(false_samples)), \n",
    "        random_state=42\n",
    "    )\n",
    "    sample_meta = pd.concat([true_samples, false_samples])\n",
    "    \n",
    "    print(f\"Sampled {len(sample_meta)} vessels (TRUE: {len(true_samples)}, FALSE: {len(false_samples)})\")\n",
    "    \n",
    "    # 데이터 로드\n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "    all_mmsi = []\n",
    "    \n",
    "    for idx, row in sample_meta.iterrows():\n",
    "        mmsi = str(row.get('MMSI', row.get('mmsi')))\n",
    "        label = int(row[label_col])  # boolean을 int로 변환\n",
    "        \n",
    "        try:\n",
    "            vessel_data = analyzer.load_and_preprocess(mmsi)\n",
    "            if len(vessel_data) < 50:\n",
    "                continue\n",
    "                \n",
    "            behavioral_features = analyzer.extract_behavioral_features(vessel_data)\n",
    "            sequence_features = seq_extractor.extract_sequence_features(vessel_data)\n",
    "            \n",
    "            all_sequences.append(sequence_features)\n",
    "            all_labels.append(label)\n",
    "            all_mmsi.append(mmsi)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully loaded: {len(all_sequences)} vessels\")\n",
    "    \n",
    "    if len(all_sequences) < 4:\n",
    "        print(\"ERROR: Not enough data for pipeline test. Need at least 4 samples.\")\n",
    "        return False\n",
    "    \n",
    "    # 2. Train/Validation Split 테스트\n",
    "    print(\"\\n[Step 2/5] Train/Validation Split\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    X_train_idx, X_val_idx, y_train, y_val = train_test_split(\n",
    "        np.arange(len(all_sequences)), all_labels, \n",
    "        test_size=0.3, \n",
    "        stratify=all_labels, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_sequences = [all_sequences[i] for i in X_train_idx]\n",
    "    val_sequences = [all_sequences[i] for i in X_val_idx]\n",
    "    \n",
    "    print(f\"Train set: {len(train_sequences)} vessels (TRUE: {y_train.sum()}, FALSE: {len(y_train)-y_train.sum()})\")\n",
    "    print(f\"Val set: {len(val_sequences)} vessels (TRUE: {y_val.sum()}, FALSE: {len(y_val)-y_val.sum()})\")\n",
    "    \n",
    "    # 3. 데이터셋 및 DataLoader 생성 테스트\n",
    "    print(\"\\n[Step 3/5] Dataset and DataLoader Creation\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    train_dataset = VesselDataset(train_sequences, y_train, augment=True)\n",
    "    val_dataset = VesselDataset(val_sequences, y_val, augment=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    print(f\"Train DataLoader: {len(train_loader)} batches\")\n",
    "    print(f\"Val DataLoader: {len(val_loader)} batches\")\n",
    "    \n",
    "    # 4. 모델 학습 테스트\n",
    "    print(\"\\n[Step 4/5] Model Training\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    model = SimpleLSTM(input_dim=10).to(device)\n",
    "    criterion = OptimizedFocalLoss(alpha=0.19, gamma=2.0)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch_x)\n",
    "            loss = criterion(logits, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                logits = model(batch_x)\n",
    "                loss = criterion(logits, batch_y)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # sigmoid 적용하여 확률로 변환\n",
    "                val_preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                val_true.extend(batch_y.cpu().numpy())\n",
    "        \n",
    "        # 성능 평가\n",
    "        val_preds = np.array(val_preds).flatten()\n",
    "        val_true = np.array(val_true).flatten()\n",
    "        val_binary = (val_preds > 0.5).astype(int)\n",
    "        \n",
    "        if val_true.sum() > 0 and (1-val_true).sum() > 0:  # 두 클래스 모두 있는 경우\n",
    "            val_f1 = f1_score(val_true, val_binary)\n",
    "        else:\n",
    "            val_f1 = 0.0\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader):.4f}, Val F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # 5. 예측 및 평가 테스트\n",
    "    print(\"\\n[Step 5/5] Prediction and Evaluation\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    # 최종 예측\n",
    "    final_preds = (val_preds > 0.5).astype(int)\n",
    "    \n",
    "    print(f\"Predictions: TRUE={final_preds.sum()}, FALSE={len(final_preds)-final_preds.sum()}\")\n",
    "    print(f\"Ground Truth: TRUE={val_true.sum()}, FALSE={len(val_true)-val_true.sum()}\")\n",
    "    \n",
    "    if val_true.sum() > 0 and (1-val_true).sum() > 0:\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(val_true, final_preds, \n",
    "                                  target_names=['Non-NK', 'NK vessel'], \n",
    "                                  zero_division=0))\n",
    "    \n",
    "    # 모델 저장 테스트\n",
    "    try:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'test_completed': True\n",
    "        }, 'test_model.pth')\n",
    "        print(\"\\n✓ Model save test successful\")\n",
    "        os.remove('test_model.pth')  # 테스트 파일 삭제\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Model save test failed: {str(e)}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ ALL PIPELINE TESTS PASSED!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 설정\n",
    "    data_path = './tracks'              # 실제 데이터 경로로 변경하세요\n",
    "    meta_path = './meta_data.csv'      # 실제 메타데이터 경로로 변경하세요\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # 테스트 모드 선택\n",
    "    print(\"=\"*70)\n",
    "    print(\"VESSEL DETECTION MODEL TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nSelect mode:\")\n",
    "    print(\"1. Quick test (basic functionality check)\")\n",
    "    print(\"2. Full pipeline test (complete test with small data)\")\n",
    "    print(\"3. Full training (complete training with all data)\")\n",
    "    \n",
    "    mode = input(\"\\nEnter mode (1/2/3): \").strip()\n",
    "    \n",
    "    if mode == '1':\n",
    "        # 빠른 테스트\n",
    "        print(\"\\nRunning quick test...\")\n",
    "        if not quick_test(data_path, meta_path, test_size=10):\n",
    "            print(\"\\nQuick test failed. Please check the errors above.\")\n",
    "            return\n",
    "        print(\"\\nQuick test completed successfully!\")\n",
    "        \n",
    "    elif mode == '2':\n",
    "        # 전체 파이프라인 테스트\n",
    "        print(\"\\nRunning full pipeline test...\")\n",
    "        if not full_pipeline_test(data_path, meta_path, test_size=20, epochs=3):\n",
    "            print(\"\\nPipeline test failed. Please check the errors above.\")\n",
    "            return\n",
    "        print(\"\\nPipeline test completed successfully!\")\n",
    "        \n",
    "        response = input(\"\\nContinue with full training? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            print(\"Training cancelled.\")\n",
    "            return\n",
    "            \n",
    "    elif mode == '3':\n",
    "        # 전체 학습 진행\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Invalid mode selected. Exiting...\")\n",
    "        return\n",
    "    \n",
    "    # 전체 학습 코드 (mode 2에서 계속하거나 mode 3에서 바로 시작)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Starting full training...\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # 데이터 분석기 초기화\n",
    "    analyzer = VesselBehaviorAnalyzer(data_path, meta_path)\n",
    "    seq_extractor = SequenceFeatureExtractor()\n",
    "    \n",
    "    # 캐시 확인\n",
    "    cache_file = 'preprocessed_data_cache.pkl'\n",
    "    use_cache = False\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"\\nCache file found: {cache_file}\")\n",
    "        response = input(\"Use cached preprocessed data? (y/n) [default: y]: \").strip() or 'y'\n",
    "        use_cache = response.lower() == 'y'\n",
    "    \n",
    "    if use_cache:\n",
    "        print(\"\\nLoading preprocessed data from cache...\")\n",
    "        with open(cache_file, 'rb') as f:\n",
    "            cache_data = pickle.load(f)\n",
    "            all_sequences = cache_data['sequences']\n",
    "            all_features = cache_data['features']\n",
    "            all_labels = cache_data['labels']\n",
    "            all_mmsi = cache_data['mmsi']\n",
    "        print(f\"Loaded {len(all_sequences)} vessels from cache\")\n",
    "    else:\n",
    "        # 파일 존재 여부 체크\n",
    "        print(\"Checking data availability...\")\n",
    "        available_files = set()\n",
    "        for f in os.listdir(data_path):\n",
    "            if f.endswith('.csv') and f != 'meta_data.csv':\n",
    "                available_files.add(f.replace('.csv', ''))\n",
    "        \n",
    "        # MMSI 컬럼명 자동 감지\n",
    "        if 'MMSI' in analyzer.meta_data.columns:\n",
    "            meta_mmsi = set(analyzer.meta_data['MMSI'].astype(str))\n",
    "        else:\n",
    "            meta_mmsi = set(analyzer.meta_data['mmsi'].astype(str))\n",
    "        \n",
    "        # 실제 사용 가능한 MMSI 찾기\n",
    "        valid_mmsi = available_files & meta_mmsi\n",
    "        files_without_label = available_files - meta_mmsi\n",
    "        meta_without_file = meta_mmsi - available_files\n",
    "        \n",
    "        print(f\"Total MMSI in meta_data: {len(meta_mmsi)}\")\n",
    "        print(f\"Available CSV files: {len(available_files)}\")\n",
    "        print(f\"Valid MMSI (file + label): {len(valid_mmsi)}\")\n",
    "        \n",
    "        if files_without_label:\n",
    "            print(f\"Warning: {len(files_without_label)} files without labels in meta_data\")\n",
    "        if meta_without_file:\n",
    "            print(f\"Warning: {len(meta_without_file)} MMSI in meta_data without files\")\n",
    "        \n",
    "        # 유효한 MMSI만 필터링\n",
    "        if 'MMSI' in analyzer.meta_data.columns:\n",
    "            valid_meta_data = analyzer.meta_data[analyzer.meta_data['MMSI'].astype(str).isin(valid_mmsi)]\n",
    "        else:\n",
    "            valid_meta_data = analyzer.meta_data[analyzer.meta_data['mmsi'].astype(str).isin(valid_mmsi)]\n",
    "        \n",
    "        # 클래스 분포 확인\n",
    "        if 'label' in valid_meta_data.columns:\n",
    "            true_count = (valid_meta_data['label'] == True).sum()  # boolean True\n",
    "            false_count = (valid_meta_data['label'] == False).sum()  # boolean False\n",
    "        else:\n",
    "            true_count = (valid_meta_data['result'] == True).sum()\n",
    "            false_count = (valid_meta_data['result'] == False).sum()\n",
    "        \n",
    "        print(f\"\\nClass distribution in valid data:\")\n",
    "        print(f\"TRUE (NK vessels): {true_count} ({true_count/len(valid_meta_data)*100:.1f}%)\")\n",
    "        print(f\"FALSE (non-NK vessels): {false_count} ({false_count/len(valid_meta_data)*100:.1f}%)\")\n",
    "        \n",
    "        # 병렬 처리 옵션\n",
    "        use_parallel = input(\"\\nUse parallel processing? (y/n) [default: y]: \").strip() or 'y'\n",
    "        \n",
    "        if use_parallel.lower() == 'y':\n",
    "            # 병렬 처리\n",
    "            all_sequences, all_features, all_labels, all_mmsi = load_data_parallel(\n",
    "                data_path, meta_path, valid_meta_data\n",
    "            )\n",
    "        else:\n",
    "            # 기존 순차 처리 (최적화된 버전)\n",
    "            all_sequences = []\n",
    "            all_features = []\n",
    "            all_labels = []\n",
    "            all_mmsi = []\n",
    "            failed_mmsi = []\n",
    "            \n",
    "            print(\"\\nLoading and processing vessel data...\")\n",
    "            for idx, row in tqdm(valid_meta_data.iterrows(), total=len(valid_meta_data)):\n",
    "                # MMSI 컬럼명 처리\n",
    "                if 'MMSI' in valid_meta_data.columns:\n",
    "                    mmsi = str(row['MMSI'])\n",
    "                else:\n",
    "                    mmsi = str(row['mmsi'])\n",
    "                \n",
    "                # 레이블 컬럼명 처리\n",
    "                if 'label' in valid_meta_data.columns:\n",
    "                    label = int(row['label'])  # boolean을 int로 변환\n",
    "                else:\n",
    "                    label = int(row['result'])\n",
    "                \n",
    "                try:\n",
    "                    # 파일 존재 확인\n",
    "                    file_path = os.path.join(data_path, f\"{mmsi}.csv\")\n",
    "                    if not os.path.exists(file_path):\n",
    "                        failed_mmsi.append(mmsi)\n",
    "                        continue\n",
    "                        \n",
    "                    # 데이터 로드 및 전처리\n",
    "                    vessel_data = analyzer.load_and_preprocess(mmsi)\n",
    "                    \n",
    "                    # 최소 데이터 포인트 체크 (너무 짧은 궤적 제외)\n",
    "                    if len(vessel_data) < 50:\n",
    "                        print(f\"Skipping MMSI {mmsi}: insufficient data points ({len(vessel_data)})\")\n",
    "                        continue\n",
    "                    \n",
    "                    # 행동 특징 추출\n",
    "                    behavioral_features = analyzer.extract_behavioral_features(vessel_data)\n",
    "                    \n",
    "                    # 시퀀스 특징 추출\n",
    "                    sequence_features = seq_extractor.extract_sequence_features(vessel_data)\n",
    "                    \n",
    "                    all_sequences.append(sequence_features)\n",
    "                    all_features.append(behavioral_features)\n",
    "                    all_labels.append(label)\n",
    "                    all_mmsi.append(mmsi)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing MMSI {mmsi}: {e}\")\n",
    "                    failed_mmsi.append(mmsi)\n",
    "                    continue\n",
    "            \n",
    "            if failed_mmsi:\n",
    "                print(f\"Failed to process: {len(failed_mmsi)} vessels\")\n",
    "        \n",
    "        print(f\"\\nSuccessfully processed: {len(all_sequences)} vessels\")\n",
    "        \n",
    "        # 캐시 저장\n",
    "        print(\"\\nSaving preprocessed data to cache...\")\n",
    "        cache_data = {\n",
    "            'sequences': all_sequences,\n",
    "            'features': all_features,\n",
    "            'labels': all_labels,\n",
    "            'mmsi': all_mmsi\n",
    "        }\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(cache_data, f)\n",
    "        print(f\"Cache saved to {cache_file}\")\n",
    "    \n",
    "    if len(all_sequences) == 0:\n",
    "        print(\"No data was successfully loaded. Exiting...\")\n",
    "        return\n",
    "    \n",
    "    # NumPy 배열로 변환\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_mmsi = np.array(all_mmsi)\n",
    "    \n",
    "    # Train/Test 분리\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Train/Test Split\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # sklearn의 train_test_split 사용\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # 전체 데이터의 20%를 test set으로 분리\n",
    "    X_temp = np.arange(len(all_sequences))\n",
    "    X_train_idx, X_test_idx, y_train, y_test = train_test_split(\n",
    "        X_temp, all_labels, \n",
    "        test_size=0.2, \n",
    "        stratify=all_labels, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # 실제 데이터 분리\n",
    "    train_sequences = [all_sequences[i] for i in X_train_idx]\n",
    "    train_labels = y_train\n",
    "    train_mmsi = [all_mmsi[i] for i in X_train_idx]\n",
    "    \n",
    "    test_sequences = [all_sequences[i] for i in X_test_idx]\n",
    "    test_labels = y_test\n",
    "    test_mmsi = [all_mmsi[i] for i in X_test_idx]\n",
    "    \n",
    "    print(f\"Train set size: {len(train_sequences)} vessels\")\n",
    "    print(f\"Test set size: {len(test_sequences)} vessels\")\n",
    "    print(f\"Train set - TRUE: {(train_labels == 1).sum()}, FALSE: {(train_labels == 0).sum()}\")\n",
    "    print(f\"Test set - TRUE: {(test_labels == 1).sum()}, FALSE: {(test_labels == 0).sum()}\")\n",
    "    \n",
    "    # Test MMSI 리스트 저장\n",
    "    with open('test_mmsi_list.txt', 'w') as f:\n",
    "        for mmsi in test_mmsi:\n",
    "            f.write(f\"{mmsi}\\n\")\n",
    "    print(\"\\nTest MMSI list saved to 'test_mmsi_list.txt'\")\n",
    "    \n",
    "    # K-Fold Cross Validation (train set에서만)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    best_models = []\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_sequences, train_labels)):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Fold {fold + 1}/5\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # 데이터셋 생성\n",
    "        fold_train_sequences = [train_sequences[i] for i in train_idx]\n",
    "        fold_train_labels = train_labels[train_idx]\n",
    "        fold_val_sequences = [train_sequences[i] for i in val_idx]\n",
    "        fold_val_labels = train_labels[val_idx]\n",
    "        \n",
    "        train_dataset = VesselDataset(fold_train_sequences, fold_train_labels, augment=True)\n",
    "        val_dataset = VesselDataset(fold_val_sequences, fold_val_labels, augment=False)\n",
    "        \n",
    "        # 배치 사이즈 동적 설정\n",
    "        batch_size = min(32, len(fold_train_sequences) // 4)\n",
    "        batch_size = max(batch_size, 1)\n",
    "        \n",
    "        # GPU 최적화된 DataLoader\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=4,  # 멀티 워커 사용\n",
    "            pin_memory=True,  # GPU 전송 속도 향상\n",
    "            persistent_workers=True  # 워커 재사용\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "        \n",
    "        # 모델 초기화 및 학습 - input_dim 변경\n",
    "        model = SimpleLSTM(input_dim=10)\n",
    "        trained_model, best_threshold = train_model_with_strategy(\n",
    "            model, train_loader, val_loader, epochs=50, device=device\n",
    "        )\n",
    "        \n",
    "        # 폴드별 모델과 임계값 저장\n",
    "        torch.save({\n",
    "            'model_state_dict': trained_model.state_dict(),\n",
    "            'threshold': best_threshold\n",
    "        }, f'vessel_model_fold{fold+1}.pth')\n",
    "        \n",
    "        best_models.append(trained_model)\n",
    "        fold_scores.append({'threshold': best_threshold})\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training completed! Evaluating on held-out test set...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Test set 평가 (앙상블) - GPU 최적화\n",
    "    test_dataset = VesselDataset(test_sequences, test_labels, augment=False)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=32, \n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 앙상블 예측\n",
    "    all_test_preds = []\n",
    "    optimal_thresholds = []\n",
    "    \n",
    "    for i, (model, fold_info) in enumerate(zip(best_models, fold_scores)):\n",
    "        model.eval()\n",
    "        fold_preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                \n",
    "                if AMP_AVAILABLE:\n",
    "                    # Mixed Precision for inference\n",
    "                    with autocast():\n",
    "                        logits = model(batch_x)\n",
    "                else:\n",
    "                    logits = model(batch_x)\n",
    "                \n",
    "                fold_preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "        \n",
    "        all_test_preds.append(fold_preds)\n",
    "        optimal_thresholds.append(fold_info['threshold'])\n",
    "    \n",
    "    # 평균 앙상블\n",
    "    ensemble_preds = np.mean(all_test_preds, axis=0)\n",
    "    \n",
    "    # 평균 최적 임계값 사용\n",
    "    avg_threshold = np.mean(optimal_thresholds)\n",
    "    print(f\"\\nAverage optimal threshold from folds: {avg_threshold:.3f}\")\n",
    "    \n",
    "    # 클래스 분포를 고려한 보정\n",
    "    # 예측 확률이 너무 높으면 조정 (FALSE가 많아야 함)\n",
    "    adjusted_preds = ensemble_preds * 0.9  # 살짝 낮춤\n",
    "    \n",
    "    # 최종 예측\n",
    "    ensemble_preds_binary = (adjusted_preds > avg_threshold).astype(int).flatten()\n",
    "    test_labels_binary = test_labels.astype(int)\n",
    "    \n",
    "    # 예측 분포 확인\n",
    "    print(f\"\\nPrediction distribution:\")\n",
    "    print(f\"TRUE predictions: {ensemble_preds_binary.sum()} ({ensemble_preds_binary.sum()/len(ensemble_preds_binary)*100:.1f}%)\")\n",
    "    print(f\"FALSE predictions: {len(ensemble_preds_binary) - ensemble_preds_binary.sum()}\")\n",
    "    \n",
    "    # 예상 점수 계산\n",
    "    if ensemble_preds_binary.sum() > 0:\n",
    "        expected_precision = 0.19  # 테스트셋 TRUE 비율\n",
    "        pred_true_ratio = ensemble_preds_binary.sum() / len(ensemble_preds_binary)\n",
    "        adjusted_precision = expected_precision / pred_true_ratio\n",
    "        expected_f1 = 2 * adjusted_precision / (1 + adjusted_precision)\n",
    "        print(f\"\\nIf all TRUE predictions are correct:\")\n",
    "        print(f\"Expected precision: {adjusted_precision:.3f}\")\n",
    "        print(f\"Expected F1: {expected_f1:.3f}\")\n",
    "    \n",
    "    # 최종 성능 평가\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(classification_report(test_labels_binary, ensemble_preds_binary, \n",
    "                              target_names=['Non-NK', 'NK vessel']))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(test_labels_binary, ensemble_preds_binary))\n",
    "    \n",
    "    # 예측 결과 저장\n",
    "    test_results = pd.DataFrame({\n",
    "        'mmsi': test_mmsi,\n",
    "        'true_label': test_labels_binary,\n",
    "        'predicted_label': ensemble_preds_binary,\n",
    "        'prediction_prob': adjusted_preds.flatten(),\n",
    "        'threshold_used': avg_threshold\n",
    "    })\n",
    "    test_results.to_csv('test_predictions.csv', index=False)\n",
    "    print(\"\\nTest predictions saved to 'test_predictions.csv'\")\n",
    "    \n",
    "    print(\"\\nTraining and evaluation completed!\")\n",
    "    \n",
    "    # 최종 제출용 함수 호출 가능\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"For competition submission, use the trained models with:\")\n",
    "    print(f\"Optimal threshold: {avg_threshold:.3f}\")\n",
    "    print(\"Remember to adjust predictions to achieve ~19% TRUE ratio\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a60e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data(test_meta_path, data_path, model_paths, threshold, device='cuda'):\n",
    "    \"\"\"학습된 모델로 테스트 데이터 예측 - 30초 샘플링, 앙상블\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PREDICTION ON TEST DATA (Ensemble)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. 테스트 메타데이터 로드\n",
    "    test_meta = pd.read_csv(test_meta_path)\n",
    "    print(f\"Loaded test metadata with {len(test_meta)} vessels\")\n",
    "    \n",
    "    # 2. 데이터 분석기 초기화 - 30초 샘플링\n",
    "    class FastVesselAnalyzer(VesselBehaviorAnalyzer):\n",
    "        def load_and_preprocess(self, mmsi):\n",
    "            \"\"\"30초 샘플링 전처리\"\"\"\n",
    "            df = pd.read_csv(os.path.join(self.data_path, f\"{mmsi}.csv\"))\n",
    "            \n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "            df = df.sort_values('timestamp')\n",
    "            df = df.drop_duplicates(subset=['timestamp'], keep='first')\n",
    "            df.set_index('timestamp', inplace=True)\n",
    "            \n",
    "            time_diff = df.index.to_series().diff().dt.total_seconds()\n",
    "            median_diff = time_diff.median()\n",
    "            \n",
    "            if median_diff > 60:  # 중간값이 60초 초과면 리샘플링\n",
    "                df = df.resample('15S').mean()  # 30초 간격\n",
    "                df = df.interpolate(method='linear', limit=3)\n",
    "            \n",
    "            df = df.dropna()\n",
    "            return df\n",
    "            \n",
    "        def _find_stationary_periods_vectorized(self, sog):\n",
    "            \"\"\"정박 기간 찾기 - 30초 간격 수정\"\"\"\n",
    "            stationary = sog < 0.5\n",
    "            \n",
    "            if len(stationary) == 0:\n",
    "                return {\n",
    "                    'num_stops': 0,\n",
    "                    'avg_stop_duration': 0,\n",
    "                    'longest_stop': 0\n",
    "                }\n",
    "            \n",
    "            padded = np.concatenate([[False], stationary, [False]])\n",
    "            diff = np.diff(padded.astype(int))\n",
    "            starts = np.where(diff == 1)[0]\n",
    "            ends = np.where(diff == -1)[0]\n",
    "            \n",
    "            if len(starts) == 0 or len(ends) == 0:\n",
    "                return {\n",
    "                    'num_stops': 0,\n",
    "                    'avg_stop_duration': 0,\n",
    "                    'longest_stop': 0\n",
    "                }\n",
    "            \n",
    "            if ends[0] < starts[0]:\n",
    "                ends = ends[1:]\n",
    "            if len(starts) > len(ends):\n",
    "                starts = starts[:len(ends)]\n",
    "            \n",
    "            durations = (ends - starts) * 30  # 30초 간격으로 변경\n",
    "            \n",
    "            return {\n",
    "                'num_stops': int(len(starts)),\n",
    "                'avg_stop_duration': float(np.mean(durations)) if len(durations) > 0 else 0,\n",
    "                'longest_stop': float(np.max(durations)) if len(durations) > 0 else 0\n",
    "            }\n",
    "    \n",
    "    analyzer = FastVesselAnalyzer(data_path, test_meta_path)\n",
    "    seq_extractor = SequenceFeatureExtractor()\n",
    "    \n",
    "    # 3. 사용 가능한 파일 확인\n",
    "    available_files = set()\n",
    "    for f in os.listdir(data_path):\n",
    "        if f.endswith('.csv') and f != 'meta_data.csv':\n",
    "            available_files.add(f.replace('.csv', ''))\n",
    "    \n",
    "    # MMSI 추출\n",
    "    if 'MMSI' in test_meta.columns:\n",
    "        test_mmsi_list = test_meta['MMSI'].astype(str).tolist()\n",
    "    else:\n",
    "        test_mmsi_list = test_meta['mmsi'].astype(str).tolist()\n",
    "    \n",
    "    # 4. 테스트 데이터 전처리\n",
    "    print(\"\\nProcessing test vessels...\")\n",
    "    test_sequences = []\n",
    "    test_mmsi_valid = []\n",
    "    \n",
    "    for mmsi in tqdm(test_mmsi_list):\n",
    "        if mmsi not in available_files:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            vessel_data = analyzer.load_and_preprocess(mmsi)\n",
    "            \n",
    "            if len(vessel_data) < 50:  # 학습 때와 동일한 임계값\n",
    "                continue\n",
    "            \n",
    "            sequence_features = seq_extractor.extract_sequence_features(vessel_data)\n",
    "            test_sequences.append(sequence_features)\n",
    "            test_mmsi_valid.append(mmsi)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {len(test_sequences)} vessels out of {len(test_mmsi_list)}\")\n",
    "    \n",
    "    # 5. 데이터셋 생성\n",
    "    if len(test_sequences) == 0:\n",
    "        print(\"No valid vessels to predict!\")\n",
    "        results_df = pd.DataFrame({\n",
    "            'MMSI': test_mmsi_list,\n",
    "            'prediction': 0,\n",
    "            'probability': 0.0\n",
    "        })\n",
    "        results_df.to_csv('test_predictions_final.csv', index=False)\n",
    "        return results_df\n",
    "    \n",
    "    dummy_labels = np.zeros(len(test_sequences))\n",
    "    test_dataset = VesselDataset(test_sequences, dummy_labels, augment=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # 6. 모델 로드 및 앙상블 예측\n",
    "    print(\"\\nLoading models and making predictions...\")\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i, model_path in enumerate(model_paths):\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Warning: {model_path} not found, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Loading model {i+1}/{len(model_paths)}: {model_path}\")\n",
    "        \n",
    "        # 모델 초기화 및 가중치 로드\n",
    "        model = SimpleLSTM(input_dim=10).to(device)\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        # 예측\n",
    "        fold_preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, _ in test_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                logits = model(batch_x)\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "                fold_preds.extend(probs)\n",
    "        \n",
    "        all_predictions.append(fold_preds)\n",
    "    \n",
    "    # 7. 앙상블 평균\n",
    "    if len(all_predictions) > 0:\n",
    "        ensemble_probs = np.mean(all_predictions, axis=0).flatten()\n",
    "    else:\n",
    "        print(\"No models loaded successfully!\")\n",
    "        ensemble_probs = np.zeros(len(test_sequences))\n",
    "    \n",
    "    # 8. 임계값 적용하여 최종 예측\n",
    "    predictions = (ensemble_probs > threshold).astype(int)\n",
    "    \n",
    "    # 9. 전체 MMSI에 대한 결과 생성\n",
    "    final_results = []\n",
    "    for mmsi in test_mmsi_list:\n",
    "        if mmsi in test_mmsi_valid:\n",
    "            idx = test_mmsi_valid.index(mmsi)\n",
    "            final_results.append({\n",
    "                'MMSI': mmsi,\n",
    "                'prediction': predictions[idx],\n",
    "                'probability': ensemble_probs[idx]\n",
    "            })\n",
    "        else:\n",
    "            # 처리되지 않은 선박은 FALSE(0)로 예측\n",
    "            final_results.append({\n",
    "                'MMSI': mmsi,\n",
    "                'prediction': 0,\n",
    "                'probability': 0.0\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(final_results)\n",
    "    \n",
    "    # 예측 분포 확인\n",
    "    print(f\"\\nPrediction distribution:\")\n",
    "    print(f\"TRUE predictions: {(results_df['prediction'] == 1).sum()} ({(results_df['prediction'] == 1).sum()/len(results_df)*100:.1f}%)\")\n",
    "    print(f\"FALSE predictions: {(results_df['prediction'] == 0).sum()}\")\n",
    "    \n",
    "    # 결과 저장\n",
    "    output_file = 'test_predictions_final.csv'\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nPredictions saved to {output_file}\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac147fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 모델 경로 설정 (5개 모두 사용)\n",
    "model_paths = [f'vessel_model_fold{i}.pth' for i in range(1, 6)]\n",
    "\n",
    "# 2. 최적 임계값 가져오기\n",
    "checkpoint = torch.load('vessel_model_fold1.pth')\n",
    "threshold = checkpoint.get('threshold', 0.5)\n",
    "print(f\"Using threshold: {threshold}\")\n",
    "\n",
    "# 3. 앙상블 예측 실행\n",
    "predictions = predict_test_data(\n",
    "    test_meta_path='./meta_test_data.csv',\n",
    "    data_path='./tracks',\n",
    "    model_paths=model_paths,\n",
    "    threshold=threshold,\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# 4. 0,1을 TRUE/FALSE로 변환\n",
    "df = pd.read_csv('test_predictions_final.csv')\n",
    "df['prediction'] = df['prediction'].map({0: 'FALSE', 1: 'TRUE'})\n",
    "df.to_csv('test_predictions_final.csv', index=False)\n",
    "\n",
    "print(\"\\nPrediction completed!\")\n",
    "print(f\"TRUE predictions: {(df['prediction'] == 'TRUE').sum()}\")\n",
    "print(f\"FALSE predictions: {(df['prediction'] == 'FALSE').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd2ae76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 시 사용하는 코드\n",
    "model_paths = './vessel_model_fold4.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c4377fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using threshold: 0.4000000000000001\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('vessel_model_fold4.pth')\n",
    "threshold = checkpoint.get('threshold', 0.5)\n",
    "print(f\"Using threshold: {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ddc8d461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PREDICTION ON TEST DATA (Single Model)\n",
      "======================================================================\n",
      "Loaded test metadata with 300 vessels\n",
      "\n",
      "Processing test vessels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [02:47<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully processed 258 vessels out of 300\n",
      "\n",
      "Loading model: vessel_model_fold4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 5/5 [00:00<00:00, 100.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction distribution:\n",
      "TRUE predictions: 58 (19.3%)\n",
      "FALSE predictions: 242\n",
      "\n",
      "Predictions saved to test_predictions_final.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_test_data_single(test_meta_path, data_path, model_path, threshold, device='cuda'):\n",
    "    \"\"\"단일 모델로 테스트 데이터 예측 - 60초 샘플링\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PREDICTION ON TEST DATA (Single Model)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. 테스트 메타데이터 로드\n",
    "    test_meta = pd.read_csv(test_meta_path)\n",
    "    print(f\"Loaded test metadata with {len(test_meta)} vessels\")\n",
    "    \n",
    "    # 2. 데이터 분석기 초기화 - 60초 샘플링\n",
    "    class FastVesselAnalyzer(VesselBehaviorAnalyzer):\n",
    "        def load_and_preprocess(self, mmsi):\n",
    "            \"\"\"60초 샘플링 전처리\"\"\"\n",
    "            df = pd.read_csv(os.path.join(self.data_path, f\"{mmsi}.csv\"))\n",
    "            \n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)\n",
    "            df = df.sort_values('timestamp')\n",
    "            df = df.drop_duplicates(subset=['timestamp'], keep='first')\n",
    "            df.set_index('timestamp', inplace=True)\n",
    "            \n",
    "            time_diff = df.index.to_series().diff().dt.total_seconds()\n",
    "            median_diff = time_diff.median()\n",
    "            \n",
    "            if median_diff > 120:\n",
    "                df = df.resample('60S').mean()\n",
    "                df = df.interpolate(method='linear', limit=3)\n",
    "            \n",
    "            df = df.dropna()\n",
    "            return df\n",
    "    \n",
    "    analyzer = FastVesselAnalyzer(data_path, test_meta_path)\n",
    "    seq_extractor = SequenceFeatureExtractor()\n",
    "    \n",
    "    # 3. 사용 가능한 파일 확인\n",
    "    available_files = set()\n",
    "    for f in os.listdir(data_path):\n",
    "        if f.endswith('.csv') and f != 'meta_data.csv':\n",
    "            available_files.add(f.replace('.csv', ''))\n",
    "    \n",
    "    # MMSI 추출\n",
    "    if 'MMSI' in test_meta.columns:\n",
    "        test_mmsi_list = test_meta['MMSI'].astype(str).tolist()\n",
    "    else:\n",
    "        test_mmsi_list = test_meta['mmsi'].astype(str).tolist()\n",
    "    \n",
    "    # 4. 테스트 데이터 전처리\n",
    "    print(\"\\nProcessing test vessels...\")\n",
    "    test_sequences = []\n",
    "    test_mmsi_valid = []\n",
    "    \n",
    "    for mmsi in tqdm(test_mmsi_list):\n",
    "        if mmsi not in available_files:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            vessel_data = analyzer.load_and_preprocess(mmsi)\n",
    "            \n",
    "            if len(vessel_data) < 25:\n",
    "                continue\n",
    "            \n",
    "            sequence_features = seq_extractor.extract_sequence_features(vessel_data)\n",
    "            test_sequences.append(sequence_features)\n",
    "            test_mmsi_valid.append(mmsi)\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nSuccessfully processed {len(test_sequences)} vessels out of {len(test_mmsi_list)}\")\n",
    "    \n",
    "    # 5. 예측\n",
    "    if len(test_sequences) == 0:\n",
    "        print(\"No valid vessels to predict!\")\n",
    "        results_df = pd.DataFrame({\n",
    "            'MMSI': test_mmsi_list,\n",
    "            'prediction': 0,\n",
    "            'probability': 0.0\n",
    "        })\n",
    "        results_df.to_csv('test_predictions_final.csv', index=False)\n",
    "        return results_df\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    dummy_labels = np.zeros(len(test_sequences))\n",
    "    test_dataset = VesselDataset(test_sequences, dummy_labels, augment=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # 6. 모델 로드 및 예측\n",
    "    print(f\"\\nLoading model: {model_path}\")\n",
    "    model = SimpleLSTM(input_dim=10).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # 예측\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, _ in tqdm(test_loader, desc=\"Predicting\"):\n",
    "            batch_x = batch_x.to(device)\n",
    "            logits = model(batch_x)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_probs.extend(probs)\n",
    "    \n",
    "    # 7. 결과 정리\n",
    "    all_probs = np.array(all_probs).flatten()\n",
    "    predictions = (all_probs > threshold).astype(int)\n",
    "    \n",
    "    # 8. 전체 MMSI에 대한 결과 생성\n",
    "    final_results = []\n",
    "    for mmsi in test_mmsi_list:\n",
    "        if mmsi in test_mmsi_valid:\n",
    "            idx = test_mmsi_valid.index(mmsi)\n",
    "            final_results.append({\n",
    "                'MMSI': mmsi,\n",
    "                'prediction': predictions[idx],\n",
    "                'probability': all_probs[idx]\n",
    "            })\n",
    "        else:\n",
    "            final_results.append({\n",
    "                'MMSI': mmsi,\n",
    "                'prediction': 0,\n",
    "                'probability': 0.0\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(final_results)\n",
    "    \n",
    "    # 예측 분포 확인\n",
    "    print(f\"\\nPrediction distribution:\")\n",
    "    print(f\"TRUE predictions: {(results_df['prediction'] == 1).sum()} ({(results_df['prediction'] == 1).sum()/len(results_df)*100:.1f}%)\")\n",
    "    print(f\"FALSE predictions: {(results_df['prediction'] == 0).sum()}\")\n",
    "    \n",
    "    # 결과 저장\n",
    "    output_file = 'test_predictions_final.csv'\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nPredictions saved to {output_file}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# 사용 방법\n",
    "model_path = 'vessel_model_fold4.pth'  # 4번째 fold 모델\n",
    "checkpoint = torch.load(model_path)\n",
    "threshold = checkpoint.get('threshold', 0.5)\n",
    "\n",
    "predictions = predict_test_data_single(\n",
    "    test_meta_path='./meta_test_data.csv',\n",
    "    data_path='./tracks',\n",
    "    model_path=model_path,\n",
    "    threshold=threshold,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9f2e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 줄로 처리\n",
    "pd.read_csv('test_predictions_final.csv').assign(prediction=lambda x: x['prediction'].map({0: 'FALSE', 1: 'TRUE'})).to_csv('test_predictions_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3605df79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comprehensive TRUE vessel analysis ===\n",
      "Total vessels in metadata: 122\n",
      "TRUE vessels: 0 (0.0%)\n",
      "FALSE vessels: 0 (0.0%)\n",
      "\n",
      "Total CSV files available: 1299\n",
      "Valid MMSI (file + label): 105\n",
      "\n",
      "TRUE vessels with files: 0/0\n",
      "TRUE vessels in valid_mmsi: 0\n",
      "\n",
      "In valid_meta_data:\n",
      "Total: 105\n",
      "TRUE: 0 (0.0%)\n",
      "FALSE: 105\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 경로 설정\n",
    "data_path = './tracks'  # 실제 경로로 변경\n",
    "meta_path = './meta_data.csv'  # 실제 경로로 변경\n",
    "\n",
    "# 메타데이터 로드\n",
    "meta_data = pd.read_csv(meta_path)\n",
    "\n",
    "print(\"\\n=== Comprehensive TRUE vessel analysis ===\")\n",
    "\n",
    "# 1. 메타데이터의 TRUE 분포\n",
    "true_vessels = meta_data[meta_data['label'] == 'TRUE']\n",
    "false_vessels = meta_data[meta_data['label'] == 'FALSE']\n",
    "print(f\"Total vessels in metadata: {len(meta_data)}\")\n",
    "print(f\"TRUE vessels: {len(true_vessels)} ({len(true_vessels)/len(meta_data)*100:.1f}%)\")\n",
    "print(f\"FALSE vessels: {len(false_vessels)} ({len(false_vessels)/len(meta_data)*100:.1f}%)\")\n",
    "\n",
    "# 2. available files 확인\n",
    "available_files = set()\n",
    "for f in os.listdir(data_path):\n",
    "    if f.endswith('.csv') and f != 'meta_data.csv':\n",
    "        available_files.add(f.replace('.csv', ''))\n",
    "\n",
    "print(f\"\\nTotal CSV files available: {len(available_files)}\")\n",
    "\n",
    "# 3. MMSI 추출 및 valid_mmsi 계산\n",
    "if 'MMSI' in meta_data.columns:\n",
    "    meta_mmsi = set(meta_data['MMSI'].astype(str))\n",
    "else:\n",
    "    meta_mmsi = set(meta_data['mmsi'].astype(str))\n",
    "\n",
    "valid_mmsi = available_files & meta_mmsi\n",
    "print(f\"Valid MMSI (file + label): {len(valid_mmsi)}\")\n",
    "\n",
    "# 4. TRUE 선박 파일 존재 여부\n",
    "true_mmsi_list = true_vessels['MMSI'].astype(str).tolist()\n",
    "true_with_files = [mmsi for mmsi in true_mmsi_list if os.path.exists(os.path.join(data_path, f\"{mmsi}.csv\"))]\n",
    "print(f\"\\nTRUE vessels with files: {len(true_with_files)}/{len(true_mmsi_list)}\")\n",
    "\n",
    "# 5. valid_mmsi에 포함된 TRUE 선박\n",
    "true_in_valid = [mmsi for mmsi in true_mmsi_list if mmsi in valid_mmsi]\n",
    "print(f\"TRUE vessels in valid_mmsi: {len(true_in_valid)}\")\n",
    "\n",
    "# 6. 데이터 포인트 확인 (샘플)\n",
    "if true_with_files:\n",
    "    print(\"\\nChecking data points for TRUE vessels (first 5):\")\n",
    "    for mmsi in true_with_files[:5]:\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(data_path, f\"{mmsi}.csv\"))\n",
    "            print(f\"  MMSI {mmsi}: {len(df)} points\")\n",
    "            \n",
    "            # 전처리 후 길이도 확인\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.sort_values('timestamp')\n",
    "            df = df.drop_duplicates(subset=['timestamp'], keep='first')\n",
    "            df = df.set_index('timestamp')\n",
    "            df = df.resample('30s').mean()\n",
    "            df = df.dropna()\n",
    "            print(f\"    After preprocessing: {len(df)} points\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  MMSI {mmsi}: Error - {e}\")\n",
    "\n",
    "# 7. valid_meta_data에서 TRUE 분포\n",
    "valid_meta_data = meta_data[meta_data['MMSI'].astype(str).isin(valid_mmsi)]\n",
    "true_in_valid_meta = (valid_meta_data['label'] == 'TRUE').sum()\n",
    "print(f\"\\nIn valid_meta_data:\")\n",
    "print(f\"Total: {len(valid_meta_data)}\")\n",
    "print(f\"TRUE: {true_in_valid_meta} ({true_in_valid_meta/len(valid_meta_data)*100:.1f}%)\")\n",
    "print(f\"FALSE: {len(valid_meta_data) - true_in_valid_meta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bca18962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Label Column Analysis ===\n",
      "\n",
      "Column names in meta_data:\n",
      "['MMSI', 'label']\n",
      "\n",
      "Unique values in 'label' column:\n",
      "label\n",
      "True     68\n",
      "False    54\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Data type of 'label': bool\n",
      "\n",
      "First 10 label values (with repr):\n",
      "  0: True (type: bool)\n",
      "  1: True (type: bool)\n",
      "  2: True (type: bool)\n",
      "  3: True (type: bool)\n",
      "  4: True (type: bool)\n",
      "  5: True (type: bool)\n",
      "  6: True (type: bool)\n",
      "  7: True (type: bool)\n",
      "  8: True (type: bool)\n",
      "  9: True (type: bool)\n",
      "\n",
      "Searching for possible label columns:\n",
      "\n",
      "label: [ True False]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 메타데이터 로드\n",
    "meta_path = './meta_data.csv'\n",
    "meta_data = pd.read_csv(meta_path)\n",
    "\n",
    "print(\"=== Label Column Analysis ===\")\n",
    "\n",
    "# 1. 컬럼명 확인\n",
    "print(f\"\\nColumn names in meta_data:\")\n",
    "print(meta_data.columns.tolist())\n",
    "\n",
    "# 2. label 컬럼의 고유값 확인\n",
    "if 'label' in meta_data.columns:\n",
    "    print(f\"\\nUnique values in 'label' column:\")\n",
    "    print(meta_data['label'].value_counts())\n",
    "    print(f\"\\nData type of 'label': {meta_data['label'].dtype}\")\n",
    "    \n",
    "    # 샘플 값 확인\n",
    "    print(f\"\\nFirst 10 label values (with repr):\")\n",
    "    for i, val in enumerate(meta_data['label'].head(10)):\n",
    "        print(f\"  {i}: {repr(val)} (type: {type(val).__name__})\")\n",
    "        \n",
    "elif 'result' in meta_data.columns:\n",
    "    print(f\"\\nUnique values in 'result' column:\")\n",
    "    print(meta_data['result'].value_counts())\n",
    "    print(f\"\\nData type of 'result': {meta_data['result'].dtype}\")\n",
    "    \n",
    "    print(f\"\\nFirst 10 result values (with repr):\")\n",
    "    for i, val in enumerate(meta_data['result'].head(10)):\n",
    "        print(f\"  {i}: {repr(val)} (type: {type(val).__name__})\")\n",
    "\n",
    "# 3. 다른 가능한 레이블 컬럼 찾기\n",
    "print(f\"\\nSearching for possible label columns:\")\n",
    "for col in meta_data.columns:\n",
    "    unique_values = meta_data[col].unique()\n",
    "    if len(unique_values) < 10:  # 카테고리형 컬럼만\n",
    "        print(f\"\\n{col}: {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "915fb060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Corrected TRUE vessel analysis ===\n",
      "Total vessels in metadata: 122\n",
      "TRUE vessels: 68 (55.7%)\n",
      "FALSE vessels: 54 (44.3%)\n",
      "\n",
      "Total CSV files available: 1299\n",
      "Valid MMSI (file + label): 105\n",
      "\n",
      "TRUE vessels with files: 54/68\n",
      "TRUE vessels in valid_mmsi: 54\n",
      "\n",
      "In valid_meta_data:\n",
      "Total: 105\n",
      "TRUE: 54 (51.4%)\n",
      "FALSE: 51\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 경로 설정\n",
    "data_path = './tracks'\n",
    "meta_path = './meta_data.csv'\n",
    "\n",
    "# 메타데이터 로드\n",
    "meta_data = pd.read_csv(meta_path)\n",
    "\n",
    "print(\"\\n=== Corrected TRUE vessel analysis ===\")\n",
    "\n",
    "# 1. 메타데이터의 TRUE 분포 (boolean으로)\n",
    "true_vessels = meta_data[meta_data['label'] == True]  # boolean True\n",
    "false_vessels = meta_data[meta_data['label'] == False]  # boolean False\n",
    "print(f\"Total vessels in metadata: {len(meta_data)}\")\n",
    "print(f\"TRUE vessels: {len(true_vessels)} ({len(true_vessels)/len(meta_data)*100:.1f}%)\")\n",
    "print(f\"FALSE vessels: {len(false_vessels)} ({len(false_vessels)/len(meta_data)*100:.1f}%)\")\n",
    "\n",
    "# 2. available files 확인\n",
    "available_files = set()\n",
    "for f in os.listdir(data_path):\n",
    "    if f.endswith('.csv') and f != 'meta_data.csv':\n",
    "        available_files.add(f.replace('.csv', ''))\n",
    "\n",
    "print(f\"\\nTotal CSV files available: {len(available_files)}\")\n",
    "\n",
    "# 3. valid_mmsi 계산\n",
    "meta_mmsi = set(meta_data['MMSI'].astype(str))\n",
    "valid_mmsi = available_files & meta_mmsi\n",
    "print(f\"Valid MMSI (file + label): {len(valid_mmsi)}\")\n",
    "\n",
    "# 4. TRUE 선박 파일 존재 여부\n",
    "true_mmsi_list = true_vessels['MMSI'].astype(str).tolist()\n",
    "true_with_files = [mmsi for mmsi in true_mmsi_list if mmsi in available_files]\n",
    "print(f\"\\nTRUE vessels with files: {len(true_with_files)}/{len(true_mmsi_list)}\")\n",
    "\n",
    "# 5. valid_mmsi에 포함된 TRUE 선박\n",
    "true_in_valid = [mmsi for mmsi in true_mmsi_list if mmsi in valid_mmsi]\n",
    "print(f\"TRUE vessels in valid_mmsi: {len(true_in_valid)}\")\n",
    "\n",
    "# 6. valid_meta_data에서 TRUE 분포\n",
    "valid_meta_data = meta_data[meta_data['MMSI'].astype(str).isin(valid_mmsi)]\n",
    "true_in_valid_meta = (valid_meta_data['label'] == True).sum()  # boolean True\n",
    "print(f\"\\nIn valid_meta_data:\")\n",
    "print(f\"Total: {len(valid_meta_data)}\")\n",
    "print(f\"TRUE: {true_in_valid_meta} ({true_in_valid_meta/len(valid_meta_data)*100:.1f}%)\")\n",
    "print(f\"FALSE: {len(valid_meta_data) - true_in_valid_meta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ccf69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
